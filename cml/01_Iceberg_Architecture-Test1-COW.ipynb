{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0942afe9-7629-4b3e-b747-b795a6454a2b",
   "metadata": {},
   "source": [
    "## Introduction to Iceberg Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f7e7d5e-5a02-414b-9177-49cdfe38fddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cmlbootstrap\n",
      "  Cloning https://github.com/fastforwardlabs/cmlbootstrap to /tmp/pip-install-493gqeea/cmlbootstrap_3b27dacd91bc4abc9cf0f1639cc4229b\n",
      "  Running command git clone -q https://github.com/fastforwardlabs/cmlbootstrap /tmp/pip-install-493gqeea/cmlbootstrap_3b27dacd91bc4abc9cf0f1639cc4229b\n",
      "Requirement already satisfied: pandas in /home/cdsw/.local/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (1.3.5)\n",
      "Requirement already satisfied: numpy==1.19.0 in /home/cdsw/.local/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (1.19.0)\n",
      "Requirement already satisfied: boto3 in /home/cdsw/.local/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (1.17.62)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 5)) (2.25.1)\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 6)) (1.26.6)\n",
      "Requirement already satisfied: dbldatagen in /home/cdsw/.local/lib/python3.7/site-packages (from -r requirements.txt (line 7)) (0.2.1)\n",
      "Requirement already satisfied: PyArrow in /home/cdsw/.local/lib/python3.7/site-packages (from -r requirements.txt (line 8)) (12.0.0)\n",
      "Requirement already satisfied: requests-kerberos==0.12.0 in /home/cdsw/.local/lib/python3.7/site-packages (from cmlbootstrap->-r requirements.txt (line 4)) (0.12.0)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.62 in /home/cdsw/.local/lib/python3.7/site-packages (from boto3->-r requirements.txt (line 3)) (1.20.112)\n",
      "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /home/cdsw/.local/lib/python3.7/site-packages (from boto3->-r requirements.txt (line 3)) (0.4.2)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/cdsw/.local/lib/python3.7/site-packages (from boto3->-r requirements.txt (line 3)) (0.10.0)\n",
      "Requirement already satisfied: pykerberos<2.0.0,>=1.1.8 in /home/cdsw/.local/lib/python3.7/site-packages (from requests-kerberos==0.12.0->cmlbootstrap->-r requirements.txt (line 4)) (1.2.4)\n",
      "Requirement already satisfied: cryptography>=1.3 in /home/cdsw/.local/lib/python3.7/site-packages (from requests-kerberos==0.12.0->cmlbootstrap->-r requirements.txt (line 4)) (40.0.2)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests->-r requirements.txt (line 5)) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests->-r requirements.txt (line 5)) (2020.11.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests->-r requirements.txt (line 5)) (2.10)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/site-packages (from botocore<1.21.0,>=1.20.62->boto3->-r requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/site-packages (from cryptography>=1.3->requests-kerberos==0.12.0->cmlbootstrap->-r requirements.txt (line 4)) (1.14.4)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/site-packages (from cffi>=1.12->cryptography>=1.3->requests-kerberos==0.12.0->cmlbootstrap->-r requirements.txt (line 4)) (2.20)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.62->boto3->-r requirements.txt (line 3)) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 1)) (2021.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd15c6f-e31d-440c-bc48-966385b7c5ec",
   "metadata": {},
   "source": [
    "#### Launching a Spark Session with Iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f139b6a-eed6-4848-9e82-593aabad58b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 22:19:43 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "23/09/04 22:19:43 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "Setting spark.hadoop.yarn.resourcemanager.principal to pauldefusco\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/04 22:19:44 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "23/09/04 22:19:44 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "23/09/04 22:19:44 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "23/09/04 22:19:46 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:135)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/04 22:19:46 WARN HttpChannel: /jobs/\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:135)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/04 22:19:46 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "23/09/04 22:19:47 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/04 22:19:47 WARN HttpChannel: /jobs/\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/04 22:19:48 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/04 22:19:48 WARN HttpChannel: /jobs/\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/04 22:19:49 WARN HiveServer2CredentialProvider: Failed to get HS2 delegation token\n",
      "java.util.NoSuchElementException: spark.sql.hive.hiveserver2.jdbc.url\n",
      "\tat org.apache.spark.SparkConf.$anonfun$get$1(SparkConf.scala:245)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.SparkConf.get(SparkConf.scala:245)\n",
      "\tat com.hortonworks.spark.deploy.yarn.security.HiveServer2CredentialProvider.obtainDelegationTokens(HiveServer2CredentialProvider.scala:64)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager.$anonfun$obtainDelegationTokens$2(HadoopDelegationTokenManager.scala:164)\n",
      "\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat scala.collection.MapLike$DefaultValuesIterable.foreach(MapLike.scala:213)\n",
      "\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)\n",
      "\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)\n",
      "\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager.org$apache$spark$deploy$security$HadoopDelegationTokenManager$$obtainDelegationTokens(HadoopDelegationTokenManager.scala:162)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager$$anon$4.run(HadoopDelegationTokenManager.scala:226)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager$$anon$4.run(HadoopDelegationTokenManager.scala:224)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager.obtainTokensAndScheduleRenewal(HadoopDelegationTokenManager.scala:224)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager.org$apache$spark$deploy$security$HadoopDelegationTokenManager$$updateTokensTask(HadoopDelegationTokenManager.scala:198)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager.start(HadoopDelegationTokenManager.scala:123)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.$anonfun$start$1(CoarseGrainedSchedulerBackend.scala:552)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.$anonfun$start$1$adapted(CoarseGrainedSchedulerBackend.scala:549)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.start(CoarseGrainedSchedulerBackend.scala:549)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.KubernetesClusterSchedulerBackend.start(KubernetesClusterSchedulerBackend.scala:95)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:220)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:581)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/04 22:19:49 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/04 22:19:49 WARN HttpChannel: /jobs/\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/04 22:19:50 WARN ExecutorAllocationManager: Dynamic allocation without a shuffle service is an experimental feature.\n",
      "23/09/04 22:19:53 WARN HiveClientImpl: Detected HiveConf hive.execution.engine is 'tez' and will be reset to 'mr' to disable useless hive logic\n",
      "Hive Session ID = 0f7ac06b-b31b-4eb7-bc9e-bd224aa194f3\n",
      "23/09/04 22:19:55 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|         01_car_data|\n",
      "|           01_car_dw|\n",
      "|              adb101|\n",
      "|            airlines|\n",
      "|        airlines_csv|\n",
      "|    airlines_iceberg|\n",
      "|airlines_iceberg_...|\n",
      "|      airlines_mjain|\n",
      "|          airquality|\n",
      "|                ajvp|\n",
      "|          atlas_demo|\n",
      "|            bankdemo|\n",
      "|          bca_jps_l0|\n",
      "|        cde_workshop|\n",
      "|             cdedemo|\n",
      "|        cdp_overview|\n",
      "|      ceht_open_data|\n",
      "|        ceht_scratch|\n",
      "| ceht_transportation|\n",
      "|        cgsifacebook|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cml.data_v1 as cmldata\n",
    "\n",
    "CONNECTION_NAME = \"go01-aw-dl\"\n",
    "conn = cmldata.get_connection(CONNECTION_NAME)\n",
    "spark = conn.get_spark_session()\n",
    "\n",
    "# Sample usage to run query through spark\n",
    "EXAMPLE_SQL_QUERY = \"show databases\"\n",
    "spark.sql(EXAMPLE_SQL_QUERY).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3bbf1bf-a0a1-4864-8c98-88dfb994060a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 22:19:56 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "23/09/04 22:19:56 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('spark.eventLog.enabled', 'true'),\n",
       " ('spark.network.crypto.enabled', 'true'),\n",
       " ('spark.sql.hive.hwc.execution.mode', 'spark'),\n",
       " ('spark.driver.port', '32809'),\n",
       " ('spark.jars',\n",
       "  '/opt/spark/optional-lib/hive-warehouse-connector-assembly.jar,/opt/spark/optional-lib/iceberg-hive-runtime.jar,/opt/spark/optional-lib/iceberg-spark-runtime.jar'),\n",
       " ('spark.kerberos.renewal.credentials', 'ccache'),\n",
       " ('spark.sql.catalog.spark_catalog',\n",
       "  'org.apache.iceberg.spark.SparkSessionCatalog'),\n",
       " ('spark.dynamicAllocation.maxExecutors', '49'),\n",
       " ('spark.eventLog.dir', 'file:///sparkeventlogs'),\n",
       " ('spark.hadoop.yarn.resourcemanager.principal', 'pauldefusco'),\n",
       " ('spark.kubernetes.driver.annotation.cluster-autoscaler.kubernetes.io/safe-to-evict',\n",
       "  'false'),\n",
       " ('spark.ui.port', '20049'),\n",
       " ('spark.yarn.access.hadoopFileSystems',\n",
       "  's3a://go01-demo/warehouse/tablespace/external/hive'),\n",
       " ('spark.sql.extensions',\n",
       "  'com.qubole.spark.hiveacid.HiveAcidAutoConvertExtension,org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions'),\n",
       " ('spark.kubernetes.executor.annotation.cluster-autoscaler.kubernetes.io/safe-to-evict',\n",
       "  'false'),\n",
       " ('spark.driver.memory', '3051m'),\n",
       " ('spark.io.encryption.enabled', 'true'),\n",
       " ('spark.driver.bindAddress', '100.100.150.190'),\n",
       " ('spark.app.startTime', '1693865984718'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.master', 'k8s://https://172.20.0.1:443'),\n",
       " ('spark.ui.allowFramingFrom',\n",
       "  'https://ml-b74f8940-b97.go01-dem.ylcu-atmi.cloudera.site'),\n",
       " ('spark.kubernetes.executor.podTemplateFile', '/tmp/spark-executor.json'),\n",
       " ('spark.driver.host', '100.100.150.190'),\n",
       " ('spark.kubernetes.container.image',\n",
       "  'docker.repository.cloudera.com/cloudera/cdsw/ml-runtime-jupyterlab-python3.7-standard:2022.11.1-b2'),\n",
       " ('spark.dynamicAllocation.shuffleTracking.enabled', 'true'),\n",
       " ('spark.kubernetes.executor.podNamePrefix', 'cdsw-4q9yjp6u4sb0krh1'),\n",
       " ('spark.kubernetes.namespace', 'mlx-user-16'),\n",
       " ('spark.app.initial.jar.urls',\n",
       "  'spark://100.100.150.190:32809/jars/iceberg-hive-runtime-1.1.0.1.19.7215.0-118.jar,spark://100.100.150.190:32809/jars/iceberg-spark-runtime-3.2_2.12-1.1.0.1.19.7215.0-118.jar,spark://100.100.150.190:32809/jars/hive-warehouse-connector-assembly-1.0.0.1.19.7215.0-118.jar'),\n",
       " ('spark.app.id', 'spark-application-1693865986682'),\n",
       " ('spark.kubernetes.executor.config.dir', '/var/spark/conf'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  's3a://go01-demo/warehouse/tablespace/external/hive'),\n",
       " ('spark.hadoop.iceberg.engine.hive.enabled', 'true'),\n",
       " ('spark.repl.local.jars',\n",
       "  'file:///runtime-addons/spark323-19-hf2-g3aapf/opt/spark/optional-lib/hive-warehouse-connector-assembly-1.0.0.1.19.7215.0-118.jar,file:///runtime-addons/spark323-19-hf2-g3aapf/opt/spark/optional-lib/iceberg-hive-runtime-1.1.0.1.19.7215.0-118.jar,file:///runtime-addons/spark323-19-hf2-g3aapf/opt/spark/optional-lib/iceberg-spark-runtime-3.2_2.12-1.1.0.1.19.7215.0-118.jar'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.ui.proxyRedirectUri',\n",
       "  'https://spark-4q9yjp6u4sb0krh1.ml-b74f8940-b97.go01-dem.ylcu-atmi.cloudera.site'),\n",
       " ('spark.kubernetes.driver.pod.name', '4q9yjp6u4sb0krh1'),\n",
       " ('spark.yarn.rmProxy.enabled', 'false'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.kryo.registrator',\n",
       "  'com.qubole.spark.hiveacid.util.HiveAcidKyroRegistrator'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.dynamicAllocation.enabled', 'true'),\n",
       " ('spark.deploy.mode', 'client'),\n",
       " ('spark.app.name', 'None'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.sql.catalog.spark_catalog.type', 'hive'),\n",
       " ('spark.authenticate', 'true')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa007770-01e4-4c2e-a784-6955c448c952",
   "metadata": {},
   "source": [
    "### Iceberg Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad5615b-c7e6-4528-bd48-db0841d38c39",
   "metadata": {},
   "source": [
    "![alt text](../img/iceberg-metadata.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c3c93c-4065-422b-9697-609ae50687c7",
   "metadata": {},
   "source": [
    "#### Iceberg Catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30001dd5-c3ef-4223-84ca-e63552c4f879",
   "metadata": {},
   "source": [
    "Iceberg comes with catalogs that enable SQL commands to manage tables and load them by name. Catalogs are configured using properties under spark.sql.catalog.(catalog_name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3819a08-8e00-4184-bdc6-79b3d8507d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+\n",
      "|      catalog|namespace|\n",
      "+-------------+---------+\n",
      "|spark_catalog|  default|\n",
      "+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show catalog and database\n",
    "spark.sql(\"SHOW CURRENT NAMESPACE\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1119faa-93a7-4ebe-85b1-3ec87eb54b7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new database\n",
    "#spark.sql(\"DROP DATABASE IF EXISTS spark_catalog.lakehouse\")\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS spark_catalog.lakehouse\")\n",
    "spark.sql(\"USE spark_catalog.lakehouse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da3225e2-a19a-471c-aaed-8f4b8f833b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"USE spark_catalog.lakehouse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "834ca50c-9c2f-43ac-9ec0-9cf28e8507d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+\n",
      "|      catalog|namespace|\n",
      "+-------------+---------+\n",
      "|spark_catalog|lakehouse|\n",
      "+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show catalog and database\n",
    "spark.sql(\"SHOW CURRENT NAMESPACE\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6cecda-848a-4cbe-b7a8-d73096cab11f",
   "metadata": {},
   "source": [
    "#### Create an Iceberg Table with Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c5daa62-cde3-440a-ad91-0c6f9b41ef3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 22:19:56 WARN HiveMetaStoreClient: Failed to connect to the MetaStore Server...\n",
      "23/09/04 22:20:14 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS lakehouse.coffees_table PURGE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b10e6c-efcc-46b5-a164-893879b79327",
   "metadata": {},
   "source": [
    "# TEST 1 with COW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b19e0de9-29c4-43dd-ae5c-c2bd69558032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE IF NOT EXISTS coffees_table (coffee_id BIGINT, coffee_size STRING, coffee_sale_ts TIMESTAMP)\\\n",
    "          USING ICEBERG\\\n",
    "          PARTITIONED BY (months(coffee_sale_ts))\\\n",
    "          TBLPROPERTIES ('write.delete.mode'='copy-on-write',\\\n",
    "                          'write.update.mode'='copy-on-write',\\\n",
    "                          'write.merge.mode'='copy-on-write',\\\n",
    "                          'format-version' = '2')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eea95e7-e221-4997-9071-208cbc9f9fc5",
   "metadata": {},
   "source": [
    "#### Verify that a Metadata JSON file has been created under the Metadata directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d865393-0703-481d-b696-5c23d31bf901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CatalogUtil import CatalogUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab7b270a-d85d-4fcd-91ad-3341e373f469",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogUtil = CatalogUtil(\"lakehouse\", \"coffees_table\", \"go01-demo\", \"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aecdf06b-74f6-42a7-84aa-021acfaeca56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are now the files in the Iceberg Metadata Layer:\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00000-30b3f58c-b3d7-4cc5-8f03-2d3a6ada2c11.metadata.json\n",
      "Total number of files in the Metadata Layer: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"These are now the files in the Iceberg Metadata Layer:\")\n",
    "catalogUtil.count_metadata_layer_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc42e771-a996-4ff0-bf11-43a892727783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are now the metadata files in the Iceberg Metadata Layer:\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00000-30b3f58c-b3d7-4cc5-8f03-2d3a6ada2c11.metadata.json\n",
      "Total Number of Metadata Files: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"These are now the metadata files in the Iceberg Metadata Layer:\")\n",
    "catalogUtil.count_metadata_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69344686-048d-4385-a8cc-5e8b550a06bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are now the manifest lists in the Iceberg Metadata Layer:\n",
      "Total Number of Manifest Lists: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"These are now the manifest lists in the Iceberg Metadata Layer:\")\n",
    "catalogUtil.count_manifest_lists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4208a40-cc86-4d5e-9b1e-585f0531eef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are now the manifest files in the Iceberg Metadata Layer:\n",
      "Total Number of Manifest Files: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"These are now the manifest files in the Iceberg Metadata Layer:\")\n",
    "catalogUtil.count_manifest_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdb926a4-2635-41c3-86e0-35e7c1e14507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00000-30b3f58c-b3d7-4cc5-8f03-2d3a6ada2c11.metadata.json\n"
     ]
    }
   ],
   "source": [
    "metadata_file_path = catalogUtil.get_latest_metadata_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50b5b013-3ad4-4e2e-a3f6-85bc072c1f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/IPython/core/display.py:911: UserWarning: JSON expects JSONable dict or list, not JSON strings\n",
      "  warnings.warn(\"JSON expects JSONable dict or list, not JSON strings\")\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "current-schema-id": 0,
       "current-snapshot-id": -1,
       "default-sort-order-id": 0,
       "default-spec-id": 0,
       "format-version": 2,
       "last-column-id": 3,
       "last-partition-id": 1000,
       "last-sequence-number": 0,
       "last-updated-ms": 1693866026299,
       "location": "s3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table",
       "metadata-log": [],
       "partition-specs": [
        {
         "fields": [
          {
           "field-id": 1000,
           "name": "coffee_sale_ts_month",
           "source-id": 3,
           "transform": "month"
          }
         ],
         "spec-id": 0
        }
       ],
       "properties": {
        "owner": "pauldefusco",
        "write.delete.mode": "copy-on-write",
        "write.merge.mode": "copy-on-write",
        "write.update.mode": "copy-on-write"
       },
       "schemas": [
        {
         "fields": [
          {
           "id": 1,
           "name": "coffee_id",
           "required": false,
           "type": "long"
          },
          {
           "id": 2,
           "name": "coffee_size",
           "required": false,
           "type": "string"
          },
          {
           "id": 3,
           "name": "coffee_sale_ts",
           "required": false,
           "type": "timestamptz"
          }
         ],
         "schema-id": 0,
         "type": "struct"
        }
       ],
       "snapshot-log": [],
       "snapshots": [],
       "sort-orders": [
        {
         "fields": [],
         "order-id": 0
        }
       ],
       "statistics": [],
       "table-uuid": "54ffc775-9282-4b00-a33d-e2a2acf12ade"
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalogUtil.print_metadata_file(spark, metadata_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357d7e23-9be4-4b92-bbb0-cb69da6f1476",
   "metadata": {},
   "source": [
    "![alt text](../img/s3_metadata.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f6b96e-d878-4e2e-91a0-1ef785833479",
   "metadata": {},
   "source": [
    "#### Notice that no snapshots or other files have been created as data has not yet been inserted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "972dced2-83a3-4c4f-a63d-2936a54534d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+---------+-------------------+\n",
      "|made_current_at|snapshot_id|parent_id|is_current_ancestor|\n",
      "+---------------+-----------+---------+-------------------+\n",
      "+---------------+-----------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM lakehouse.coffees_table.history\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d726f825-38b0-4342-a0ea-0f932ce20355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+---------+---------+-------------+-------+\n",
      "|committed_at|snapshot_id|parent_id|operation|manifest_list|summary|\n",
      "+------------+-----------+---------+---------+-------------+-------+\n",
      "+------------+-----------+---------+---------+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM lakehouse.coffees_table.snapshots;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68f5ad55-0adb-4ab3-8574-da44c3af6b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----------+-------+---------+------------+------------------+------------+------------+-----------------+----------------+------------+------------+------------+-------------+------------+-------------+\n",
      "|content|file_path|file_format|spec_id|partition|record_count|file_size_in_bytes|column_sizes|value_counts|null_value_counts|nan_value_counts|lower_bounds|upper_bounds|key_metadata|split_offsets|equality_ids|sort_order_id|\n",
      "+-------+---------+-----------+-------+---------+------------+------------------+------------+------------+-----------------+----------------+------------+------------+------------+-------------+------------+-------------+\n",
      "+-------+---------+-----------+-------+---------+------------+------------------+------------+------------+-----------------+----------------+------------+------------+------------+-------------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM lakehouse.coffees_table.files;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "636820a4-528b-42f2-9153-2180206420de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+-----------------+-----------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+-------------------+\n",
      "|content|path|length|partition_spec_id|added_snapshot_id|added_data_files_count|existing_data_files_count|deleted_data_files_count|added_delete_files_count|existing_delete_files_count|deleted_delete_files_count|partition_summaries|\n",
      "+-------+----+------+-----------------+-----------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+-------------------+\n",
      "+-------+----+------+-----------------+-----------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM lakehouse.coffees_table.manifests;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8544499-8ba1-4156-b641-89c10c281926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----------+-------+---------+------------+------------------+------------+------------+-----------------+----------------+------------+------------+------------+-------------+------------+-------------+\n",
      "|content|file_path|file_format|spec_id|partition|record_count|file_size_in_bytes|column_sizes|value_counts|null_value_counts|nan_value_counts|lower_bounds|upper_bounds|key_metadata|split_offsets|equality_ids|sort_order_id|\n",
      "+-------+---------+-----------+-------+---------+------------+------------------+------------+------------+-----------------+----------------+------------+------------+------------+-------------+------------+-------------+\n",
      "+-------+---------+-----------+-------+---------+------------+------------------+------------+------------+-----------------+----------------+------------+------------+------------+-------------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM lakehouse.coffees_table.all_data_files;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "323639fc-1ed3-418d-ae0e-8c1674a30ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+-----------------+-----------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+-------------------+---------------------+\n",
      "|content|path|length|partition_spec_id|added_snapshot_id|added_data_files_count|existing_data_files_count|deleted_data_files_count|added_delete_files_count|existing_delete_files_count|deleted_delete_files_count|partition_summaries|reference_snapshot_id|\n",
      "+-------+----+------+-----------------+-----------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+-------------------+---------------------+\n",
      "+-------+----+------+-----------------+-----------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+-------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM lakehouse.coffees_table.all_manifests;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24046132-45da-4f28-a60b-f0227b8fb7cc",
   "metadata": {},
   "source": [
    "### Table Insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "028f245c-e167-4418-9e96-fd098e0cb6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f2d7725-fe94-4a8c-82d0-319c1cefef8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Coffee_id = 1, Coffee_size = venti, coffee_sale_ts = 2023-09-01\n",
    "\n",
    "spark.sql(\"INSERT INTO lakehouse.coffees_table VALUES (1, 'venti', cast(date_format('2023-09-01 10:00:00', 'yyyy-MM-dd HH:mm:ss') as timestamp))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f0d668-9e68-4b1c-a881-5d1265f999c7",
   "metadata": {},
   "source": [
    "#### Data has been added to the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da0242b8-93b9-4471-9aae-7a8f162a2320",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = \"select h.made_current_at,\\\n",
    "            s.operation,\\\n",
    "            h.snapshot_id,\\\n",
    "            h.is_current_ancestor,\\\n",
    "            s.summary['spark.app.id']\\\n",
    "        from lakehouse.coffees_table.history h\\\n",
    "        join lakehouse.coffees_table.snapshots s\\\n",
    "            on h.snapshot_id = s.snapshot_id\\\n",
    "            order by made_current_at;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "76eaa58b-c77b-437a-979d-feaa4ac9b536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 22:20:42 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>made_current_at</th>\n",
       "      <th>operation</th>\n",
       "      <th>snapshot_id</th>\n",
       "      <th>is_current_ancestor</th>\n",
       "      <th>summary[spark.app.id]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-09-04 22:20:37.323</td>\n",
       "      <td>append</td>\n",
       "      <td>7462507546055427409</td>\n",
       "      <td>True</td>\n",
       "      <td>spark-application-1693865986682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          made_current_at operation          snapshot_id  is_current_ancestor  \\\n",
       "0 2023-09-04 22:20:37.323    append  7462507546055427409                 True   \n",
       "\n",
       "             summary[spark.app.id]  \n",
       "0  spark-application-1693865986682  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(QUERY).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3728c7dc-ac75-4859-8790-501210b43381",
   "metadata": {},
   "source": [
    "#### Notice there are now two json files and two avro files. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44f25b0-3aea-4b67-a5e5-bed3bf2b6a9d",
   "metadata": {},
   "source": [
    "The first json file is the metadata file created when the table was created. This is the metata file prefixed by 00000. The second json file is the new metadata file reflecting the insert of one row. This is the metadata file prefixed by 00001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3519f9f-c58e-46ca-875a-9bb3d3a2c180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are now the files in the Iceberg Metadata Layer:\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00000-30b3f58c-b3d7-4cc5-8f03-2d3a6ada2c11.metadata.json\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00001-7d5f111e-852b-4cae-bfa3-a574cdb56a14.metadata.json\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/ee02a218-f8f4-449c-8781-ce82b0450e0c-m0.avro\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/snap-7462507546055427409-1-ee02a218-f8f4-449c-8781-ce82b0450e0c.avro\n",
      "Total number of files in the Metadata Layer: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"These are now the files in the Iceberg Metadata Layer:\")\n",
    "catalogUtil.count_metadata_layer_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c76a7a4e-dbbb-489a-b233-c70c247d98ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are now the metadata files in the Iceberg Metadata Layer:\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00000-30b3f58c-b3d7-4cc5-8f03-2d3a6ada2c11.metadata.json\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00001-7d5f111e-852b-4cae-bfa3-a574cdb56a14.metadata.json\n",
      "Total Number of Metadata Files: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"These are now the metadata files in the Iceberg Metadata Layer:\")\n",
    "catalogUtil.count_metadata_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd378253-2cde-440c-b559-53429572228e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are now the manifest lists in the Iceberg Metadata Layer:\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/snap-7462507546055427409-1-ee02a218-f8f4-449c-8781-ce82b0450e0c.avro\n",
      "Total Number of Manifest Lists: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"These are now the manifest lists in the Iceberg Metadata Layer:\")\n",
    "catalogUtil.count_manifest_lists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f08951aa-6207-4eef-9cbc-4fc186368070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are now the manifest files in the Iceberg Metadata Layer:\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/ee02a218-f8f4-449c-8781-ce82b0450e0c-m0.avro\n",
      "Total Number of Manifest Files: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"These are now the manifest files in the Iceberg Metadata Layer:\")\n",
    "catalogUtil.count_manifest_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b6d6560-93d4-4e18-9610-50805e74d471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00000-30b3f58c-b3d7-4cc5-8f03-2d3a6ada2c11.metadata.json\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00001-7d5f111e-852b-4cae-bfa3-a574cdb56a14.metadata.json\n"
     ]
    }
   ],
   "source": [
    "metadata_file_path_list = catalogUtil.get_all_metadata_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "236ad68d-8741-4e44-a3b3-496e1d7eff81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/IPython/core/display.py:911: UserWarning: JSON expects JSONable dict or list, not JSON strings\n",
      "  warnings.warn(\"JSON expects JSONable dict or list, not JSON strings\")\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "current-schema-id": 0,
       "current-snapshot-id": -1,
       "default-sort-order-id": 0,
       "default-spec-id": 0,
       "format-version": 2,
       "last-column-id": 3,
       "last-partition-id": 1000,
       "last-sequence-number": 0,
       "last-updated-ms": 1693866026299,
       "location": "s3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table",
       "metadata-log": [],
       "partition-specs": [
        {
         "fields": [
          {
           "field-id": 1000,
           "name": "coffee_sale_ts_month",
           "source-id": 3,
           "transform": "month"
          }
         ],
         "spec-id": 0
        }
       ],
       "properties": {
        "owner": "pauldefusco",
        "write.delete.mode": "copy-on-write",
        "write.merge.mode": "copy-on-write",
        "write.update.mode": "copy-on-write"
       },
       "schemas": [
        {
         "fields": [
          {
           "id": 1,
           "name": "coffee_id",
           "required": false,
           "type": "long"
          },
          {
           "id": 2,
           "name": "coffee_size",
           "required": false,
           "type": "string"
          },
          {
           "id": 3,
           "name": "coffee_sale_ts",
           "required": false,
           "type": "timestamptz"
          }
         ],
         "schema-id": 0,
         "type": "struct"
        }
       ],
       "snapshot-log": [],
       "snapshots": [],
       "sort-orders": [
        {
         "fields": [],
         "order-id": 0
        }
       ],
       "statistics": [],
       "table-uuid": "54ffc775-9282-4b00-a33d-e2a2acf12ade"
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalogUtil.print_metadata_file(spark, metadata_file_path_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1e7891d0-b323-42e5-b47a-fe1c98f22e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "application/json": {
       "current-schema-id": 0,
       "current-snapshot-id": 7462507546055427000,
       "default-sort-order-id": 0,
       "default-spec-id": 0,
       "format-version": 2,
       "last-column-id": 3,
       "last-partition-id": 1000,
       "last-sequence-number": 1,
       "last-updated-ms": 1693866037323,
       "location": "s3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table",
       "metadata-log": [
        {
         "metadata-file": "s3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00000-30b3f58c-b3d7-4cc5-8f03-2d3a6ada2c11.metadata.json",
         "timestamp-ms": 1693866026299
        }
       ],
       "partition-specs": [
        {
         "fields": [
          {
           "field-id": 1000,
           "name": "coffee_sale_ts_month",
           "source-id": 3,
           "transform": "month"
          }
         ],
         "spec-id": 0
        }
       ],
       "properties": {
        "owner": "pauldefusco",
        "write.delete.mode": "copy-on-write",
        "write.merge.mode": "copy-on-write",
        "write.update.mode": "copy-on-write"
       },
       "refs": {
        "main": {
         "snapshot-id": 7462507546055427000,
         "type": "branch"
        }
       },
       "schemas": [
        {
         "fields": [
          {
           "id": 1,
           "name": "coffee_id",
           "required": false,
           "type": "long"
          },
          {
           "id": 2,
           "name": "coffee_size",
           "required": false,
           "type": "string"
          },
          {
           "id": 3,
           "name": "coffee_sale_ts",
           "required": false,
           "type": "timestamptz"
          }
         ],
         "schema-id": 0,
         "type": "struct"
        }
       ],
       "snapshot-log": [
        {
         "snapshot-id": 7462507546055427000,
         "timestamp-ms": 1693866037323
        }
       ],
       "snapshots": [
        {
         "manifest-list": "s3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/snap-7462507546055427409-1-ee02a218-f8f4-449c-8781-ce82b0450e0c.avro",
         "schema-id": 0,
         "sequence-number": 1,
         "snapshot-id": 7462507546055427000,
         "summary": {
          "added-data-files": "1",
          "added-files-size": "976",
          "added-records": "1",
          "changed-partition-count": "1",
          "operation": "append",
          "spark.app.id": "spark-application-1693865986682",
          "total-data-files": "1",
          "total-delete-files": "0",
          "total-equality-deletes": "0",
          "total-files-size": "976",
          "total-position-deletes": "0",
          "total-records": "1"
         },
         "timestamp-ms": 1693866037323
        }
       ],
       "sort-orders": [
        {
         "fields": [],
         "order-id": 0
        }
       ],
       "statistics": [],
       "table-uuid": "54ffc775-9282-4b00-a33d-e2a2acf12ade"
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalogUtil.print_metadata_file(spark, metadata_file_path_list[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f46899f-2ecd-4651-9dcf-7e6c4d912273",
   "metadata": {},
   "source": [
    "The Avro file with the \"snap\" prefix is the manifest list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "08d79d77-e537-4b3a-919d-2ea5514559e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/snap-7462507546055427409-1-ee02a218-f8f4-449c-8781-ce82b0450e0c.avro\n"
     ]
    }
   ],
   "source": [
    "manifest_lists_list = catalogUtil.get_all_manifest_lists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9f499d0a-421f-461f-85dd-16e565c2f54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "application/json": {
       "added_data_files_count": 1,
       "added_rows_count": 1,
       "added_snapshot_id": 7462507546055427000,
       "content": 0,
       "deleted_data_files_count": 0,
       "deleted_rows_count": 0,
       "existing_data_files_count": 0,
       "existing_rows_count": 0,
       "manifest_length": 7074,
       "manifest_path": "s3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/ee02a218-f8f4-449c-8781-ce82b0450e0c-m0.avro",
       "min_sequence_number": 1,
       "partition_spec_id": 0,
       "partitions": [
        {
         "contains_nan": false,
         "contains_null": false,
         "lower_bound": "hAIAAA==",
         "upper_bound": "hAIAAA=="
        }
       ],
       "sequence_number": 1
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalogUtil.print_manifest_file(spark, manifest_lists_list[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e77b3e-81e4-4c29-b6d3-6e6db6f6edad",
   "metadata": {},
   "source": [
    "The Avro file without the \"snap\" prefix is the manifest file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "828e4eae-8499-4e6a-b466-01f86e64ea28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/ee02a218-f8f4-449c-8781-ce82b0450e0c-m0.avro\n"
     ]
    }
   ],
   "source": [
    "manifest_files_list = catalogUtil.get_all_manifest_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "83fc8914-b20f-44c3-928e-98243dc0fe4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "application/json": {
       "data_file": {
        "column_sizes": [
         {
          "key": 1,
          "value": 33
         },
         {
          "key": 2,
          "value": 34
         },
         {
          "key": 3,
          "value": 39
         }
        ],
        "content": 0,
        "file_format": "PARQUET",
        "file_path": "s3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table/data/coffee_sale_ts_month=2023-09/00072-211-d8611c4f-62fa-464e-b998-1dccb751e0f5-00001.parquet",
        "file_size_in_bytes": 976,
        "lower_bounds": [
         {
          "key": 1,
          "value": "AQAAAAAAAAA="
         },
         {
          "key": 2,
          "value": "dmVudGk="
         },
         {
          "key": 3,
          "value": "AMhBOUkEBgA="
         }
        ],
        "nan_value_counts": [],
        "null_value_counts": [
         {
          "key": 1,
          "value": 0
         },
         {
          "key": 2,
          "value": 0
         },
         {
          "key": 3,
          "value": 0
         }
        ],
        "partition": {
         "coffee_sale_ts_month": 644
        },
        "record_count": 1,
        "sort_order_id": 0,
        "split_offsets": [
         4
        ],
        "upper_bounds": [
         {
          "key": 1,
          "value": "AQAAAAAAAAA="
         },
         {
          "key": 2,
          "value": "dmVudGk="
         },
         {
          "key": 3,
          "value": "AMhBOUkEBgA="
         }
        ],
        "value_counts": [
         {
          "key": 1,
          "value": 1
         },
         {
          "key": 2,
          "value": 1
         },
         {
          "key": 3,
          "value": 1
         }
        ]
       },
       "snapshot_id": 7462507546055427000,
       "status": 1
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalogUtil.print_manifest_file(spark, manifest_files_list[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c8c364dd-1dd8-4ce4-bd3b-2b38ed7cb070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/data/coffee_sale_ts_month=2023-09/00072-211-d8611c4f-62fa-464e-b998-1dccb751e0f5-00001.parquet\n"
     ]
    }
   ],
   "source": [
    "data_files_list = catalogUtil.get_all_data_layer_files()\n",
    "# could also query the all_data_files metadata table:\n",
    "# spark.sql(\"SELECT * FROM lakehouse.coffees_table.all_data_files;\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb91747c-9f0b-46cc-b363-fc134d97a9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------------+\n",
      "|coffee_id|coffee_size|     coffee_sale_ts|\n",
      "+---------+-----------+-------------------+\n",
      "|        1|      venti|2023-09-01 10:00:00|\n",
      "+---------+-----------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "catalogUtil.print_data_file(spark, data_files_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f023c1ca-48c8-4d4d-b329-4825d473bf4b",
   "metadata": {},
   "source": [
    "### Table Merge Into"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bef6e0-6ea1-4f75-aeac-fd84b0b7f2f7",
   "metadata": {},
   "source": [
    "Create a staging table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a8287cfa-9f8e-40fb-a880-a2dabcdbd2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS lakehouse.coffee_staging PURGE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1a742c99-a211-4fa6-bdd5-e63f8079f9d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE IF NOT EXISTS lakehouse.coffee_staging\\\n",
    "            (coffee_id BIGINT, coffee_size STRING, coffee_sale_ts TIMESTAMP)\\\n",
    "            USING iceberg\\\n",
    "            PARTITIONED BY (months(coffee_sale_ts))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d4dc4207-34e0-4bb3-a147-dfcd4e601990",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"INSERT INTO lakehouse.coffee_staging\\\n",
    "            VALUES (1, 'grande', cast(date_format('2023-07-01 11:00:00', 'yyyy-MM-dd HH:mm:ss') as timestamp)),\\\n",
    "            (2, 'grande', cast(date_format('2023-07-01 11:10:00', 'yyyy-MM-dd HH:mm:ss') as timestamp)),\\\n",
    "            (3, 'tall', cast(date_format('2023-04-01 12:01:00', 'yyyy-MM-dd HH:mm:ss') as timestamp))\")\n",
    "\n",
    "#Row: Coffee_id = 1, coffee_size = venti, coffee_sale_ts = 2023-07-01\n",
    "#Row: Coffee_id = 2, coffee_size = grande, coffee_sale_ts = 2023-07-01\n",
    "#Row: Coffee_id = 3, coffee_size = tall, coffee_sale_ts = 2023-04-01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c3720c-3011-4a15-924f-de39a525501d",
   "metadata": {},
   "source": [
    "Merge Into Customers Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4b62e634-935e-4219-b240-fdb8411b2984",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 22:21:22 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"MERGE INTO lakehouse.coffees_table c\\\n",
    "            USING (SELECT * FROM lakehouse.coffee_staging) s\\\n",
    "            ON c.coffee_id = s.coffee_id \\\n",
    "            WHEN MATCHED THEN UPDATE SET c.coffee_size = s.coffee_size\\\n",
    "            WHEN NOT MATCHED THEN INSERT *\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c94305e6-c808-4d67-8903-864a4d80b5dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>committed_at</th>\n",
       "      <th>snapshot_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>operation</th>\n",
       "      <th>manifest_list</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-09-04 22:20:37.323</td>\n",
       "      <td>7462507546055427409</td>\n",
       "      <td>NaN</td>\n",
       "      <td>append</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>{'spark.app.id': 'spark-application-1693865986...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-09-04 22:21:30.476</td>\n",
       "      <td>7257983078605640789</td>\n",
       "      <td>7.462508e+18</td>\n",
       "      <td>overwrite</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>{'added-data-files': '3', 'total-equality-dele...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             committed_at          snapshot_id     parent_id  operation  \\\n",
       "0 2023-09-04 22:20:37.323  7462507546055427409           NaN     append   \n",
       "1 2023-09-04 22:21:30.476  7257983078605640789  7.462508e+18  overwrite   \n",
       "\n",
       "                                       manifest_list  \\\n",
       "0  s3a://go01-demo/warehouse/tablespace/external/...   \n",
       "1  s3a://go01-demo/warehouse/tablespace/external/...   \n",
       "\n",
       "                                             summary  \n",
       "0  {'spark.app.id': 'spark-application-1693865986...  \n",
       "1  {'added-data-files': '3', 'total-equality-dele...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM lakehouse.coffees_table.snapshots;\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2d2d2db3-6d38-428e-bc8d-51286ff2e11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>path</th>\n",
       "      <th>length</th>\n",
       "      <th>partition_spec_id</th>\n",
       "      <th>added_snapshot_id</th>\n",
       "      <th>added_data_files_count</th>\n",
       "      <th>existing_data_files_count</th>\n",
       "      <th>deleted_data_files_count</th>\n",
       "      <th>added_delete_files_count</th>\n",
       "      <th>existing_delete_files_count</th>\n",
       "      <th>deleted_delete_files_count</th>\n",
       "      <th>partition_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>7210</td>\n",
       "      <td>0</td>\n",
       "      <td>7257983078605640789</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(False, False, 2023-04, 2023-09)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>7071</td>\n",
       "      <td>0</td>\n",
       "      <td>7257983078605640789</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(False, False, 2023-09, 2023-09)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   content                                               path  length  \\\n",
       "0        0  s3a://go01-demo/warehouse/tablespace/external/...    7210   \n",
       "1        0  s3a://go01-demo/warehouse/tablespace/external/...    7071   \n",
       "\n",
       "   partition_spec_id    added_snapshot_id  added_data_files_count  \\\n",
       "0                  0  7257983078605640789                       3   \n",
       "1                  0  7257983078605640789                       0   \n",
       "\n",
       "   existing_data_files_count  deleted_data_files_count  \\\n",
       "0                          0                         0   \n",
       "1                          0                         1   \n",
       "\n",
       "   added_delete_files_count  existing_delete_files_count  \\\n",
       "0                         0                            0   \n",
       "1                         0                            0   \n",
       "\n",
       "   deleted_delete_files_count                 partition_summaries  \n",
       "0                           0  [(False, False, 2023-04, 2023-09)]  \n",
       "1                           0  [(False, False, 2023-09, 2023-09)]  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM lakehouse.coffees_table.manifests;\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "db59e20d-b4fa-455c-8a34-b5538d06b2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>file_path</th>\n",
       "      <th>file_format</th>\n",
       "      <th>spec_id</th>\n",
       "      <th>partition</th>\n",
       "      <th>record_count</th>\n",
       "      <th>file_size_in_bytes</th>\n",
       "      <th>column_sizes</th>\n",
       "      <th>value_counts</th>\n",
       "      <th>null_value_counts</th>\n",
       "      <th>nan_value_counts</th>\n",
       "      <th>lower_bounds</th>\n",
       "      <th>upper_bounds</th>\n",
       "      <th>key_metadata</th>\n",
       "      <th>split_offsets</th>\n",
       "      <th>equality_ids</th>\n",
       "      <th>sort_order_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>PARQUET</td>\n",
       "      <td>0</td>\n",
       "      <td>(644,)</td>\n",
       "      <td>1</td>\n",
       "      <td>976</td>\n",
       "      <td>{1: 33, 2: 34, 3: 39}</td>\n",
       "      <td>{1: 1, 2: 1, 3: 1}</td>\n",
       "      <td>{1: 0, 2: 0, 3: 0}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{1: [1, 0, 0, 0, 0, 0, 0, 0], 2: [118, 101, 11...</td>\n",
       "      <td>{1: [1, 0, 0, 0, 0, 0, 0, 0], 2: [118, 101, 11...</td>\n",
       "      <td>None</td>\n",
       "      <td>[4]</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>PARQUET</td>\n",
       "      <td>0</td>\n",
       "      <td>(642,)</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>{1: 39, 2: 41, 3: 39}</td>\n",
       "      <td>{1: 1, 2: 1, 3: 1}</td>\n",
       "      <td>{1: 0, 2: 0, 3: 0}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{1: [2, 0, 0, 0, 0, 0, 0, 0], 2: [103, 114, 97...</td>\n",
       "      <td>{1: [2, 0, 0, 0, 0, 0, 0, 0], 2: [103, 114, 97...</td>\n",
       "      <td>None</td>\n",
       "      <td>[4]</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>PARQUET</td>\n",
       "      <td>0</td>\n",
       "      <td>(644,)</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>{1: 39, 2: 41, 3: 39}</td>\n",
       "      <td>{1: 1, 2: 1, 3: 1}</td>\n",
       "      <td>{1: 0, 2: 0, 3: 0}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{1: [1, 0, 0, 0, 0, 0, 0, 0], 2: [103, 114, 97...</td>\n",
       "      <td>{1: [1, 0, 0, 0, 0, 0, 0, 0], 2: [103, 114, 97...</td>\n",
       "      <td>None</td>\n",
       "      <td>[4]</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>PARQUET</td>\n",
       "      <td>0</td>\n",
       "      <td>(639,)</td>\n",
       "      <td>1</td>\n",
       "      <td>985</td>\n",
       "      <td>{1: 39, 2: 39, 3: 39}</td>\n",
       "      <td>{1: 1, 2: 1, 3: 1}</td>\n",
       "      <td>{1: 0, 2: 0, 3: 0}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{1: [3, 0, 0, 0, 0, 0, 0, 0], 2: [116, 97, 108...</td>\n",
       "      <td>{1: [3, 0, 0, 0, 0, 0, 0, 0], 2: [116, 97, 108...</td>\n",
       "      <td>None</td>\n",
       "      <td>[4]</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   content                                          file_path file_format  \\\n",
       "0        0  s3a://go01-demo/warehouse/tablespace/external/...     PARQUET   \n",
       "1        0  s3a://go01-demo/warehouse/tablespace/external/...     PARQUET   \n",
       "2        0  s3a://go01-demo/warehouse/tablespace/external/...     PARQUET   \n",
       "3        0  s3a://go01-demo/warehouse/tablespace/external/...     PARQUET   \n",
       "\n",
       "   spec_id partition  record_count  file_size_in_bytes           column_sizes  \\\n",
       "0        0    (644,)             1                 976  {1: 33, 2: 34, 3: 39}   \n",
       "1        0    (642,)             1                 999  {1: 39, 2: 41, 3: 39}   \n",
       "2        0    (644,)             1                 999  {1: 39, 2: 41, 3: 39}   \n",
       "3        0    (639,)             1                 985  {1: 39, 2: 39, 3: 39}   \n",
       "\n",
       "         value_counts   null_value_counts nan_value_counts  \\\n",
       "0  {1: 1, 2: 1, 3: 1}  {1: 0, 2: 0, 3: 0}               {}   \n",
       "1  {1: 1, 2: 1, 3: 1}  {1: 0, 2: 0, 3: 0}               {}   \n",
       "2  {1: 1, 2: 1, 3: 1}  {1: 0, 2: 0, 3: 0}               {}   \n",
       "3  {1: 1, 2: 1, 3: 1}  {1: 0, 2: 0, 3: 0}               {}   \n",
       "\n",
       "                                        lower_bounds  \\\n",
       "0  {1: [1, 0, 0, 0, 0, 0, 0, 0], 2: [118, 101, 11...   \n",
       "1  {1: [2, 0, 0, 0, 0, 0, 0, 0], 2: [103, 114, 97...   \n",
       "2  {1: [1, 0, 0, 0, 0, 0, 0, 0], 2: [103, 114, 97...   \n",
       "3  {1: [3, 0, 0, 0, 0, 0, 0, 0], 2: [116, 97, 108...   \n",
       "\n",
       "                                        upper_bounds key_metadata  \\\n",
       "0  {1: [1, 0, 0, 0, 0, 0, 0, 0], 2: [118, 101, 11...         None   \n",
       "1  {1: [2, 0, 0, 0, 0, 0, 0, 0], 2: [103, 114, 97...         None   \n",
       "2  {1: [1, 0, 0, 0, 0, 0, 0, 0], 2: [103, 114, 97...         None   \n",
       "3  {1: [3, 0, 0, 0, 0, 0, 0, 0], 2: [116, 97, 108...         None   \n",
       "\n",
       "  split_offsets equality_ids  sort_order_id  \n",
       "0           [4]         None              0  \n",
       "1           [4]         None              0  \n",
       "2           [4]         None              0  \n",
       "3           [4]         None              0  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM lakehouse.coffees_table.all_data_files;\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67491c12-86aa-4ced-acf4-8926bfedd947",
   "metadata": {},
   "source": [
    "RECAP: The target table coffee was empty. We ran an insert and that created one new metadata file, one new manifest file, and one new data file.\n",
    "Then we ran a Merge Into from the staging table. Of the three rows that we compared against, one was a match and two were not.\n",
    "The one that was a match was rewritten to a brand new data file in the same partition (coffee_sale_ts_month=2023-09) which now has two data files.\n",
    "The remaining two rows that were not a match were written to two new data files in the two respective partitions. If they had fallen under the same partition they may have been written into the same data file (next example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1e53fd83-0c3f-4049-8b49-9563596a0feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are now the files in the Iceberg Metadata Layer:\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00000-30b3f58c-b3d7-4cc5-8f03-2d3a6ada2c11.metadata.json\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00001-7d5f111e-852b-4cae-bfa3-a574cdb56a14.metadata.json\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00002-3512e62e-a490-4057-802c-09060baa6911.metadata.json\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/cf556cc3-2af0-4bd8-9228-d4c11755e0b7-m0.avro\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/cf556cc3-2af0-4bd8-9228-d4c11755e0b7-m1.avro\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/ee02a218-f8f4-449c-8781-ce82b0450e0c-m0.avro\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/snap-7257983078605640789-1-cf556cc3-2af0-4bd8-9228-d4c11755e0b7.avro\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/snap-7462507546055427409-1-ee02a218-f8f4-449c-8781-ce82b0450e0c.avro\n",
      "Total number of files in the Metadata Layer: 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"These are now the files in the Iceberg Metadata Layer:\")\n",
    "catalogUtil.count_metadata_layer_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8061e9ab-274f-4dea-a4ae-82557ff80ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are now the metadata files in the Iceberg Metadata Layer:\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00000-30b3f58c-b3d7-4cc5-8f03-2d3a6ada2c11.metadata.json\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00001-7d5f111e-852b-4cae-bfa3-a574cdb56a14.metadata.json\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00002-3512e62e-a490-4057-802c-09060baa6911.metadata.json\n",
      "Total Number of Metadata Files: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"These are now the metadata files in the Iceberg Metadata Layer:\")\n",
    "catalogUtil.count_metadata_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c0f25040-ca4a-47d9-a4a4-b1041c0c8894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are now the manifest lists in the Iceberg Metadata Layer:\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/snap-7257983078605640789-1-cf556cc3-2af0-4bd8-9228-d4c11755e0b7.avro\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/snap-7462507546055427409-1-ee02a218-f8f4-449c-8781-ce82b0450e0c.avro\n",
      "Total Number of Manifest Lists: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"These are now the manifest lists in the Iceberg Metadata Layer:\")\n",
    "catalogUtil.count_manifest_lists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b8c74805-0b51-4879-861e-3bbca8698479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are now the manifest files in the Iceberg Metadata Layer:\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/cf556cc3-2af0-4bd8-9228-d4c11755e0b7-m0.avro\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/cf556cc3-2af0-4bd8-9228-d4c11755e0b7-m1.avro\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/ee02a218-f8f4-449c-8781-ce82b0450e0c-m0.avro\n",
      "Total Number of Manifest Files: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"These are now the manifest files in the Iceberg Metadata Layer:\")\n",
    "catalogUtil.count_manifest_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3015a87e-cd0d-4303-b3b9-f424ee1cba8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00000-30b3f58c-b3d7-4cc5-8f03-2d3a6ada2c11.metadata.json\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00001-7d5f111e-852b-4cae-bfa3-a574cdb56a14.metadata.json\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00002-3512e62e-a490-4057-802c-09060baa6911.metadata.json\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/cf556cc3-2af0-4bd8-9228-d4c11755e0b7-m0.avro\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/cf556cc3-2af0-4bd8-9228-d4c11755e0b7-m1.avro\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/ee02a218-f8f4-449c-8781-ce82b0450e0c-m0.avro\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/snap-7257983078605640789-1-cf556cc3-2af0-4bd8-9228-d4c11755e0b7.avro\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/snap-7462507546055427409-1-ee02a218-f8f4-449c-8781-ce82b0450e0c.avro\n"
     ]
    }
   ],
   "source": [
    "metadata_file_path_list = catalogUtil.get_all_metadata_layer_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "37363956-0414-4e18-95b6-4811d35571b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/data/coffee_sale_ts_month=2023-04/00156-1033-a93cedac-db38-4e28-ad16-6e1267c35d7f-00001.parquet\n",
      "Data File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/data/coffee_sale_ts_month=2023-07/00011-834-6ed85fcc-cf79-4762-8245-494b01333410-00001.parquet\n",
      "Data File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/data/coffee_sale_ts_month=2023-09/00072-1031-4166a8f5-7efa-4708-9ede-13ce4bf5b75b-00001.parquet\n",
      "Data File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/data/coffee_sale_ts_month=2023-09/00072-211-d8611c4f-62fa-464e-b998-1dccb751e0f5-00001.parquet\n"
     ]
    }
   ],
   "source": [
    "data_files_list = catalogUtil.get_all_data_layer_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "86a3bdd4-bb0b-42b3-8dce-b989ec81ee03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00000-30b3f58c-b3d7-4cc5-8f03-2d3a6ada2c11.metadata.json\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00001-7d5f111e-852b-4cae-bfa3-a574cdb56a14.metadata.json\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00002-3512e62e-a490-4057-802c-09060baa6911.metadata.json\n"
     ]
    }
   ],
   "source": [
    "metadata_file_path_list = catalogUtil.get_all_metadata_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a9527acf-a0a5-4ff4-9a83-5cca763aa281",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/IPython/core/display.py:911: UserWarning: JSON expects JSONable dict or list, not JSON strings\n",
      "  warnings.warn(\"JSON expects JSONable dict or list, not JSON strings\")\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "current-schema-id": 0,
       "current-snapshot-id": 7257983078605641000,
       "default-sort-order-id": 0,
       "default-spec-id": 0,
       "format-version": 2,
       "last-column-id": 3,
       "last-partition-id": 1000,
       "last-sequence-number": 2,
       "last-updated-ms": 1693866090476,
       "location": "s3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table",
       "metadata-log": [
        {
         "metadata-file": "s3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00000-30b3f58c-b3d7-4cc5-8f03-2d3a6ada2c11.metadata.json",
         "timestamp-ms": 1693866026299
        },
        {
         "metadata-file": "s3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00001-7d5f111e-852b-4cae-bfa3-a574cdb56a14.metadata.json",
         "timestamp-ms": 1693866037323
        }
       ],
       "partition-specs": [
        {
         "fields": [
          {
           "field-id": 1000,
           "name": "coffee_sale_ts_month",
           "source-id": 3,
           "transform": "month"
          }
         ],
         "spec-id": 0
        }
       ],
       "properties": {
        "owner": "pauldefusco",
        "write.delete.mode": "copy-on-write",
        "write.merge.mode": "copy-on-write",
        "write.update.mode": "copy-on-write"
       },
       "refs": {
        "main": {
         "snapshot-id": 7257983078605641000,
         "type": "branch"
        }
       },
       "schemas": [
        {
         "fields": [
          {
           "id": 1,
           "name": "coffee_id",
           "required": false,
           "type": "long"
          },
          {
           "id": 2,
           "name": "coffee_size",
           "required": false,
           "type": "string"
          },
          {
           "id": 3,
           "name": "coffee_sale_ts",
           "required": false,
           "type": "timestamptz"
          }
         ],
         "schema-id": 0,
         "type": "struct"
        }
       ],
       "snapshot-log": [
        {
         "snapshot-id": 7462507546055427000,
         "timestamp-ms": 1693866037323
        },
        {
         "snapshot-id": 7257983078605641000,
         "timestamp-ms": 1693866090476
        }
       ],
       "snapshots": [
        {
         "manifest-list": "s3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/snap-7462507546055427409-1-ee02a218-f8f4-449c-8781-ce82b0450e0c.avro",
         "schema-id": 0,
         "sequence-number": 1,
         "snapshot-id": 7462507546055427000,
         "summary": {
          "added-data-files": "1",
          "added-files-size": "976",
          "added-records": "1",
          "changed-partition-count": "1",
          "operation": "append",
          "spark.app.id": "spark-application-1693865986682",
          "total-data-files": "1",
          "total-delete-files": "0",
          "total-equality-deletes": "0",
          "total-files-size": "976",
          "total-position-deletes": "0",
          "total-records": "1"
         },
         "timestamp-ms": 1693866037323
        },
        {
         "manifest-list": "s3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/snap-7257983078605640789-1-cf556cc3-2af0-4bd8-9228-d4c11755e0b7.avro",
         "parent-snapshot-id": 7462507546055427000,
         "schema-id": 0,
         "sequence-number": 2,
         "snapshot-id": 7257983078605641000,
         "summary": {
          "added-data-files": "3",
          "added-files-size": "2983",
          "added-records": "3",
          "changed-partition-count": "3",
          "deleted-data-files": "1",
          "deleted-records": "1",
          "operation": "overwrite",
          "removed-files-size": "976",
          "spark.app.id": "spark-application-1693865986682",
          "total-data-files": "3",
          "total-delete-files": "0",
          "total-equality-deletes": "0",
          "total-files-size": "2983",
          "total-position-deletes": "0",
          "total-records": "3"
         },
         "timestamp-ms": 1693866090476
        }
       ],
       "sort-orders": [
        {
         "fields": [],
         "order-id": 0
        }
       ],
       "statistics": [],
       "table-uuid": "54ffc775-9282-4b00-a33d-e2a2acf12ade"
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 55,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalogUtil.print_metadata_file(spark, metadata_file_path_list[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3cd8b42d-d544-4d19-81f2-9b0677a6fe14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/snap-7257983078605640789-1-cf556cc3-2af0-4bd8-9228-d4c11755e0b7.avro\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/snap-7462507546055427409-1-ee02a218-f8f4-449c-8781-ce82b0450e0c.avro\n"
     ]
    }
   ],
   "source": [
    "manifest_lists_list = catalogUtil.get_all_manifest_lists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c0733e43-0159-471c-b527-289a0d1a37a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "application/json": {
       "added_data_files_count": 1,
       "added_rows_count": 1,
       "added_snapshot_id": 7462507546055427000,
       "content": 0,
       "deleted_data_files_count": 0,
       "deleted_rows_count": 0,
       "existing_data_files_count": 0,
       "existing_rows_count": 0,
       "manifest_length": 7074,
       "manifest_path": "s3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/ee02a218-f8f4-449c-8781-ce82b0450e0c-m0.avro",
       "min_sequence_number": 1,
       "partition_spec_id": 0,
       "partitions": [
        {
         "contains_nan": false,
         "contains_null": false,
         "lower_bound": "hAIAAA==",
         "upper_bound": "hAIAAA=="
        }
       ],
       "sequence_number": 1
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 57,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalogUtil.print_manifest_file(spark, manifest_lists_list[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "20ebe917-3632-46b4-9d97-e20ede40b53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/cf556cc3-2af0-4bd8-9228-d4c11755e0b7-m0.avro\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/cf556cc3-2af0-4bd8-9228-d4c11755e0b7-m1.avro\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/ee02a218-f8f4-449c-8781-ce82b0450e0c-m0.avro\n"
     ]
    }
   ],
   "source": [
    "manifest_files_list = catalogUtil.get_all_manifest_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "66075c85-4377-4054-a04f-7b814e439b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "application/json": {
       "data_file": {
        "column_sizes": [
         {
          "key": 1,
          "value": 39
         },
         {
          "key": 2,
          "value": 41
         },
         {
          "key": 3,
          "value": 39
         }
        ],
        "content": 0,
        "file_format": "PARQUET",
        "file_path": "s3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table/data/coffee_sale_ts_month=2023-07/00011-834-6ed85fcc-cf79-4762-8245-494b01333410-00001.parquet",
        "file_size_in_bytes": 999,
        "lower_bounds": [
         {
          "key": 1,
          "value": "AgAAAAAAAAA="
         },
         {
          "key": 2,
          "value": "Z3JhbmRl"
         },
         {
          "key": 3,
          "value": "AHJv+Wr/BQA="
         }
        ],
        "nan_value_counts": [],
        "null_value_counts": [
         {
          "key": 1,
          "value": 0
         },
         {
          "key": 2,
          "value": 0
         },
         {
          "key": 3,
          "value": 0
         }
        ],
        "partition": {
         "coffee_sale_ts_month": 642
        },
        "record_count": 1,
        "sort_order_id": 0,
        "split_offsets": [
         4
        ],
        "upper_bounds": [
         {
          "key": 1,
          "value": "AgAAAAAAAAA="
         },
         {
          "key": 2,
          "value": "Z3JhbmRl"
         },
         {
          "key": 3,
          "value": "AHJv+Wr/BQA="
         }
        ],
        "value_counts": [
         {
          "key": 1,
          "value": 1
         },
         {
          "key": 2,
          "value": 1
         },
         {
          "key": 3,
          "value": 1
         }
        ]
       },
       "snapshot_id": 7257983078605641000,
       "status": 1
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalogUtil.print_manifest_file(spark, manifest_files_list[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "20b77e7d-5719-4a89-b392-ff43d4579c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "data_file": {
        "column_sizes": [
         {
          "key": 1,
          "value": 33
         },
         {
          "key": 2,
          "value": 34
         },
         {
          "key": 3,
          "value": 39
         }
        ],
        "content": 0,
        "file_format": "PARQUET",
        "file_path": "s3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table/data/coffee_sale_ts_month=2023-09/00072-211-d8611c4f-62fa-464e-b998-1dccb751e0f5-00001.parquet",
        "file_size_in_bytes": 976,
        "lower_bounds": [
         {
          "key": 1,
          "value": "AQAAAAAAAAA="
         },
         {
          "key": 2,
          "value": "dmVudGk="
         },
         {
          "key": 3,
          "value": "AMhBOUkEBgA="
         }
        ],
        "nan_value_counts": [],
        "null_value_counts": [
         {
          "key": 1,
          "value": 0
         },
         {
          "key": 2,
          "value": 0
         },
         {
          "key": 3,
          "value": 0
         }
        ],
        "partition": {
         "coffee_sale_ts_month": 644
        },
        "record_count": 1,
        "sort_order_id": 0,
        "split_offsets": [
         4
        ],
        "upper_bounds": [
         {
          "key": 1,
          "value": "AQAAAAAAAAA="
         },
         {
          "key": 2,
          "value": "dmVudGk="
         },
         {
          "key": 3,
          "value": "AMhBOUkEBgA="
         }
        ],
        "value_counts": [
         {
          "key": 1,
          "value": 1
         },
         {
          "key": 2,
          "value": 1
         },
         {
          "key": 3,
          "value": 1
         }
        ]
       },
       "snapshot_id": 7462507546055427000,
       "status": 1
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 60,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalogUtil.print_manifest_file(spark, manifest_files_list[2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bb34d9d8-f821-49e2-8d16-bcff74240be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/data/coffee_sale_ts_month=2023-04/00156-1033-a93cedac-db38-4e28-ad16-6e1267c35d7f-00001.parquet\n",
      "Data File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/data/coffee_sale_ts_month=2023-07/00011-834-6ed85fcc-cf79-4762-8245-494b01333410-00001.parquet\n",
      "Data File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/data/coffee_sale_ts_month=2023-09/00072-1031-4166a8f5-7efa-4708-9ede-13ce4bf5b75b-00001.parquet\n",
      "Data File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/data/coffee_sale_ts_month=2023-09/00072-211-d8611c4f-62fa-464e-b998-1dccb751e0f5-00001.parquet\n"
     ]
    }
   ],
   "source": [
    "data_files_list = catalogUtil.get_all_data_layer_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2d22d9e1-418e-4ab1-ba08-b49e971f62e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 53:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------------+\n",
      "|coffee_id|coffee_size|     coffee_sale_ts|\n",
      "+---------+-----------+-------------------+\n",
      "|        3|       tall|2023-04-01 12:01:00|\n",
      "+---------+-----------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "catalogUtil.print_data_file(spark, data_files_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4999c930-f5c6-40f7-b4e8-604030e8b263",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------------+\n",
      "|coffee_id|coffee_size|     coffee_sale_ts|\n",
      "+---------+-----------+-------------------+\n",
      "|        2|     grande|2023-07-01 11:10:00|\n",
      "+---------+-----------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "catalogUtil.print_data_file(spark, data_files_list[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0e4b41-8570-45a9-ac95-a8bdc9f47189",
   "metadata": {},
   "source": [
    "NB: The row with coffee_id = 1 was a match between the target and staging tables. Because COW was used, a new file was written with the same row and the updated value for the \"coffee_size\" field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9ac839e3-251b-4683-9a5a-3c91e44d3198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new data file with the updated row:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'warehouse/tablespace/external/hive/lakehouse.db/coffees_table/data/coffee_sale_ts_month=2023-09/00072-1031-4166a8f5-7efa-4708-9ede-13ce4bf5b75b-00001.parquet'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The new data file with the updated row (size grande):\")\n",
    "data_files_list[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "678965bc-ecb8-4cda-b6fc-7205b375d3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 48:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------------+\n",
      "|coffee_id|coffee_size|     coffee_sale_ts|\n",
      "+---------+-----------+-------------------+\n",
      "|        1|     grande|2023-09-01 10:00:00|\n",
      "+---------+-----------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "catalogUtil.print_data_file(spark, data_files_list[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8c35716a-e747-4bef-a8fb-3af9d3f1bc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The old data file with the original row:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'warehouse/tablespace/external/hive/lakehouse.db/coffees_table/data/coffee_sale_ts_month=2023-09/00072-211-d8611c4f-62fa-464e-b998-1dccb751e0f5-00001.parquet'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The old data file with the original row (size venti):\")\n",
    "data_files_list[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9c8e4c66-ed74-4496-8b23-1bfc77e7e260",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 50:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------------+\n",
      "|coffee_id|coffee_size|     coffee_sale_ts|\n",
      "+---------+-----------+-------------------+\n",
      "|        1|      venti|2023-09-01 10:00:00|\n",
      "+---------+-----------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "catalogUtil.print_data_file(spark, data_files_list[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f6b01f74-4cef-4ade-8f16-1a1b4a19eec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 51:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------------+\n",
      "|coffee_id|coffee_size|     coffee_sale_ts|\n",
      "+---------+-----------+-------------------+\n",
      "|        2|     grande|2023-07-01 11:10:00|\n",
      "|        1|     grande|2023-09-01 10:00:00|\n",
      "|        3|       tall|2023-04-01 12:01:00|\n",
      "+---------+-----------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM lakehouse.coffees_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7caceab-04d6-4f70-96a1-612b8325e4b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9486357-7744-4068-998c-7bb4a9edf5da",
   "metadata": {},
   "source": [
    "#### There is a new metadata file (json) prefixed by 0002."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9308c678-d334-4a0b-8b14-94c0164c9be5",
   "metadata": {},
   "source": [
    "#### There is a new manifest list file (avro) prefixed by \"snap\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1685d9f-422a-44dc-a969-e4e3d5b0718c",
   "metadata": {},
   "source": [
    "#### There is a new manifest file (avro)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62108a25-7170-4760-88a9-bbe2a80581b3",
   "metadata": {},
   "source": [
    "### Time Travel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c566c5c6-8b5d-4aba-9c20-409233c4827b",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshots_df = spark.sql(\"SELECT * FROM lakehouse.customer_table.snapshots;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0852473c-3639-43c1-a446-8920be731f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_snapshot = snapshots_df.select(\"snapshot_id\").head(1)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb18fe8c-fb4f-4669-a52e-fbe024979e4b",
   "metadata": {},
   "source": [
    "#### Validate that the output dataframe only includes one row per the original insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0cd767-66f2-4393-8a2f-0d1d89e6a973",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read\\\n",
    "    .option(\"snapshot-id\", first_snapshot)\\\n",
    "    .format(\"iceberg\")\\\n",
    "    .load(\"lakehouse.customer_table\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbc2468-98ab-4e80-a7cd-717cb515b045",
   "metadata": {},
   "outputs": [],
   "source": [
    "avro_tempdf = spark.read.format(\"avro\").load(\"s3a://go01-demo/\" + metadata_file_list[6]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e45b8d9-0368-4f66-80e6-2e5aa2766144",
   "metadata": {},
   "outputs": [],
   "source": [
    "avro_tempdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dda7fb9-9e96-4a4d-89fc-e6f0c1197213",
   "metadata": {},
   "outputs": [],
   "source": [
    "avro_tempdf['partitions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7990d583-eedb-4959-a90c-1151920ab963",
   "metadata": {},
   "outputs": [],
   "source": [
    "avro_tempdf['added_rows_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d808a437-8439-4b0a-afeb-33d5817f0636",
   "metadata": {},
   "outputs": [],
   "source": [
    "avro_tempdf['existing_rows_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f5a019-bbf7-431a-96e3-984e79a249a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "avro_tempdf['added_data_files_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2ef22c-733c-42b7-a022-2955407ec2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Showing \" + metadata_file_list[2])\n",
    "json_tempdf = spark.read.option(\"multiline\",\"true\").json(\"s3a://go01-demo/\" + metadata_file_list[2]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb05bf2-2a10-4964-a75c-f2c732d09682",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_tempdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44b9003-ec1e-4a66-a7f1-aa90e2fcf8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_tempdf['current-schema-id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd86e88-aec0-4602-b8db-8f96cb053a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(json_tempdf['snapshots'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340197e4-5203-483d-9700-a3fd982ceedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_tempdf['partition-spec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2001b532-b800-4377-bfd9-e7fe5799d9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(json_tempdf['partition-specs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156c215d-ba62-4046-82f8-6a8e21a96001",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM lakehouse.coffees_table_2.all_data_files;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3a962b-9da5-4256-b48e-b5cd17690323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c23bd6a-9e1a-4909-9f09-7caeb5524647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca0e554-abdf-4fbd-9a56-840c7602a57f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b08cb0-0cf9-4a87-ab17-5b4286023c05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d86f6c6-682c-45d6-a26e-54c93430a0d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a00307f-365a-492a-a710-692510544b69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fadb12-a5fc-4138-89e8-455459b7cab9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe197cc5-122b-4fa0-ba41-6d3889a466b3",
   "metadata": {},
   "source": [
    "### Partition Evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63961d6-9c2a-451d-b772-adc382e1c0da",
   "metadata": {},
   "source": [
    "Spark partitioning is a way to split the data into multiple partitions so that you can execute transformations on multiple partitions in parallel which allows completing the job faster. You can also write partitioned data into a file system (multiple sub-directories) for faster reads by downstream systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0463d053-187c-4f32-8bbe-53c4ece328dc",
   "metadata": {},
   "source": [
    "Spark has several partitioning methods to achieve parallelism, based on your need, you should choose which one to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb47968-03c8-400b-93c4-2101ac1f14cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "738a10ca-a464-40f6-baa6-2aafdfb2d2bb",
   "metadata": {},
   "source": [
    "Creating New Data to Test Partition Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b096c653-df75-4ba4-bf13-5bcbfacd9f43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3851f4-d681-44cc-965c-835667ffd4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType, IntegerType, StringType\n",
    "\n",
    "import dbldatagen as dg\n",
    "\n",
    "shuffle_partitions_requested = 20\n",
    "device_population = 100000\n",
    "data_rows = 20 * 1000000\n",
    "#partitions_requested = 20\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", shuffle_partitions_requested)\n",
    "\n",
    "country_codes = [\n",
    "    \"CN\", \"US\", \"FR\", \"CA\", \"IN\", \"JM\", \"IE\", \"PK\", \"GB\", \"IL\", \"AU\", \n",
    "    \"SG\", \"ES\", \"GE\", \"MX\", \"ET\", \"SA\", \"LB\", \"NL\", \"IT\"\n",
    "]\n",
    "#country_weights = [\n",
    "#    1300, 365, 67, 38, 1300, 3, 7, 212, 67, 9, 25, 6, 47, 83, \n",
    "#    126, 109, 58, 8, 17,\n",
    "#]\n",
    "\n",
    "manufacturers = [\n",
    "    \"Delta corp\", \"Xyzzy Inc.\", \"Lakehouse Ltd\", \"Acme Corp\", \"Embanks Devices\",\n",
    "]\n",
    "\n",
    "lines = [\"delta\", \"xyzzy\", \"lakehouse\", \"gadget\", \"droid\"]\n",
    "\n",
    "testDataSpec = (\n",
    "    dg.DataGenerator(spark, name=\"device_data_set\", rows=data_rows) \n",
    "                     #,partitions=partitions_requested)\n",
    "    .withIdOutput()\n",
    "    # we'll use hash of the base field to generate the ids to\n",
    "    # avoid a simple incrementing sequence\n",
    "    .withColumn(\"internal_device_id\", \"long\", minValue=0x1000000000000, \n",
    "                uniqueValues=device_population, omit=True, baseColumnType=\"hash\",\n",
    "    )\n",
    "    # note for format strings, we must use \"%lx\" not \"%x\" as the\n",
    "    # underlying value is a long\n",
    "    .withColumn(\n",
    "        \"device_id\", \"string\", format=\"0x%013x\", baseColumn=\"internal_device_id\"\n",
    "    )\n",
    "    # the device / user attributes will be the same for the same device id\n",
    "    # so lets use the internal device id as the base column for these attribute\n",
    "    .withColumn(\"country\", \"string\", values=country_codes, #weights=country_weights, \n",
    "                baseColumn=\"internal_device_id\")\n",
    "    .withColumn(\"manufacturer\", \"string\", values=manufacturers, \n",
    "                baseColumn=\"internal_device_id\", )\n",
    "    # use omit = True if you don't want a column to appear in the final output\n",
    "    # but just want to use it as part of generation of another column\n",
    "    .withColumn(\"line\", \"string\", values=lines, baseColumn=\"manufacturer\", \n",
    "                baseColumnType=\"hash\", omit=True )\n",
    "    .withColumn(\"model_ser\", \"integer\", minValue=1, maxValue=11, baseColumn=\"device_id\", \n",
    "                baseColumnType=\"hash\", omit=True, )\n",
    "    .withColumn(\"model_line\", \"string\", expr=\"concat(line, '#', model_ser)\", \n",
    "                baseColumn=[\"line\", \"model_ser\"] )\n",
    "    .withColumn(\"event_type\", \"string\", \n",
    "                values=[\"activation\", \"deactivation\", \"plan change\", \"telecoms activity\", \n",
    "                        \"internet activity\", \"device error\", ],\n",
    "                random=True)\n",
    "    .withColumn(\"event_ts\", \"timestamp\", begin=\"2020-01-01 01:00:00\", \n",
    "                end=\"2020-12-31 23:59:00\", \n",
    "                interval=\"1 minute\", random=True )\n",
    ")\n",
    "\n",
    "dfTestData = testDataSpec.build()\n",
    "\n",
    "display(dfTestData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a41d6d-fd68-4fa0-914d-a84bbf298fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTestData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbedb46-48c7-472f-a1db-2e2df8ce4adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.sources.partitionColumnTypeInference.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842ee72a-ec1a-4ed4-8aba-3a2a4baf5863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cb0635-e535-4758-919f-d4f04bacf0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS spark_catalog.lakehouse.partition_evol_tbl PURGE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559589db-5dd5-4e49-ba44-b8b8143d12b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfTestData.groupBy(\"country\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4ed5eb-90ae-4960-b9f6-83a3df408409",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfTestData.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0594e7-4e60-45cf-a7e9-ac26c9478e87",
   "metadata": {},
   "source": [
    "Iceberg requires the data to be sorted according to the partition spec per task (Spark partition) in prior to write against partitioned table. This applies both Writing with SQL and Writing with DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50873884-250f-4872-8df8-befeadf6b797",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTestData.sortWithinPartitions(\"country\").writeTo(\"spark_catalog.lakehouse.p_evol_tbl\").partitionedBy(\"country\").using(\"iceberg\").create()#.append()#replace()#overwritePartitions()#create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85761304-fcbb-405c-aab8-221c6fad602b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"SELECT * FROM spark_catalog.lakehouse.part_evol_tbl.PARTITIONS\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04502b8-874e-49ec-b3c3-aac117ce29ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"SELECT * FROM spark_catalog.lakehouse.part_evol_tbl.files\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f76c097-d1c1-4339-af78-c97c245ea00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"SELECT * FROM spark_catalog.lakehouse.part_evol_tbl.manifests\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e6a8be-01f8-4be9-beb1-f953c0ad7e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"SELECT * FROM spark_catalog.lakehouse.part_evol_tbl.all_manifests\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f2bf66-d219-4e65-a58e-260bf891e2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"SELECT * FROM spark_catalog.lakehouse.part_evol_tbl.all_data_files\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88b8694-61bb-4507-af92-7b288b50e4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"SELECT * FROM spark_catalog.lakehouse.part_evol_tbl.snapshots\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684e0727-c58d-4f37-a682-7748f566e205",
   "metadata": {},
   "source": [
    "Adding a partition field is a metadata operation and does not change any of the existing table data. New data will be written with the new partitioning, but existing data will remain in the old partition layout. Old data files will have null values for the new partition fields in metadata tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2c9fe1-7594-4909-8744-59dde048eb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TABLE PARTITIONS BEFORE ALTER PARTITION STATEMENT: \")\n",
    "spark.sql(\"SELECT * FROM spark_catalog.lakehouse.p_evol_tbl.PARTITIONS\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dd4330-e06c-46fa-87b4-3624555daa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ADD PARTITION BY EVENT TIMESTAMP MONTHS: \")\n",
    "print(\"ALTER TABLE spark_catalog.lakehouse.p_evol_tbl ADD PARTITION FIELD months(event_ts)\")\n",
    "spark.sql(\"ALTER TABLE spark_catalog.lakehouse.p_evol_tbl ADD PARTITION FIELD months(event_ts)\")\n",
    "#spark.sql(\"ALTER TABLE spark_catalog.lakehouse.part_evol_tbl REPLACE PARTITION FIELD hours(dob) WITH state\")\n",
    "#spark.sql(\"ALTER TABLE prod.db.sample ADD PARTITION FIELD month\")\n",
    "\n",
    "#ALTER TABLE spark_catalog.lakehouse.part_evol_tbl ADD PARTITION FIELD days(event_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee150f5-9d59-4b23-8df3-e6f5d70e2fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TABLE PARTITIONS AFTER ALTER PARTITION STATEMENT: \")\n",
    "spark.sql(\"SELECT * FROM spark_catalog.lakehouse.p_evol_tbl.PARTITIONS\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa64157-5a82-4c6d-82f8-f94bd08e8601",
   "metadata": {},
   "outputs": [],
   "source": [
    "appendDf = dfTestData.sample(fraction=0.3, seed=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494691b4-e56a-41d5-98e3-0f033f26d59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "appendDf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f2ca29-d54b-4aba-a9f8-ad1019616b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "appendDf.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33af64b-c2ca-447f-a0b7-185e223d973c",
   "metadata": {},
   "outputs": [],
   "source": [
    "appendDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea0b278-5cb4-4bc3-aff9-f988b338c9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "appendDf.sortWithinPartitions(\"country\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf4dcea-b8d5-43a9-ae67-b3aebf07d938",
   "metadata": {},
   "outputs": [],
   "source": [
    "#appendDf.sortWithinPartitions(\"country\", \"month(event_ts)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e1c1ea-6949-49e1-a6ea-5022033abc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "appendDf.sortWithinPartitions(\"country\").writeTo(\"spark_catalog.lakehouse.p_evol_tbl\").using(\"iceberg\").append() #.append()#replace()#overwritePartitions()#create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef167cb7-a230-4d45-8619-87090d850789",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TABLE PARTITIONS AFTER APPEND: \")\n",
    "spark.sql(\"SELECT * FROM spark_catalog.lakehouse.p_evol_tbl.PARTITIONS\").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ef51fc-950b-4a96-85c3-6cdb7c472e37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5406060-fea0-4002-a915-be5bd29aa08b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b3860d-d5b9-47f5-b494-0ffe5f0d190e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a009cb8e-e564-40aa-bebf-e1229b4c5341",
   "metadata": {},
   "source": [
    "Dropping a partition field is a metadata operation and does not change any of the existing table data. New data will be written with the new partitioning, but existing data will remain in the old partition layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7e0fb9-374e-4965-93da-776e91c2b787",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"ALTER TABLE spark_catalog.lakehouse.part_evol_tbl DROP PARTITION FIELD bucket(16, device_id)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5340bace-0227-48b8-b984-d0136381f672",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TABLE PARTITIONS AFTER ALTER PARTITION STATEMENT: \")\n",
    "spark.sql(\"SELECT * FROM spark_catalog.lakehouse.part_evol_tbl.PARTITIONS\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068a2029-59ec-44dd-bbe0-1ba04777548e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8512c690-0009-47a8-8046-7477ed7b99b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144ebaf9-0d8c-4ff0-90fb-516b9b2f97a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e57e5ac-497b-4692-afb3-d28f4d54d5fb",
   "metadata": {},
   "source": [
    "##### Only json files have been added (one per each time you repartitioned) but Avro files have stayed the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489132be-d671-46a6-92c5-6dc291638f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket(\"go01-demo\")\n",
    "\n",
    "metadata_file_list = []\n",
    "\n",
    "print(\"Current Metadata Files: \\n\")\n",
    "\n",
    "for object_summary in my_bucket.objects.filter(Prefix=metadata_path+\"/metadata\"):\n",
    "    #print(object_summary.key +\"\\n\")\n",
    "    metadata_file_list.append(object_summary.key)\n",
    "    \n",
    "metadata_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6af64da-19fc-47b3-afd8-5a938704ee11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1b5aed-4bba-482d-ac22-8e0f30cb970a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abd06d2-fb84-4a96-ab3d-9c9697cd85ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b998fc45-091b-4776-81d0-402aeb8673e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162a1ede-72a3-40bf-9cbf-1ae4a410832f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc464b86-ebea-41a7-9afe-6917f262b39c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd33dd11-1439-408e-912b-dc65d7f12fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE TABLE IF NOT EXISTS customer_table (id BIGINT, state STRING, country STRING, dob TIMESTAMP) USING iceberg PARTITIONED BY ( hours(dob))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7930a494-94d8-4fee-b873-8120257fbbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT HOUR(dob) FROM spark_catalog.lakehouse.customer_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485b48f5-4eaf-4edf-ad66-2a30a19b6c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT DAY(dob) FROM spark_catalog.lakehouse.customer_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178a933e-5ee3-4a64-93e0-85fac5dc544c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0086fbd-2b28-4df6-baf9-17768c5a7b9b",
   "metadata": {},
   "source": [
    "### Dropping Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abab5842-1722-470e-8066-c58e7659192e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS lakehouse.staging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f903baae-7c19-45a9-95d3-79d929491ed2",
   "metadata": {},
   "source": [
    "Validate that the metadata folder is now empty but the data folder still retains parquet files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fef6a1-ca58-48d2-83c2-e4e66dc2e3b1",
   "metadata": {},
   "source": [
    "![alt text](../img/s3_droptable_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eba9f0-a03c-4c17-bb4f-d1b9e6404a97",
   "metadata": {},
   "source": [
    "![alt text](../img/s3_droptable_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de2b0a5-8b24-4055-8250-d2ce689949c3",
   "metadata": {},
   "source": [
    "![alt text](../img/s3_droptable_3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43739d1f-8682-4575-98de-58f4ca4416f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"ALTER TABLE lakehouse.customers_table\\\n",
    "            SET TBLPROPERTIES ('format-version' = '2')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38b4f0c-f054-4721-9789-5e3968be2985",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket(\"go01-demo\")\n",
    "\n",
    "metadata_file_list = []\n",
    "\n",
    "for object_summary in my_bucket.objects.filter(Prefix=metadata_path):\n",
    "    print(object_summary.key +\"\\n\")\n",
    "    metadata_file_list.append(object_summary.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ba146b-fdfb-41e3-bc75-65ce5c4c5bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Showing \" + metadata_file_list[3])\n",
    "spark.read.option(\"multiline\",\"true\").json(\"s3a://go01-demo/\" + metadata_file_list[3]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdce88a-df1c-4e93-960e-fcf732bb9df0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
