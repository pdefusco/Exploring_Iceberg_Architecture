{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0942afe9-7629-4b3e-b747-b795a6454a2b",
   "metadata": {},
   "source": [
    "## Introduction to Iceberg Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f7e7d5e-5a02-414b-9177-49cdfe38fddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cmlbootstrap\n",
      "  Cloning https://github.com/fastforwardlabs/cmlbootstrap to /tmp/pip-install-fn_87rg0/cmlbootstrap_da688e5f248c490bb9c0193bda96be24\n",
      "  Running command git clone -q https://github.com/fastforwardlabs/cmlbootstrap /tmp/pip-install-fn_87rg0/cmlbootstrap_da688e5f248c490bb9c0193bda96be24\n",
      "Requirement already satisfied: pandas in /home/cdsw/.local/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (1.3.5)\n",
      "Requirement already satisfied: numpy==1.19.0 in /home/cdsw/.local/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (1.19.0)\n",
      "Requirement already satisfied: boto3 in /home/cdsw/.local/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (1.17.62)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 5)) (2.25.1)\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 6)) (1.26.6)\n",
      "Requirement already satisfied: dbldatagen in /home/cdsw/.local/lib/python3.7/site-packages (from -r requirements.txt (line 7)) (0.2.1)\n",
      "Requirement already satisfied: PyArrow in /home/cdsw/.local/lib/python3.7/site-packages (from -r requirements.txt (line 8)) (12.0.0)\n",
      "Requirement already satisfied: requests-kerberos==0.12.0 in /home/cdsw/.local/lib/python3.7/site-packages (from cmlbootstrap->-r requirements.txt (line 4)) (0.12.0)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.62 in /home/cdsw/.local/lib/python3.7/site-packages (from boto3->-r requirements.txt (line 3)) (1.20.112)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/cdsw/.local/lib/python3.7/site-packages (from boto3->-r requirements.txt (line 3)) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /home/cdsw/.local/lib/python3.7/site-packages (from boto3->-r requirements.txt (line 3)) (0.4.2)\n",
      "Requirement already satisfied: pykerberos<2.0.0,>=1.1.8 in /home/cdsw/.local/lib/python3.7/site-packages (from requests-kerberos==0.12.0->cmlbootstrap->-r requirements.txt (line 4)) (1.2.4)\n",
      "Requirement already satisfied: cryptography>=1.3 in /home/cdsw/.local/lib/python3.7/site-packages (from requests-kerberos==0.12.0->cmlbootstrap->-r requirements.txt (line 4)) (40.0.2)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests->-r requirements.txt (line 5)) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests->-r requirements.txt (line 5)) (2020.11.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests->-r requirements.txt (line 5)) (2.10)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/site-packages (from botocore<1.21.0,>=1.20.62->boto3->-r requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/site-packages (from cryptography>=1.3->requests-kerberos==0.12.0->cmlbootstrap->-r requirements.txt (line 4)) (1.14.4)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/site-packages (from cffi>=1.12->cryptography>=1.3->requests-kerberos==0.12.0->cmlbootstrap->-r requirements.txt (line 4)) (2.20)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.62->boto3->-r requirements.txt (line 3)) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 1)) (2021.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd15c6f-e31d-440c-bc48-966385b7c5ec",
   "metadata": {},
   "source": [
    "#### Launching a Spark Session with Iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f139b6a-eed6-4848-9e82-593aabad58b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/23 22:00:10 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "23/09/23 22:00:11 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "Setting spark.hadoop.yarn.resourcemanager.principal to pauldefusco\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/23 22:00:11 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "23/09/23 22:00:11 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "23/09/23 22:00:11 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "23/09/23 22:00:13 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:135)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/23 22:00:13 WARN HttpChannel: /jobs/\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:135)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/23 22:00:14 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "23/09/23 22:00:14 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/23 22:00:14 WARN HttpChannel: /jobs/\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/23 22:00:15 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/23 22:00:15 WARN HttpChannel: /jobs/\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/23 22:00:16 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/23 22:00:16 WARN HttpChannel: /jobs/\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/23 22:00:17 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/23 22:00:17 WARN HttpChannel: /jobs/\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/23 22:00:18 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/23 22:00:18 WARN HttpChannel: /jobs/\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/23 22:00:19 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/23 22:00:19 WARN HttpChannel: /jobs/\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/23 22:00:20 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/23 22:00:20 WARN HttpChannel: /jobs/\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/23 22:00:21 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/23 22:00:21 WARN HttpChannel: /jobs/\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/23 22:00:22 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/23 22:00:22 WARN HttpChannel: /jobs/\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/23 22:00:23 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/23 22:00:23 WARN HttpChannel: /jobs/\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/23 22:00:24 WARN HiveServer2CredentialProvider: Failed to get HS2 delegation token\n",
      "java.util.NoSuchElementException: spark.sql.hive.hiveserver2.jdbc.url\n",
      "\tat org.apache.spark.SparkConf.$anonfun$get$1(SparkConf.scala:245)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.SparkConf.get(SparkConf.scala:245)\n",
      "\tat com.hortonworks.spark.deploy.yarn.security.HiveServer2CredentialProvider.obtainDelegationTokens(HiveServer2CredentialProvider.scala:64)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager.$anonfun$obtainDelegationTokens$2(HadoopDelegationTokenManager.scala:164)\n",
      "\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat scala.collection.MapLike$DefaultValuesIterable.foreach(MapLike.scala:213)\n",
      "\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)\n",
      "\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)\n",
      "\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager.org$apache$spark$deploy$security$HadoopDelegationTokenManager$$obtainDelegationTokens(HadoopDelegationTokenManager.scala:162)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager$$anon$4.run(HadoopDelegationTokenManager.scala:226)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager$$anon$4.run(HadoopDelegationTokenManager.scala:224)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager.obtainTokensAndScheduleRenewal(HadoopDelegationTokenManager.scala:224)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager.org$apache$spark$deploy$security$HadoopDelegationTokenManager$$updateTokensTask(HadoopDelegationTokenManager.scala:198)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager.start(HadoopDelegationTokenManager.scala:123)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.$anonfun$start$1(CoarseGrainedSchedulerBackend.scala:552)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.$anonfun$start$1$adapted(CoarseGrainedSchedulerBackend.scala:549)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.start(CoarseGrainedSchedulerBackend.scala:549)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.KubernetesClusterSchedulerBackend.start(KubernetesClusterSchedulerBackend.scala:95)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:220)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:581)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/23 22:00:25 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/23 22:00:25 WARN HttpChannel: /jobs/\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/23 22:00:25 WARN ExecutorAllocationManager: Dynamic allocation without a shuffle service is an experimental feature.\n",
      "23/09/23 22:00:26 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/23 22:00:26 WARN HttpChannel: /jobs/\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/23 22:00:29 WARN HiveClientImpl: Detected HiveConf hive.execution.engine is 'tez' and will be reset to 'mr' to disable useless hive logic\n",
      "Hive Session ID = 7dfd10d3-a138-4e1a-aea1-03eae466f4a4\n",
      "23/09/23 22:00:31 WARN HiveMetaStoreClient: Failed to connect to the MetaStore Server...\n",
      "23/09/23 22:00:31 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|         01_car_data|\n",
      "|           01_car_dw|\n",
      "|              adb101|\n",
      "|            airlines|\n",
      "|        airlines_csv|\n",
      "|    airlines_iceberg|\n",
      "|airlines_iceberg_...|\n",
      "|      airlines_mjain|\n",
      "|          airquality|\n",
      "|                ajvp|\n",
      "|          atlas_demo|\n",
      "|            bankdemo|\n",
      "|          bca_jps_l0|\n",
      "|        cde_workshop|\n",
      "|cde_workshop_pdef...|\n",
      "|             cdedemo|\n",
      "|        cdp_overview|\n",
      "|      ceht_open_data|\n",
      "|        ceht_scratch|\n",
      "| ceht_transportation|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cml.data_v1 as cmldata\n",
    "\n",
    "CONNECTION_NAME = \"go01-aw-dl\"\n",
    "conn = cmldata.get_connection(CONNECTION_NAME)\n",
    "spark = conn.get_spark_session()\n",
    "\n",
    "# Sample usage to run query through spark\n",
    "EXAMPLE_SQL_QUERY = \"show databases\"\n",
    "spark.sql(EXAMPLE_SQL_QUERY).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3bbf1bf-a0a1-4864-8c98-88dfb994060a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/23 22:00:32 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "23/09/23 22:00:32 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('spark.eventLog.enabled', 'true'),\n",
       " ('spark.network.crypto.enabled', 'true'),\n",
       " ('spark.sql.hive.hwc.execution.mode', 'spark'),\n",
       " ('spark.jars',\n",
       "  '/opt/spark/optional-lib/hive-warehouse-connector-assembly.jar,/opt/spark/optional-lib/iceberg-hive-runtime.jar,/opt/spark/optional-lib/iceberg-spark-runtime.jar'),\n",
       " ('spark.kerberos.renewal.credentials', 'ccache'),\n",
       " ('spark.sql.catalog.spark_catalog',\n",
       "  'org.apache.iceberg.spark.SparkSessionCatalog'),\n",
       " ('spark.app.initial.jar.urls',\n",
       "  'spark://100.100.175.202:42265/jars/iceberg-hive-runtime-1.1.0.1.19.7215.0-118.jar,spark://100.100.175.202:42265/jars/iceberg-spark-runtime-3.2_2.12-1.1.0.1.19.7215.0-118.jar,spark://100.100.175.202:42265/jars/hive-warehouse-connector-assembly-1.0.0.1.19.7215.0-118.jar'),\n",
       " ('spark.dynamicAllocation.maxExecutors', '49'),\n",
       " ('spark.eventLog.dir', 'file:///sparkeventlogs'),\n",
       " ('spark.hadoop.yarn.resourcemanager.principal', 'pauldefusco'),\n",
       " ('spark.kubernetes.driver.annotation.cluster-autoscaler.kubernetes.io/safe-to-evict',\n",
       "  'false'),\n",
       " ('spark.ui.port', '20049'),\n",
       " ('spark.kubernetes.driver.pod.name', 'a8r5dpdadtfatd8k'),\n",
       " ('spark.driver.bindAddress', '100.100.175.202'),\n",
       " ('spark.yarn.access.hadoopFileSystems',\n",
       "  's3a://go01-demo/warehouse/tablespace/external/hive'),\n",
       " ('spark.sql.extensions',\n",
       "  'com.qubole.spark.hiveacid.HiveAcidAutoConvertExtension,org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions'),\n",
       " ('spark.kubernetes.executor.annotation.cluster-autoscaler.kubernetes.io/safe-to-evict',\n",
       "  'false'),\n",
       " ('spark.driver.memory', '3051m'),\n",
       " ('spark.app.startTime', '1695506411896'),\n",
       " ('spark.driver.port', '42265'),\n",
       " ('spark.io.encryption.enabled', 'true'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.master', 'k8s://https://172.20.0.1:443'),\n",
       " ('spark.ui.allowFramingFrom',\n",
       "  'https://ml-b74f8940-b97.go01-dem.ylcu-atmi.cloudera.site'),\n",
       " ('spark.kubernetes.executor.podTemplateFile', '/tmp/spark-executor.json'),\n",
       " ('spark.kubernetes.container.image',\n",
       "  'docker.repository.cloudera.com/cloudera/cdsw/ml-runtime-jupyterlab-python3.7-standard:2022.11.1-b2'),\n",
       " ('spark.dynamicAllocation.shuffleTracking.enabled', 'true'),\n",
       " ('spark.app.id', 'spark-application-1695506413967'),\n",
       " ('spark.kubernetes.namespace', 'mlx-user-16'),\n",
       " ('spark.ui.proxyRedirectUri',\n",
       "  'https://spark-a8r5dpdadtfatd8k.ml-b74f8940-b97.go01-dem.ylcu-atmi.cloudera.site'),\n",
       " ('spark.kubernetes.executor.config.dir', '/var/spark/conf'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  's3a://go01-demo/warehouse/tablespace/external/hive'),\n",
       " ('spark.hadoop.iceberg.engine.hive.enabled', 'true'),\n",
       " ('spark.driver.host', '100.100.175.202'),\n",
       " ('spark.repl.local.jars',\n",
       "  'file:///runtime-addons/spark323-19-hf2-g3aapf/opt/spark/optional-lib/hive-warehouse-connector-assembly-1.0.0.1.19.7215.0-118.jar,file:///runtime-addons/spark323-19-hf2-g3aapf/opt/spark/optional-lib/iceberg-hive-runtime-1.1.0.1.19.7215.0-118.jar,file:///runtime-addons/spark323-19-hf2-g3aapf/opt/spark/optional-lib/iceberg-spark-runtime-3.2_2.12-1.1.0.1.19.7215.0-118.jar'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.yarn.rmProxy.enabled', 'false'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.kryo.registrator',\n",
       "  'com.qubole.spark.hiveacid.util.HiveAcidKyroRegistrator'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.dynamicAllocation.enabled', 'true'),\n",
       " ('spark.deploy.mode', 'client'),\n",
       " ('spark.kubernetes.executor.podNamePrefix', 'cdsw-a8r5dpdadtfatd8k'),\n",
       " ('spark.app.name', 'None'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.sql.catalog.spark_catalog.type', 'hive'),\n",
       " ('spark.authenticate', 'true')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa007770-01e4-4c2e-a784-6955c448c952",
   "metadata": {},
   "source": [
    "### Iceberg Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad5615b-c7e6-4528-bd48-db0841d38c39",
   "metadata": {},
   "source": [
    "![alt text](../img/iceberg-metadata.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c3c93c-4065-422b-9697-609ae50687c7",
   "metadata": {},
   "source": [
    "#### Iceberg Catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30001dd5-c3ef-4223-84ca-e63552c4f879",
   "metadata": {},
   "source": [
    "Iceberg comes with catalogs that enable SQL commands to manage tables and load them by name. Catalogs are configured using properties under spark.sql.catalog.(catalog_name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3819a08-8e00-4184-bdc6-79b3d8507d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+\n",
      "|      catalog|namespace|\n",
      "+-------------+---------+\n",
      "|spark_catalog|  default|\n",
      "+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show catalog and database\n",
    "spark.sql(\"SHOW CURRENT NAMESPACE\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1119faa-93a7-4ebe-85b1-3ec87eb54b7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new database\n",
    "#spark.sql(\"DROP DATABASE IF EXISTS spark_catalog.lakehouse\")\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS spark_catalog.lakehouse\")\n",
    "spark.sql(\"USE spark_catalog.lakehouse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da3225e2-a19a-471c-aaed-8f4b8f833b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"USE spark_catalog.lakehouse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "834ca50c-9c2f-43ac-9ec0-9cf28e8507d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+\n",
      "|      catalog|namespace|\n",
      "+-------------+---------+\n",
      "|spark_catalog|lakehouse|\n",
      "+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show catalog and database\n",
    "spark.sql(\"SHOW CURRENT NAMESPACE\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6cecda-848a-4cbe-b7a8-d73096cab11f",
   "metadata": {},
   "source": [
    "#### Create an Iceberg Table with Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c5daa62-cde3-440a-ad91-0c6f9b41ef3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/23 22:46:09 WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed (this is expected if the application is shutting down.)\n",
      "io.fabric8.kubernetes.client.WatcherException: too old resource version: 22174829 (22181089)\n",
      "\tat io.fabric8.kubernetes.client.dsl.internal.AbstractWatchManager.onStatus(AbstractWatchManager.java:265)\n",
      "\tat io.fabric8.kubernetes.client.dsl.internal.AbstractWatchManager.onMessage(AbstractWatchManager.java:249)\n",
      "\tat io.fabric8.kubernetes.client.dsl.internal.WatcherWebSocketListener.onMessage(WatcherWebSocketListener.java:93)\n",
      "\tat okhttp3.internal.ws.RealWebSocket.onReadMessage(RealWebSocket.java:323)\n",
      "\tat okhttp3.internal.ws.WebSocketReader.readMessageFrame(WebSocketReader.java:219)\n",
      "\tat okhttp3.internal.ws.WebSocketReader.processNextFrame(WebSocketReader.java:105)\n",
      "\tat okhttp3.internal.ws.RealWebSocket.loopReader(RealWebSocket.java:274)\n",
      "\tat okhttp3.internal.ws.RealWebSocket$2.onResponse(RealWebSocket.java:214)\n",
      "\tat okhttp3.RealCall$AsyncCall.execute(RealCall.java:203)\n",
      "\tat okhttp3.internal.NamedRunnable.run(NamedRunnable.java:32)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: io.fabric8.kubernetes.client.KubernetesClientException: too old resource version: 22174829 (22181089)\n",
      "\t... 13 more\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS lakehouse.coffees_table PURGE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b10e6c-efcc-46b5-a164-893879b79327",
   "metadata": {},
   "source": [
    "# TEST 1 with COW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b19e0de9-29c4-43dd-ae5c-c2bd69558032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE IF NOT EXISTS coffees_table (coffee_id BIGINT, coffee_size STRING, coffee_sale_ts TIMESTAMP)\\\n",
    "          USING ICEBERG\\\n",
    "          PARTITIONED BY (months(coffee_sale_ts))\\\n",
    "          TBLPROPERTIES ('write.delete.mode'='copy-on-write',\\\n",
    "                          'write.update.mode'='copy-on-write',\\\n",
    "                          'write.merge.mode'='copy-on-write',\\\n",
    "                          'format-version' = '2')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eea95e7-e221-4997-9071-208cbc9f9fc5",
   "metadata": {},
   "source": [
    "#### Verify that a Metadata JSON file has been created under the Metadata directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d865393-0703-481d-b696-5c23d31bf901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CatalogUtil import CatalogUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab7b270a-d85d-4fcd-91ad-3341e373f469",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogUtil = CatalogUtil(\"lakehouse\", \"coffees_table\", \"go01-demo\", \"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aecdf06b-74f6-42a7-84aa-021acfaeca56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are now the files in the Iceberg Metadata Layer:\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00000-be30580e-87c6-45b6-9480-2d6d8a3b47da.metadata.json\n",
      "Total number of files in the Metadata Layer: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"These are now the files in the Iceberg Metadata Layer:\")\n",
    "catalogUtil.count_metadata_layer_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc42e771-a996-4ff0-bf11-43a892727783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are now the metadata files in the Iceberg Metadata Layer:\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00000-be30580e-87c6-45b6-9480-2d6d8a3b47da.metadata.json\n",
      "Total Number of Metadata Files: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"These are now the metadata files in the Iceberg Metadata Layer:\")\n",
    "catalogUtil.count_metadata_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69344686-048d-4385-a8cc-5e8b550a06bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are now the manifest lists in the Iceberg Metadata Layer:\n",
      "Total Number of Manifest Lists: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"These are now the manifest lists in the Iceberg Metadata Layer:\")\n",
    "catalogUtil.count_manifest_lists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4208a40-cc86-4d5e-9b1e-585f0531eef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are now the manifest files in the Iceberg Metadata Layer:\n",
      "Total Number of Manifest Files: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"These are now the manifest files in the Iceberg Metadata Layer:\")\n",
    "catalogUtil.count_manifest_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdb926a4-2635-41c3-86e0-35e7c1e14507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00000-be30580e-87c6-45b6-9480-2d6d8a3b47da.metadata.json\n"
     ]
    }
   ],
   "source": [
    "metadata_file_path = catalogUtil.get_latest_metadata_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50b5b013-3ad4-4e2e-a3f6-85bc072c1f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/IPython/core/display.py:911: UserWarning: JSON expects JSONable dict or list, not JSON strings\n",
      "  warnings.warn(\"JSON expects JSONable dict or list, not JSON strings\")\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "current-schema-id": 0,
       "current-snapshot-id": -1,
       "default-sort-order-id": 0,
       "default-spec-id": 0,
       "format-version": 2,
       "last-column-id": 3,
       "last-partition-id": 1000,
       "last-sequence-number": 0,
       "last-updated-ms": 1694209476692,
       "location": "s3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table",
       "metadata-log": [],
       "partition-specs": [
        {
         "fields": [
          {
           "field-id": 1000,
           "name": "coffee_sale_ts_month",
           "source-id": 3,
           "transform": "month"
          }
         ],
         "spec-id": 0
        }
       ],
       "properties": {
        "owner": "pauldefusco",
        "write.delete.mode": "copy-on-write",
        "write.merge.mode": "copy-on-write",
        "write.update.mode": "copy-on-write"
       },
       "schemas": [
        {
         "fields": [
          {
           "id": 1,
           "name": "coffee_id",
           "required": false,
           "type": "long"
          },
          {
           "id": 2,
           "name": "coffee_size",
           "required": false,
           "type": "string"
          },
          {
           "id": 3,
           "name": "coffee_sale_ts",
           "required": false,
           "type": "timestamptz"
          }
         ],
         "schema-id": 0,
         "type": "struct"
        }
       ],
       "snapshot-log": [],
       "snapshots": [],
       "sort-orders": [
        {
         "fields": [],
         "order-id": 0
        }
       ],
       "statistics": [],
       "table-uuid": "ac664378-5621-476d-a899-ebddda4f4d27"
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalogUtil.print_metadata_file(spark, metadata_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357d7e23-9be4-4b92-bbb0-cb69da6f1476",
   "metadata": {},
   "source": [
    "![alt text](../img/s3_metadata.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f6b96e-d878-4e2e-91a0-1ef785833479",
   "metadata": {},
   "source": [
    "#### Notice that no snapshots or other files have been created as data has not yet been inserted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "972dced2-83a3-4c4f-a63d-2936a54534d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+---------+-------------------+\n",
      "|made_current_at|snapshot_id|parent_id|is_current_ancestor|\n",
      "+---------------+-----------+---------+-------------------+\n",
      "+---------------+-----------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM lakehouse.coffees_table.history\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d726f825-38b0-4342-a0ea-0f932ce20355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+---------+---------+-------------+-------+\n",
      "|committed_at|snapshot_id|parent_id|operation|manifest_list|summary|\n",
      "+------------+-----------+---------+---------+-------------+-------+\n",
      "+------------+-----------+---------+---------+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM lakehouse.coffees_table.snapshots;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68f5ad55-0adb-4ab3-8574-da44c3af6b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----------+-------+---------+------------+------------------+------------+------------+-----------------+----------------+------------+------------+------------+-------------+------------+-------------+\n",
      "|content|file_path|file_format|spec_id|partition|record_count|file_size_in_bytes|column_sizes|value_counts|null_value_counts|nan_value_counts|lower_bounds|upper_bounds|key_metadata|split_offsets|equality_ids|sort_order_id|\n",
      "+-------+---------+-----------+-------+---------+------------+------------------+------------+------------+-----------------+----------------+------------+------------+------------+-------------+------------+-------------+\n",
      "+-------+---------+-----------+-------+---------+------------+------------------+------------+------------+-----------------+----------------+------------+------------+------------+-------------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM lakehouse.coffees_table.files;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "636820a4-528b-42f2-9153-2180206420de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+-----------------+-----------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+-------------------+\n",
      "|content|path|length|partition_spec_id|added_snapshot_id|added_data_files_count|existing_data_files_count|deleted_data_files_count|added_delete_files_count|existing_delete_files_count|deleted_delete_files_count|partition_summaries|\n",
      "+-------+----+------+-----------------+-----------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+-------------------+\n",
      "+-------+----+------+-----------------+-----------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM lakehouse.coffees_table.manifests;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8544499-8ba1-4156-b641-89c10c281926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----------+-------+---------+------------+------------------+------------+------------+-----------------+----------------+------------+------------+------------+-------------+------------+-------------+\n",
      "|content|file_path|file_format|spec_id|partition|record_count|file_size_in_bytes|column_sizes|value_counts|null_value_counts|nan_value_counts|lower_bounds|upper_bounds|key_metadata|split_offsets|equality_ids|sort_order_id|\n",
      "+-------+---------+-----------+-------+---------+------------+------------------+------------+------------+-----------------+----------------+------------+------------+------------+-------------+------------+-------------+\n",
      "+-------+---------+-----------+-------+---------+------------+------------------+------------+------------+-----------------+----------------+------------+------------+------------+-------------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM lakehouse.coffees_table.all_data_files;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "323639fc-1ed3-418d-ae0e-8c1674a30ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+-----------------+-----------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+-------------------+---------------------+\n",
      "|content|path|length|partition_spec_id|added_snapshot_id|added_data_files_count|existing_data_files_count|deleted_data_files_count|added_delete_files_count|existing_delete_files_count|deleted_delete_files_count|partition_summaries|reference_snapshot_id|\n",
      "+-------+----+------+-----------------+-----------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+-------------------+---------------------+\n",
      "+-------+----+------+-----------------+-----------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+-------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM lakehouse.coffees_table.all_manifests;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24046132-45da-4f28-a60b-f0227b8fb7cc",
   "metadata": {},
   "source": [
    "### Table Insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "028f245c-e167-4418-9e96-fd098e0cb6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f2d7725-fe94-4a8c-82d0-319c1cefef8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Coffee_id = 1, Coffee_size = venti, coffee_sale_ts = 2023-09-01\n",
    "\n",
    "spark.sql(\"INSERT INTO lakehouse.coffees_table VALUES (1, 'venti', cast(date_format('2023-09-01 10:00:00', 'yyyy-MM-dd HH:mm:ss') as timestamp))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f0d668-9e68-4b1c-a881-5d1265f999c7",
   "metadata": {},
   "source": [
    "#### Data has been added to the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da0242b8-93b9-4471-9aae-7a8f162a2320",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = \"select h.made_current_at,\\\n",
    "            s.operation,\\\n",
    "            h.snapshot_id,\\\n",
    "            h.is_current_ancestor,\\\n",
    "            s.summary['spark.app.id']\\\n",
    "        from lakehouse.coffees_table.history h\\\n",
    "        join lakehouse.coffees_table.snapshots s\\\n",
    "            on h.snapshot_id = s.snapshot_id\\\n",
    "            order by made_current_at;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "76eaa58b-c77b-437a-979d-feaa4ac9b536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/08 21:44:57 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>made_current_at</th>\n",
       "      <th>operation</th>\n",
       "      <th>snapshot_id</th>\n",
       "      <th>is_current_ancestor</th>\n",
       "      <th>summary[spark.app.id]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-09-08 21:44:52.022</td>\n",
       "      <td>append</td>\n",
       "      <td>3743804437087925131</td>\n",
       "      <td>True</td>\n",
       "      <td>spark-application-1694209434520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          made_current_at operation          snapshot_id  is_current_ancestor  \\\n",
       "0 2023-09-08 21:44:52.022    append  3743804437087925131                 True   \n",
       "\n",
       "             summary[spark.app.id]  \n",
       "0  spark-application-1694209434520  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(QUERY).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3728c7dc-ac75-4859-8790-501210b43381",
   "metadata": {},
   "source": [
    "#### Notice there are now two json files and two avro files. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44f25b0-3aea-4b67-a5e5-bed3bf2b6a9d",
   "metadata": {},
   "source": [
    "The first json file is the metadata file created when the table was created. This is the metata file prefixed by 00000. The second json file is the new metadata file reflecting the insert of one row. This is the metadata file prefixed by 00001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3519f9f-c58e-46ca-875a-9bb3d3a2c180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are now the files in the Iceberg Metadata Layer:\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00000-be30580e-87c6-45b6-9480-2d6d8a3b47da.metadata.json\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00001-607ac9ea-b355-4064-b43a-321fa77bb56e.metadata.json\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/1c96feb7-51dd-47a2-b08a-0c339b086082-m0.avro\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/snap-3743804437087925131-1-1c96feb7-51dd-47a2-b08a-0c339b086082.avro\n",
      "Total number of files in the Metadata Layer: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"These are now the files in the Iceberg Metadata Layer:\")\n",
    "catalogUtil.count_metadata_layer_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c76a7a4e-dbbb-489a-b233-c70c247d98ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are now the metadata files in the Iceberg Metadata Layer:\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00000-be30580e-87c6-45b6-9480-2d6d8a3b47da.metadata.json\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00001-607ac9ea-b355-4064-b43a-321fa77bb56e.metadata.json\n",
      "Total Number of Metadata Files: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"These are now the metadata files in the Iceberg Metadata Layer:\")\n",
    "catalogUtil.count_metadata_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd378253-2cde-440c-b559-53429572228e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are now the manifest lists in the Iceberg Metadata Layer:\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/snap-3743804437087925131-1-1c96feb7-51dd-47a2-b08a-0c339b086082.avro\n",
      "Total Number of Manifest Lists: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"These are now the manifest lists in the Iceberg Metadata Layer:\")\n",
    "catalogUtil.count_manifest_lists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f08951aa-6207-4eef-9cbc-4fc186368070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are now the manifest files in the Iceberg Metadata Layer:\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/1c96feb7-51dd-47a2-b08a-0c339b086082-m0.avro\n",
      "Total Number of Manifest Files: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"These are now the manifest files in the Iceberg Metadata Layer:\")\n",
    "catalogUtil.count_manifest_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b6d6560-93d4-4e18-9610-50805e74d471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00000-be30580e-87c6-45b6-9480-2d6d8a3b47da.metadata.json\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00001-607ac9ea-b355-4064-b43a-321fa77bb56e.metadata.json\n"
     ]
    }
   ],
   "source": [
    "metadata_file_path_list = catalogUtil.get_all_metadata_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "236ad68d-8741-4e44-a3b3-496e1d7eff81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/IPython/core/display.py:911: UserWarning: JSON expects JSONable dict or list, not JSON strings\n",
      "  warnings.warn(\"JSON expects JSONable dict or list, not JSON strings\")\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "current-schema-id": 0,
       "current-snapshot-id": -1,
       "default-sort-order-id": 0,
       "default-spec-id": 0,
       "format-version": 2,
       "last-column-id": 3,
       "last-partition-id": 1000,
       "last-sequence-number": 0,
       "last-updated-ms": 1694209476692,
       "location": "s3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table",
       "metadata-log": [],
       "partition-specs": [
        {
         "fields": [
          {
           "field-id": 1000,
           "name": "coffee_sale_ts_month",
           "source-id": 3,
           "transform": "month"
          }
         ],
         "spec-id": 0
        }
       ],
       "properties": {
        "owner": "pauldefusco",
        "write.delete.mode": "copy-on-write",
        "write.merge.mode": "copy-on-write",
        "write.update.mode": "copy-on-write"
       },
       "schemas": [
        {
         "fields": [
          {
           "id": 1,
           "name": "coffee_id",
           "required": false,
           "type": "long"
          },
          {
           "id": 2,
           "name": "coffee_size",
           "required": false,
           "type": "string"
          },
          {
           "id": 3,
           "name": "coffee_sale_ts",
           "required": false,
           "type": "timestamptz"
          }
         ],
         "schema-id": 0,
         "type": "struct"
        }
       ],
       "snapshot-log": [],
       "snapshots": [],
       "sort-orders": [
        {
         "fields": [],
         "order-id": 0
        }
       ],
       "statistics": [],
       "table-uuid": "ac664378-5621-476d-a899-ebddda4f4d27"
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalogUtil.print_metadata_file(spark, metadata_file_path_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1e7891d0-b323-42e5-b47a-fe1c98f22e24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "current-schema-id": 0,
       "current-snapshot-id": 3743804437087925000,
       "default-sort-order-id": 0,
       "default-spec-id": 0,
       "format-version": 2,
       "last-column-id": 3,
       "last-partition-id": 1000,
       "last-sequence-number": 1,
       "last-updated-ms": 1694209492022,
       "location": "s3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table",
       "metadata-log": [
        {
         "metadata-file": "s3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00000-be30580e-87c6-45b6-9480-2d6d8a3b47da.metadata.json",
         "timestamp-ms": 1694209476692
        }
       ],
       "partition-specs": [
        {
         "fields": [
          {
           "field-id": 1000,
           "name": "coffee_sale_ts_month",
           "source-id": 3,
           "transform": "month"
          }
         ],
         "spec-id": 0
        }
       ],
       "properties": {
        "owner": "pauldefusco",
        "write.delete.mode": "copy-on-write",
        "write.merge.mode": "copy-on-write",
        "write.update.mode": "copy-on-write"
       },
       "refs": {
        "main": {
         "snapshot-id": 3743804437087925000,
         "type": "branch"
        }
       },
       "schemas": [
        {
         "fields": [
          {
           "id": 1,
           "name": "coffee_id",
           "required": false,
           "type": "long"
          },
          {
           "id": 2,
           "name": "coffee_size",
           "required": false,
           "type": "string"
          },
          {
           "id": 3,
           "name": "coffee_sale_ts",
           "required": false,
           "type": "timestamptz"
          }
         ],
         "schema-id": 0,
         "type": "struct"
        }
       ],
       "snapshot-log": [
        {
         "snapshot-id": 3743804437087925000,
         "timestamp-ms": 1694209492022
        }
       ],
       "snapshots": [
        {
         "manifest-list": "s3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/snap-3743804437087925131-1-1c96feb7-51dd-47a2-b08a-0c339b086082.avro",
         "schema-id": 0,
         "sequence-number": 1,
         "snapshot-id": 3743804437087925000,
         "summary": {
          "added-data-files": "1",
          "added-files-size": "976",
          "added-records": "1",
          "changed-partition-count": "1",
          "operation": "append",
          "spark.app.id": "spark-application-1694209434520",
          "total-data-files": "1",
          "total-delete-files": "0",
          "total-equality-deletes": "0",
          "total-files-size": "976",
          "total-position-deletes": "0",
          "total-records": "1"
         },
         "timestamp-ms": 1694209492022
        }
       ],
       "sort-orders": [
        {
         "fields": [],
         "order-id": 0
        }
       ],
       "statistics": [],
       "table-uuid": "ac664378-5621-476d-a899-ebddda4f4d27"
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalogUtil.print_metadata_file(spark, metadata_file_path_list[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f46899f-2ecd-4651-9dcf-7e6c4d912273",
   "metadata": {},
   "source": [
    "The Avro file with the \"snap\" prefix is the manifest list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "08d79d77-e537-4b3a-919d-2ea5514559e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/snap-3743804437087925131-1-1c96feb7-51dd-47a2-b08a-0c339b086082.avro\n"
     ]
    }
   ],
   "source": [
    "manifest_lists_list = catalogUtil.get_all_manifest_lists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9f499d0a-421f-461f-85dd-16e565c2f54d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "added_data_files_count": 1,
       "added_rows_count": 1,
       "added_snapshot_id": 3743804437087925000,
       "content": 0,
       "deleted_data_files_count": 0,
       "deleted_rows_count": 0,
       "existing_data_files_count": 0,
       "existing_rows_count": 0,
       "manifest_length": 7069,
       "manifest_path": "s3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/1c96feb7-51dd-47a2-b08a-0c339b086082-m0.avro",
       "min_sequence_number": 1,
       "partition_spec_id": 0,
       "partitions": [
        {
         "contains_nan": false,
         "contains_null": false,
         "lower_bound": "hAIAAA==",
         "upper_bound": "hAIAAA=="
        }
       ],
       "sequence_number": 1
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalogUtil.print_manifest_file(spark, manifest_lists_list[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e77b3e-81e4-4c29-b6d3-6e6db6f6edad",
   "metadata": {},
   "source": [
    "The Avro file without the \"snap\" prefix is the manifest file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "828e4eae-8499-4e6a-b466-01f86e64ea28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/1c96feb7-51dd-47a2-b08a-0c339b086082-m0.avro\n"
     ]
    }
   ],
   "source": [
    "manifest_files_list = catalogUtil.get_all_manifest_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "83fc8914-b20f-44c3-928e-98243dc0fe4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "data_file": {
        "column_sizes": [
         {
          "key": 1,
          "value": 33
         },
         {
          "key": 2,
          "value": 34
         },
         {
          "key": 3,
          "value": 39
         }
        ],
        "content": 0,
        "file_format": "PARQUET",
        "file_path": "s3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table/data/coffee_sale_ts_month=2023-09/00072-212-fd2d233b-96d1-4611-bbe4-677cedb22ad0-00001.parquet",
        "file_size_in_bytes": 976,
        "lower_bounds": [
         {
          "key": 1,
          "value": "AQAAAAAAAAA="
         },
         {
          "key": 2,
          "value": "dmVudGk="
         },
         {
          "key": 3,
          "value": "AMhBOUkEBgA="
         }
        ],
        "nan_value_counts": [],
        "null_value_counts": [
         {
          "key": 1,
          "value": 0
         },
         {
          "key": 2,
          "value": 0
         },
         {
          "key": 3,
          "value": 0
         }
        ],
        "partition": {
         "coffee_sale_ts_month": 644
        },
        "record_count": 1,
        "sort_order_id": 0,
        "split_offsets": [
         4
        ],
        "upper_bounds": [
         {
          "key": 1,
          "value": "AQAAAAAAAAA="
         },
         {
          "key": 2,
          "value": "dmVudGk="
         },
         {
          "key": 3,
          "value": "AMhBOUkEBgA="
         }
        ],
        "value_counts": [
         {
          "key": 1,
          "value": 1
         },
         {
          "key": 2,
          "value": 1
         },
         {
          "key": 3,
          "value": 1
         }
        ]
       },
       "snapshot_id": 3743804437087925000,
       "status": 1
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalogUtil.print_manifest_file(spark, manifest_files_list[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c8c364dd-1dd8-4ce4-bd3b-2b38ed7cb070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/data/coffee_sale_ts_month=2023-09/00072-212-fd2d233b-96d1-4611-bbe4-677cedb22ad0-00001.parquet\n"
     ]
    }
   ],
   "source": [
    "data_files_list = catalogUtil.get_all_data_layer_files()\n",
    "# could also query the all_data_files metadata table:\n",
    "# spark.sql(\"SELECT * FROM lakehouse.coffees_table.all_data_files;\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb91747c-9f0b-46cc-b363-fc134d97a9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------------+\n",
      "|coffee_id|coffee_size|     coffee_sale_ts|\n",
      "+---------+-----------+-------------------+\n",
      "|        1|      venti|2023-09-01 10:00:00|\n",
      "+---------+-----------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "catalogUtil.print_data_file(spark, data_files_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f023c1ca-48c8-4d4d-b329-4825d473bf4b",
   "metadata": {},
   "source": [
    "### Table Merge Into"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bef6e0-6ea1-4f75-aeac-fd84b0b7f2f7",
   "metadata": {},
   "source": [
    "Create a staging table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a8287cfa-9f8e-40fb-a880-a2dabcdbd2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS lakehouse.coffee_staging PURGE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1a742c99-a211-4fa6-bdd5-e63f8079f9d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE IF NOT EXISTS lakehouse.coffee_staging\\\n",
    "            (coffee_id BIGINT, coffee_size STRING, coffee_sale_ts TIMESTAMP)\\\n",
    "            USING iceberg\\\n",
    "            PARTITIONED BY (months(coffee_sale_ts))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d4dc4207-34e0-4bb3-a147-dfcd4e601990",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"INSERT INTO lakehouse.coffee_staging\\\n",
    "            VALUES (1, 'grande', cast(date_format('2023-07-01 11:00:00', 'yyyy-MM-dd HH:mm:ss') as timestamp)),\\\n",
    "            (2, 'grande', cast(date_format('2023-07-01 11:10:00', 'yyyy-MM-dd HH:mm:ss') as timestamp)),\\\n",
    "            (3, 'tall', cast(date_format('2023-04-01 12:01:00', 'yyyy-MM-dd HH:mm:ss') as timestamp))\")\n",
    "\n",
    "#Row: Coffee_id = 1, coffee_size = venti, coffee_sale_ts = 2023-07-01\n",
    "#Row: Coffee_id = 2, coffee_size = grande, coffee_sale_ts = 2023-07-01\n",
    "#Row: Coffee_id = 3, coffee_size = tall, coffee_sale_ts = 2023-04-01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c3720c-3011-4a15-924f-de39a525501d",
   "metadata": {},
   "source": [
    "Merge Into Customers Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4b62e634-935e-4219-b240-fdb8411b2984",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/08 21:45:18 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"MERGE INTO lakehouse.coffees_table c\\\n",
    "            USING (SELECT * FROM lakehouse.coffee_staging) s\\\n",
    "            ON c.coffee_id = s.coffee_id \\\n",
    "            WHEN MATCHED THEN UPDATE SET c.coffee_size = s.coffee_size\\\n",
    "            WHEN NOT MATCHED THEN INSERT *\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c94305e6-c808-4d67-8903-864a4d80b5dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>committed_at</th>\n",
       "      <th>snapshot_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>operation</th>\n",
       "      <th>manifest_list</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-09-08 21:44:52.022</td>\n",
       "      <td>3743804437087925131</td>\n",
       "      <td>NaN</td>\n",
       "      <td>append</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>{'spark.app.id': 'spark-application-1694209434...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-09-08 21:45:21.010</td>\n",
       "      <td>8285711116837308020</td>\n",
       "      <td>3.743804e+18</td>\n",
       "      <td>overwrite</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>{'added-data-files': '3', 'total-equality-dele...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             committed_at          snapshot_id     parent_id  operation  \\\n",
       "0 2023-09-08 21:44:52.022  3743804437087925131           NaN     append   \n",
       "1 2023-09-08 21:45:21.010  8285711116837308020  3.743804e+18  overwrite   \n",
       "\n",
       "                                       manifest_list  \\\n",
       "0  s3a://go01-demo/warehouse/tablespace/external/...   \n",
       "1  s3a://go01-demo/warehouse/tablespace/external/...   \n",
       "\n",
       "                                             summary  \n",
       "0  {'spark.app.id': 'spark-application-1694209434...  \n",
       "1  {'added-data-files': '3', 'total-equality-dele...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM lakehouse.coffees_table.snapshots;\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2d2d2db3-6d38-428e-bc8d-51286ff2e11c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>path</th>\n",
       "      <th>length</th>\n",
       "      <th>partition_spec_id</th>\n",
       "      <th>added_snapshot_id</th>\n",
       "      <th>added_data_files_count</th>\n",
       "      <th>existing_data_files_count</th>\n",
       "      <th>deleted_data_files_count</th>\n",
       "      <th>added_delete_files_count</th>\n",
       "      <th>existing_delete_files_count</th>\n",
       "      <th>deleted_delete_files_count</th>\n",
       "      <th>partition_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>7204</td>\n",
       "      <td>0</td>\n",
       "      <td>8285711116837308020</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(False, False, 2023-04, 2023-09)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>7071</td>\n",
       "      <td>0</td>\n",
       "      <td>8285711116837308020</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(False, False, 2023-09, 2023-09)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   content                                               path  length  \\\n",
       "0        0  s3a://go01-demo/warehouse/tablespace/external/...    7204   \n",
       "1        0  s3a://go01-demo/warehouse/tablespace/external/...    7071   \n",
       "\n",
       "   partition_spec_id    added_snapshot_id  added_data_files_count  \\\n",
       "0                  0  8285711116837308020                       3   \n",
       "1                  0  8285711116837308020                       0   \n",
       "\n",
       "   existing_data_files_count  deleted_data_files_count  \\\n",
       "0                          0                         0   \n",
       "1                          0                         1   \n",
       "\n",
       "   added_delete_files_count  existing_delete_files_count  \\\n",
       "0                         0                            0   \n",
       "1                         0                            0   \n",
       "\n",
       "   deleted_delete_files_count                 partition_summaries  \n",
       "0                           0  [(False, False, 2023-04, 2023-09)]  \n",
       "1                           0  [(False, False, 2023-09, 2023-09)]  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM lakehouse.coffees_table.manifests;\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "db59e20d-b4fa-455c-8a34-b5538d06b2ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>file_path</th>\n",
       "      <th>file_format</th>\n",
       "      <th>spec_id</th>\n",
       "      <th>partition</th>\n",
       "      <th>record_count</th>\n",
       "      <th>file_size_in_bytes</th>\n",
       "      <th>column_sizes</th>\n",
       "      <th>value_counts</th>\n",
       "      <th>null_value_counts</th>\n",
       "      <th>nan_value_counts</th>\n",
       "      <th>lower_bounds</th>\n",
       "      <th>upper_bounds</th>\n",
       "      <th>key_metadata</th>\n",
       "      <th>split_offsets</th>\n",
       "      <th>equality_ids</th>\n",
       "      <th>sort_order_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>PARQUET</td>\n",
       "      <td>0</td>\n",
       "      <td>(644,)</td>\n",
       "      <td>1</td>\n",
       "      <td>976</td>\n",
       "      <td>{1: 33, 2: 34, 3: 39}</td>\n",
       "      <td>{1: 1, 2: 1, 3: 1}</td>\n",
       "      <td>{1: 0, 2: 0, 3: 0}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{1: [1, 0, 0, 0, 0, 0, 0, 0], 2: [118, 101, 11...</td>\n",
       "      <td>{1: [1, 0, 0, 0, 0, 0, 0, 0], 2: [118, 101, 11...</td>\n",
       "      <td>None</td>\n",
       "      <td>[4]</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>PARQUET</td>\n",
       "      <td>0</td>\n",
       "      <td>(642,)</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>{1: 39, 2: 41, 3: 39}</td>\n",
       "      <td>{1: 1, 2: 1, 3: 1}</td>\n",
       "      <td>{1: 0, 2: 0, 3: 0}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{1: [2, 0, 0, 0, 0, 0, 0, 0], 2: [103, 114, 97...</td>\n",
       "      <td>{1: [2, 0, 0, 0, 0, 0, 0, 0], 2: [103, 114, 97...</td>\n",
       "      <td>None</td>\n",
       "      <td>[4]</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>PARQUET</td>\n",
       "      <td>0</td>\n",
       "      <td>(644,)</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>{1: 39, 2: 41, 3: 39}</td>\n",
       "      <td>{1: 1, 2: 1, 3: 1}</td>\n",
       "      <td>{1: 0, 2: 0, 3: 0}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{1: [1, 0, 0, 0, 0, 0, 0, 0], 2: [103, 114, 97...</td>\n",
       "      <td>{1: [1, 0, 0, 0, 0, 0, 0, 0], 2: [103, 114, 97...</td>\n",
       "      <td>None</td>\n",
       "      <td>[4]</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>PARQUET</td>\n",
       "      <td>0</td>\n",
       "      <td>(639,)</td>\n",
       "      <td>1</td>\n",
       "      <td>985</td>\n",
       "      <td>{1: 39, 2: 39, 3: 39}</td>\n",
       "      <td>{1: 1, 2: 1, 3: 1}</td>\n",
       "      <td>{1: 0, 2: 0, 3: 0}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{1: [3, 0, 0, 0, 0, 0, 0, 0], 2: [116, 97, 108...</td>\n",
       "      <td>{1: [3, 0, 0, 0, 0, 0, 0, 0], 2: [116, 97, 108...</td>\n",
       "      <td>None</td>\n",
       "      <td>[4]</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   content                                          file_path file_format  \\\n",
       "0        0  s3a://go01-demo/warehouse/tablespace/external/...     PARQUET   \n",
       "1        0  s3a://go01-demo/warehouse/tablespace/external/...     PARQUET   \n",
       "2        0  s3a://go01-demo/warehouse/tablespace/external/...     PARQUET   \n",
       "3        0  s3a://go01-demo/warehouse/tablespace/external/...     PARQUET   \n",
       "\n",
       "   spec_id partition  record_count  file_size_in_bytes           column_sizes  \\\n",
       "0        0    (644,)             1                 976  {1: 33, 2: 34, 3: 39}   \n",
       "1        0    (642,)             1                 999  {1: 39, 2: 41, 3: 39}   \n",
       "2        0    (644,)             1                 999  {1: 39, 2: 41, 3: 39}   \n",
       "3        0    (639,)             1                 985  {1: 39, 2: 39, 3: 39}   \n",
       "\n",
       "         value_counts   null_value_counts nan_value_counts  \\\n",
       "0  {1: 1, 2: 1, 3: 1}  {1: 0, 2: 0, 3: 0}               {}   \n",
       "1  {1: 1, 2: 1, 3: 1}  {1: 0, 2: 0, 3: 0}               {}   \n",
       "2  {1: 1, 2: 1, 3: 1}  {1: 0, 2: 0, 3: 0}               {}   \n",
       "3  {1: 1, 2: 1, 3: 1}  {1: 0, 2: 0, 3: 0}               {}   \n",
       "\n",
       "                                        lower_bounds  \\\n",
       "0  {1: [1, 0, 0, 0, 0, 0, 0, 0], 2: [118, 101, 11...   \n",
       "1  {1: [2, 0, 0, 0, 0, 0, 0, 0], 2: [103, 114, 97...   \n",
       "2  {1: [1, 0, 0, 0, 0, 0, 0, 0], 2: [103, 114, 97...   \n",
       "3  {1: [3, 0, 0, 0, 0, 0, 0, 0], 2: [116, 97, 108...   \n",
       "\n",
       "                                        upper_bounds key_metadata  \\\n",
       "0  {1: [1, 0, 0, 0, 0, 0, 0, 0], 2: [118, 101, 11...         None   \n",
       "1  {1: [2, 0, 0, 0, 0, 0, 0, 0], 2: [103, 114, 97...         None   \n",
       "2  {1: [1, 0, 0, 0, 0, 0, 0, 0], 2: [103, 114, 97...         None   \n",
       "3  {1: [3, 0, 0, 0, 0, 0, 0, 0], 2: [116, 97, 108...         None   \n",
       "\n",
       "  split_offsets equality_ids  sort_order_id  \n",
       "0           [4]         None              0  \n",
       "1           [4]         None              0  \n",
       "2           [4]         None              0  \n",
       "3           [4]         None              0  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM lakehouse.coffees_table.all_data_files;\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67491c12-86aa-4ced-acf4-8926bfedd947",
   "metadata": {},
   "source": [
    "RECAP: The target table coffee was empty. We ran an insert and that created one new metadata file, one new manifest file, and one new data file.\n",
    "Then we ran a Merge Into from the staging table. Of the three rows that we compared against, one was a match and two were not.\n",
    "The one that was a match was rewritten to a brand new data file in the same partition (coffee_sale_ts_month=2023-09) which now has two data files.\n",
    "The remaining two rows that were not a match were written to two new data files in the two respective partitions. If they had fallen under the same partition they may have been written into the same data file (next example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1e53fd83-0c3f-4049-8b49-9563596a0feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are now the files in the Iceberg Metadata Layer:\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00000-be30580e-87c6-45b6-9480-2d6d8a3b47da.metadata.json\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00001-607ac9ea-b355-4064-b43a-321fa77bb56e.metadata.json\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00002-870aae3f-3044-4d3c-a377-acfc7d584e5e.metadata.json\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/1c96feb7-51dd-47a2-b08a-0c339b086082-m0.avro\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/46c20f3b-ab53-42fc-9f92-8bfa15aa081d-m0.avro\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/46c20f3b-ab53-42fc-9f92-8bfa15aa081d-m1.avro\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/snap-3743804437087925131-1-1c96feb7-51dd-47a2-b08a-0c339b086082.avro\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/snap-8285711116837308020-1-46c20f3b-ab53-42fc-9f92-8bfa15aa081d.avro\n",
      "Total number of files in the Metadata Layer: 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"These are now the files in the Iceberg Metadata Layer:\")\n",
    "catalogUtil.count_metadata_layer_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8061e9ab-274f-4dea-a4ae-82557ff80ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are now the metadata files in the Iceberg Metadata Layer:\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00000-be30580e-87c6-45b6-9480-2d6d8a3b47da.metadata.json\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00001-607ac9ea-b355-4064-b43a-321fa77bb56e.metadata.json\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00002-870aae3f-3044-4d3c-a377-acfc7d584e5e.metadata.json\n",
      "Total Number of Metadata Files: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"These are now the metadata files in the Iceberg Metadata Layer:\")\n",
    "catalogUtil.count_metadata_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c0f25040-ca4a-47d9-a4a4-b1041c0c8894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are now the manifest lists in the Iceberg Metadata Layer:\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/snap-3743804437087925131-1-1c96feb7-51dd-47a2-b08a-0c339b086082.avro\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/snap-8285711116837308020-1-46c20f3b-ab53-42fc-9f92-8bfa15aa081d.avro\n",
      "Total Number of Manifest Lists: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"These are now the manifest lists in the Iceberg Metadata Layer:\")\n",
    "catalogUtil.count_manifest_lists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b8c74805-0b51-4879-861e-3bbca8698479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are now the manifest files in the Iceberg Metadata Layer:\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/1c96feb7-51dd-47a2-b08a-0c339b086082-m0.avro\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/46c20f3b-ab53-42fc-9f92-8bfa15aa081d-m0.avro\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/46c20f3b-ab53-42fc-9f92-8bfa15aa081d-m1.avro\n",
      "Total Number of Manifest Files: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"These are now the manifest files in the Iceberg Metadata Layer:\")\n",
    "catalogUtil.count_manifest_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3015a87e-cd0d-4303-b3b9-f424ee1cba8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00000-be30580e-87c6-45b6-9480-2d6d8a3b47da.metadata.json\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00001-607ac9ea-b355-4064-b43a-321fa77bb56e.metadata.json\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00002-870aae3f-3044-4d3c-a377-acfc7d584e5e.metadata.json\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/1c96feb7-51dd-47a2-b08a-0c339b086082-m0.avro\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/46c20f3b-ab53-42fc-9f92-8bfa15aa081d-m0.avro\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/46c20f3b-ab53-42fc-9f92-8bfa15aa081d-m1.avro\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/snap-3743804437087925131-1-1c96feb7-51dd-47a2-b08a-0c339b086082.avro\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/snap-8285711116837308020-1-46c20f3b-ab53-42fc-9f92-8bfa15aa081d.avro\n"
     ]
    }
   ],
   "source": [
    "metadata_file_path_list = catalogUtil.get_all_metadata_layer_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "37363956-0414-4e18-95b6-4811d35571b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/data/coffee_sale_ts_month=2023-04/00156-860-51bc1b8c-3b36-40b2-ab1c-c3956b89469f-00001.parquet\n",
      "Data File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/data/coffee_sale_ts_month=2023-07/00011-833-347aaeb0-795a-4e94-9e29-c08eee837c78-00001.parquet\n",
      "Data File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/data/coffee_sale_ts_month=2023-09/00072-212-fd2d233b-96d1-4611-bbe4-677cedb22ad0-00001.parquet\n",
      "Data File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/data/coffee_sale_ts_month=2023-09/00072-841-14ecce1e-e231-45e0-8478-e460a3197248-00001.parquet\n"
     ]
    }
   ],
   "source": [
    "data_files_list = catalogUtil.get_all_data_layer_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "86a3bdd4-bb0b-42b3-8dce-b989ec81ee03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00000-be30580e-87c6-45b6-9480-2d6d8a3b47da.metadata.json\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00001-607ac9ea-b355-4064-b43a-321fa77bb56e.metadata.json\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00002-870aae3f-3044-4d3c-a377-acfc7d584e5e.metadata.json\n"
     ]
    }
   ],
   "source": [
    "metadata_file_path_list = catalogUtil.get_all_metadata_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a9527acf-a0a5-4ff4-9a83-5cca763aa281",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/IPython/core/display.py:911: UserWarning: JSON expects JSONable dict or list, not JSON strings\n",
      "  warnings.warn(\"JSON expects JSONable dict or list, not JSON strings\")\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "current-schema-id": 0,
       "current-snapshot-id": 8285711116837308000,
       "default-sort-order-id": 0,
       "default-spec-id": 0,
       "format-version": 2,
       "last-column-id": 3,
       "last-partition-id": 1000,
       "last-sequence-number": 2,
       "last-updated-ms": 1694209521010,
       "location": "s3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table",
       "metadata-log": [
        {
         "metadata-file": "s3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00000-be30580e-87c6-45b6-9480-2d6d8a3b47da.metadata.json",
         "timestamp-ms": 1694209476692
        },
        {
         "metadata-file": "s3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/00001-607ac9ea-b355-4064-b43a-321fa77bb56e.metadata.json",
         "timestamp-ms": 1694209492022
        }
       ],
       "partition-specs": [
        {
         "fields": [
          {
           "field-id": 1000,
           "name": "coffee_sale_ts_month",
           "source-id": 3,
           "transform": "month"
          }
         ],
         "spec-id": 0
        }
       ],
       "properties": {
        "owner": "pauldefusco",
        "write.delete.mode": "copy-on-write",
        "write.merge.mode": "copy-on-write",
        "write.update.mode": "copy-on-write"
       },
       "refs": {
        "main": {
         "snapshot-id": 8285711116837308000,
         "type": "branch"
        }
       },
       "schemas": [
        {
         "fields": [
          {
           "id": 1,
           "name": "coffee_id",
           "required": false,
           "type": "long"
          },
          {
           "id": 2,
           "name": "coffee_size",
           "required": false,
           "type": "string"
          },
          {
           "id": 3,
           "name": "coffee_sale_ts",
           "required": false,
           "type": "timestamptz"
          }
         ],
         "schema-id": 0,
         "type": "struct"
        }
       ],
       "snapshot-log": [
        {
         "snapshot-id": 3743804437087925000,
         "timestamp-ms": 1694209492022
        },
        {
         "snapshot-id": 8285711116837308000,
         "timestamp-ms": 1694209521010
        }
       ],
       "snapshots": [
        {
         "manifest-list": "s3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/snap-3743804437087925131-1-1c96feb7-51dd-47a2-b08a-0c339b086082.avro",
         "schema-id": 0,
         "sequence-number": 1,
         "snapshot-id": 3743804437087925000,
         "summary": {
          "added-data-files": "1",
          "added-files-size": "976",
          "added-records": "1",
          "changed-partition-count": "1",
          "operation": "append",
          "spark.app.id": "spark-application-1694209434520",
          "total-data-files": "1",
          "total-delete-files": "0",
          "total-equality-deletes": "0",
          "total-files-size": "976",
          "total-position-deletes": "0",
          "total-records": "1"
         },
         "timestamp-ms": 1694209492022
        },
        {
         "manifest-list": "s3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/snap-8285711116837308020-1-46c20f3b-ab53-42fc-9f92-8bfa15aa081d.avro",
         "parent-snapshot-id": 3743804437087925000,
         "schema-id": 0,
         "sequence-number": 2,
         "snapshot-id": 8285711116837308000,
         "summary": {
          "added-data-files": "3",
          "added-files-size": "2983",
          "added-records": "3",
          "changed-partition-count": "3",
          "deleted-data-files": "1",
          "deleted-records": "1",
          "operation": "overwrite",
          "removed-files-size": "976",
          "spark.app.id": "spark-application-1694209434520",
          "total-data-files": "3",
          "total-delete-files": "0",
          "total-equality-deletes": "0",
          "total-files-size": "2983",
          "total-position-deletes": "0",
          "total-records": "3"
         },
         "timestamp-ms": 1694209521010
        }
       ],
       "sort-orders": [
        {
         "fields": [],
         "order-id": 0
        }
       ],
       "statistics": [],
       "table-uuid": "ac664378-5621-476d-a899-ebddda4f4d27"
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 55,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalogUtil.print_metadata_file(spark, metadata_file_path_list[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3cd8b42d-d544-4d19-81f2-9b0677a6fe14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/snap-3743804437087925131-1-1c96feb7-51dd-47a2-b08a-0c339b086082.avro\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/snap-8285711116837308020-1-46c20f3b-ab53-42fc-9f92-8bfa15aa081d.avro\n"
     ]
    }
   ],
   "source": [
    "manifest_lists_list = catalogUtil.get_all_manifest_lists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c0733e43-0159-471c-b527-289a0d1a37a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "application/json": {
       "added_data_files_count": 3,
       "added_rows_count": 3,
       "added_snapshot_id": 8285711116837308000,
       "content": 0,
       "deleted_data_files_count": 0,
       "deleted_rows_count": 0,
       "existing_data_files_count": 0,
       "existing_rows_count": 0,
       "manifest_length": 7204,
       "manifest_path": "s3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/46c20f3b-ab53-42fc-9f92-8bfa15aa081d-m1.avro",
       "min_sequence_number": 2,
       "partition_spec_id": 0,
       "partitions": [
        {
         "contains_nan": false,
         "contains_null": false,
         "lower_bound": "fwIAAA==",
         "upper_bound": "hAIAAA=="
        }
       ],
       "sequence_number": 2
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 57,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalogUtil.print_manifest_file(spark, manifest_lists_list[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "20ebe917-3632-46b4-9d97-e20ede40b53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/1c96feb7-51dd-47a2-b08a-0c339b086082-m0.avro\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/46c20f3b-ab53-42fc-9f92-8bfa15aa081d-m0.avro\n",
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/metadata/46c20f3b-ab53-42fc-9f92-8bfa15aa081d-m1.avro\n"
     ]
    }
   ],
   "source": [
    "manifest_files_list = catalogUtil.get_all_manifest_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "66075c85-4377-4054-a04f-7b814e439b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "data_file": {
        "column_sizes": [
         {
          "key": 1,
          "value": 33
         },
         {
          "key": 2,
          "value": 34
         },
         {
          "key": 3,
          "value": 39
         }
        ],
        "content": 0,
        "file_format": "PARQUET",
        "file_path": "s3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table/data/coffee_sale_ts_month=2023-09/00072-212-fd2d233b-96d1-4611-bbe4-677cedb22ad0-00001.parquet",
        "file_size_in_bytes": 976,
        "lower_bounds": [
         {
          "key": 1,
          "value": "AQAAAAAAAAA="
         },
         {
          "key": 2,
          "value": "dmVudGk="
         },
         {
          "key": 3,
          "value": "AMhBOUkEBgA="
         }
        ],
        "nan_value_counts": [],
        "null_value_counts": [
         {
          "key": 1,
          "value": 0
         },
         {
          "key": 2,
          "value": 0
         },
         {
          "key": 3,
          "value": 0
         }
        ],
        "partition": {
         "coffee_sale_ts_month": 644
        },
        "record_count": 1,
        "sort_order_id": 0,
        "split_offsets": [
         4
        ],
        "upper_bounds": [
         {
          "key": 1,
          "value": "AQAAAAAAAAA="
         },
         {
          "key": 2,
          "value": "dmVudGk="
         },
         {
          "key": 3,
          "value": "AMhBOUkEBgA="
         }
        ],
        "value_counts": [
         {
          "key": 1,
          "value": 1
         },
         {
          "key": 2,
          "value": 1
         },
         {
          "key": 3,
          "value": 1
         }
        ]
       },
       "file_sequence_number": 1,
       "sequence_number": 1,
       "snapshot_id": 8285711116837308000,
       "status": 2
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalogUtil.print_manifest_file(spark, manifest_files_list[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "20b77e7d-5719-4a89-b392-ff43d4579c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "data_file": {
        "column_sizes": [
         {
          "key": 1,
          "value": 39
         },
         {
          "key": 2,
          "value": 41
         },
         {
          "key": 3,
          "value": 39
         }
        ],
        "content": 0,
        "file_format": "PARQUET",
        "file_path": "s3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table/data/coffee_sale_ts_month=2023-07/00011-833-347aaeb0-795a-4e94-9e29-c08eee837c78-00001.parquet",
        "file_size_in_bytes": 999,
        "lower_bounds": [
         {
          "key": 1,
          "value": "AgAAAAAAAAA="
         },
         {
          "key": 2,
          "value": "Z3JhbmRl"
         },
         {
          "key": 3,
          "value": "AHJv+Wr/BQA="
         }
        ],
        "nan_value_counts": [],
        "null_value_counts": [
         {
          "key": 1,
          "value": 0
         },
         {
          "key": 2,
          "value": 0
         },
         {
          "key": 3,
          "value": 0
         }
        ],
        "partition": {
         "coffee_sale_ts_month": 642
        },
        "record_count": 1,
        "sort_order_id": 0,
        "split_offsets": [
         4
        ],
        "upper_bounds": [
         {
          "key": 1,
          "value": "AgAAAAAAAAA="
         },
         {
          "key": 2,
          "value": "Z3JhbmRl"
         },
         {
          "key": 3,
          "value": "AHJv+Wr/BQA="
         }
        ],
        "value_counts": [
         {
          "key": 1,
          "value": 1
         },
         {
          "key": 2,
          "value": 1
         },
         {
          "key": 3,
          "value": 1
         }
        ]
       },
       "snapshot_id": 8285711116837308000,
       "status": 1
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 60,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalogUtil.print_manifest_file(spark, manifest_files_list[2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bb34d9d8-f821-49e2-8d16-bcff74240be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/data/coffee_sale_ts_month=2023-04/00156-860-51bc1b8c-3b36-40b2-ab1c-c3956b89469f-00001.parquet\n",
      "Data File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/data/coffee_sale_ts_month=2023-07/00011-833-347aaeb0-795a-4e94-9e29-c08eee837c78-00001.parquet\n",
      "Data File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/data/coffee_sale_ts_month=2023-09/00072-212-fd2d233b-96d1-4611-bbe4-677cedb22ad0-00001.parquet\n",
      "Data File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table/data/coffee_sale_ts_month=2023-09/00072-841-14ecce1e-e231-45e0-8478-e460a3197248-00001.parquet\n"
     ]
    }
   ],
   "source": [
    "data_files_list = catalogUtil.get_all_data_layer_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2d22d9e1-418e-4ab1-ba08-b49e971f62e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------------+\n",
      "|coffee_id|coffee_size|     coffee_sale_ts|\n",
      "+---------+-----------+-------------------+\n",
      "|        3|       tall|2023-04-01 12:01:00|\n",
      "+---------+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "catalogUtil.print_data_file(spark, data_files_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4999c930-f5c6-40f7-b4e8-604030e8b263",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 48:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------------+\n",
      "|coffee_id|coffee_size|     coffee_sale_ts|\n",
      "+---------+-----------+-------------------+\n",
      "|        2|     grande|2023-07-01 11:10:00|\n",
      "+---------+-----------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "catalogUtil.print_data_file(spark, data_files_list[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0e4b41-8570-45a9-ac95-a8bdc9f47189",
   "metadata": {},
   "source": [
    "NB: The row with coffee_id = 1 was a match between the target and staging tables. Because COW was used, a new file was written with the same row and the updated value for the \"coffee_size\" field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9ac839e3-251b-4683-9a5a-3c91e44d3198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new data file with the updated row (size grande):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'warehouse/tablespace/external/hive/lakehouse.db/coffees_table/data/coffee_sale_ts_month=2023-09/00072-212-fd2d233b-96d1-4611-bbe4-677cedb22ad0-00001.parquet'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The new data file with the updated row (size grande):\")\n",
    "data_files_list[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "678965bc-ecb8-4cda-b6fc-7205b375d3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------------+\n",
      "|coffee_id|coffee_size|     coffee_sale_ts|\n",
      "+---------+-----------+-------------------+\n",
      "|        1|      venti|2023-09-01 10:00:00|\n",
      "+---------+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "catalogUtil.print_data_file(spark, data_files_list[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8c35716a-e747-4bef-a8fb-3af9d3f1bc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The old data file with the original row (size venti):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'warehouse/tablespace/external/hive/lakehouse.db/coffees_table/data/coffee_sale_ts_month=2023-09/00072-841-14ecce1e-e231-45e0-8478-e460a3197248-00001.parquet'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The old data file with the original row (size venti):\")\n",
    "data_files_list[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9c8e4c66-ed74-4496-8b23-1bfc77e7e260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------------+\n",
      "|coffee_id|coffee_size|     coffee_sale_ts|\n",
      "+---------+-----------+-------------------+\n",
      "|        1|     grande|2023-09-01 10:00:00|\n",
      "+---------+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "catalogUtil.print_data_file(spark, data_files_list[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f6b01f74-4cef-4ade-8f16-1a1b4a19eec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------------+\n",
      "|coffee_id|coffee_size|     coffee_sale_ts|\n",
      "+---------+-----------+-------------------+\n",
      "|        2|     grande|2023-07-01 11:10:00|\n",
      "|        1|     grande|2023-09-01 10:00:00|\n",
      "|        3|       tall|2023-04-01 12:01:00|\n",
      "+---------+-----------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/08 22:31:08 WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed (this is expected if the application is shutting down.)\n",
      "io.fabric8.kubernetes.client.WatcherException: too old resource version: 15832225 (15840015)\n",
      "\tat io.fabric8.kubernetes.client.dsl.internal.AbstractWatchManager.onStatus(AbstractWatchManager.java:265)\n",
      "\tat io.fabric8.kubernetes.client.dsl.internal.AbstractWatchManager.onMessage(AbstractWatchManager.java:249)\n",
      "\tat io.fabric8.kubernetes.client.dsl.internal.WatcherWebSocketListener.onMessage(WatcherWebSocketListener.java:93)\n",
      "\tat okhttp3.internal.ws.RealWebSocket.onReadMessage(RealWebSocket.java:323)\n",
      "\tat okhttp3.internal.ws.WebSocketReader.readMessageFrame(WebSocketReader.java:219)\n",
      "\tat okhttp3.internal.ws.WebSocketReader.processNextFrame(WebSocketReader.java:105)\n",
      "\tat okhttp3.internal.ws.RealWebSocket.loopReader(RealWebSocket.java:274)\n",
      "\tat okhttp3.internal.ws.RealWebSocket$2.onResponse(RealWebSocket.java:214)\n",
      "\tat okhttp3.RealCall$AsyncCall.execute(RealCall.java:203)\n",
      "\tat okhttp3.internal.NamedRunnable.run(NamedRunnable.java:32)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: io.fabric8.kubernetes.client.KubernetesClientException: too old resource version: 15832225 (15840015)\n",
      "\t... 13 more\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM lakehouse.coffees_table\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684e0727-c58d-4f37-a682-7748f566e205",
   "metadata": {},
   "source": [
    "Adding a partition field is a metadata operation and does not change any of the existing table data. New data will be written with the new partitioning, but existing data will remain in the old partition layout. Old data files will have null values for the new partition fields in metadata tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2c9fe1-7594-4909-8744-59dde048eb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TABLE PARTITIONS BEFORE ALTER PARTITION STATEMENT: \")\n",
    "spark.sql(\"SELECT * FROM spark_catalog.lakehouse.p_evol_tbl.PARTITIONS\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dd4330-e06c-46fa-87b4-3624555daa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ADD PARTITION BY EVENT TIMESTAMP MONTHS: \")\n",
    "print(\"ALTER TABLE spark_catalog.lakehouse.p_evol_tbl ADD PARTITION FIELD months(event_ts)\")\n",
    "spark.sql(\"ALTER TABLE spark_catalog.lakehouse.p_evol_tbl ADD PARTITION FIELD months(event_ts)\")\n",
    "#spark.sql(\"ALTER TABLE spark_catalog.lakehouse.part_evol_tbl REPLACE PARTITION FIELD hours(dob) WITH state\")\n",
    "#spark.sql(\"ALTER TABLE prod.db.sample ADD PARTITION FIELD month\")\n",
    "\n",
    "#ALTER TABLE spark_catalog.lakehouse.part_evol_tbl ADD PARTITION FIELD days(event_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee150f5-9d59-4b23-8df3-e6f5d70e2fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TABLE PARTITIONS AFTER ALTER PARTITION STATEMENT: \")\n",
    "spark.sql(\"SELECT * FROM spark_catalog.lakehouse.p_evol_tbl.PARTITIONS\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa64157-5a82-4c6d-82f8-f94bd08e8601",
   "metadata": {},
   "outputs": [],
   "source": [
    "appendDf = dfTestData.sample(fraction=0.3, seed=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494691b4-e56a-41d5-98e3-0f033f26d59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "appendDf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f2ca29-d54b-4aba-a9f8-ad1019616b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "appendDf.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33af64b-c2ca-447f-a0b7-185e223d973c",
   "metadata": {},
   "outputs": [],
   "source": [
    "appendDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea0b278-5cb4-4bc3-aff9-f988b338c9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "appendDf.sortWithinPartitions(\"country\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf4dcea-b8d5-43a9-ae67-b3aebf07d938",
   "metadata": {},
   "outputs": [],
   "source": [
    "#appendDf.sortWithinPartitions(\"country\", \"month(event_ts)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e1c1ea-6949-49e1-a6ea-5022033abc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "appendDf.sortWithinPartitions(\"country\").writeTo(\"spark_catalog.lakehouse.p_evol_tbl\").using(\"iceberg\").append() #.append()#replace()#overwritePartitions()#create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef167cb7-a230-4d45-8619-87090d850789",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TABLE PARTITIONS AFTER APPEND: \")\n",
    "spark.sql(\"SELECT * FROM spark_catalog.lakehouse.p_evol_tbl.PARTITIONS\").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ef51fc-950b-4a96-85c3-6cdb7c472e37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5406060-fea0-4002-a915-be5bd29aa08b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b3860d-d5b9-47f5-b494-0ffe5f0d190e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a009cb8e-e564-40aa-bebf-e1229b4c5341",
   "metadata": {},
   "source": [
    "Dropping a partition field is a metadata operation and does not change any of the existing table data. New data will be written with the new partitioning, but existing data will remain in the old partition layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7e0fb9-374e-4965-93da-776e91c2b787",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"ALTER TABLE spark_catalog.lakehouse.part_evol_tbl DROP PARTITION FIELD bucket(16, device_id)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5340bace-0227-48b8-b984-d0136381f672",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TABLE PARTITIONS AFTER ALTER PARTITION STATEMENT: \")\n",
    "spark.sql(\"SELECT * FROM spark_catalog.lakehouse.part_evol_tbl.PARTITIONS\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068a2029-59ec-44dd-bbe0-1ba04777548e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8512c690-0009-47a8-8046-7477ed7b99b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144ebaf9-0d8c-4ff0-90fb-516b9b2f97a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e57e5ac-497b-4692-afb3-d28f4d54d5fb",
   "metadata": {},
   "source": [
    "##### Only json files have been added (one per each time you repartitioned) but Avro files have stayed the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489132be-d671-46a6-92c5-6dc291638f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket(\"go01-demo\")\n",
    "\n",
    "metadata_file_list = []\n",
    "\n",
    "print(\"Current Metadata Files: \\n\")\n",
    "\n",
    "for object_summary in my_bucket.objects.filter(Prefix=metadata_path+\"/metadata\"):\n",
    "    #print(object_summary.key +\"\\n\")\n",
    "    metadata_file_list.append(object_summary.key)\n",
    "    \n",
    "metadata_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6af64da-19fc-47b3-afd8-5a938704ee11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1b5aed-4bba-482d-ac22-8e0f30cb970a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abd06d2-fb84-4a96-ab3d-9c9697cd85ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b998fc45-091b-4776-81d0-402aeb8673e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162a1ede-72a3-40bf-9cbf-1ae4a410832f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc464b86-ebea-41a7-9afe-6917f262b39c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd33dd11-1439-408e-912b-dc65d7f12fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE TABLE IF NOT EXISTS customer_table (id BIGINT, state STRING, country STRING, dob TIMESTAMP) USING iceberg PARTITIONED BY ( hours(dob))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7930a494-94d8-4fee-b873-8120257fbbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT HOUR(dob) FROM spark_catalog.lakehouse.customer_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485b48f5-4eaf-4edf-ad66-2a30a19b6c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT DAY(dob) FROM spark_catalog.lakehouse.customer_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178a933e-5ee3-4a64-93e0-85fac5dc544c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0086fbd-2b28-4df6-baf9-17768c5a7b9b",
   "metadata": {},
   "source": [
    "### Dropping Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abab5842-1722-470e-8066-c58e7659192e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS lakehouse.staging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f903baae-7c19-45a9-95d3-79d929491ed2",
   "metadata": {},
   "source": [
    "Validate that the metadata folder is now empty but the data folder still retains parquet files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fef6a1-ca58-48d2-83c2-e4e66dc2e3b1",
   "metadata": {},
   "source": [
    "![alt text](../img/s3_droptable_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eba9f0-a03c-4c17-bb4f-d1b9e6404a97",
   "metadata": {},
   "source": [
    "![alt text](../img/s3_droptable_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de2b0a5-8b24-4055-8250-d2ce689949c3",
   "metadata": {},
   "source": [
    "![alt text](../img/s3_droptable_3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43739d1f-8682-4575-98de-58f4ca4416f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"ALTER TABLE lakehouse.customers_table\\\n",
    "            SET TBLPROPERTIES ('format-version' = '2')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38b4f0c-f054-4721-9789-5e3968be2985",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket(\"go01-demo\")\n",
    "\n",
    "metadata_file_list = []\n",
    "\n",
    "for object_summary in my_bucket.objects.filter(Prefix=metadata_path):\n",
    "    print(object_summary.key +\"\\n\")\n",
    "    metadata_file_list.append(object_summary.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ba146b-fdfb-41e3-bc75-65ce5c4c5bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Showing \" + metadata_file_list[3])\n",
    "spark.read.option(\"multiline\",\"true\").json(\"s3a://go01-demo/\" + metadata_file_list[3]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdce88a-df1c-4e93-960e-fcf732bb9df0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
