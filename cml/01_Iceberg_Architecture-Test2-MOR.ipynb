{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0942afe9-7629-4b3e-b747-b795a6454a2b",
   "metadata": {},
   "source": [
    "## Introduction to Iceberg Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f7e7d5e-5a02-414b-9177-49cdfe38fddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cmlbootstrap\n",
      "  Cloning https://github.com/fastforwardlabs/cmlbootstrap to /tmp/pip-install-sd37j61q/cmlbootstrap_f87bc57457124a2bacc9a6c53274f060\n",
      "  Running command git clone -q https://github.com/fastforwardlabs/cmlbootstrap /tmp/pip-install-sd37j61q/cmlbootstrap_f87bc57457124a2bacc9a6c53274f060\n",
      "Requirement already satisfied: pandas in /home/cdsw/.local/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (1.3.5)\n",
      "Requirement already satisfied: numpy==1.19.0 in /home/cdsw/.local/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (1.19.0)\n",
      "Requirement already satisfied: boto3 in /home/cdsw/.local/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (1.17.62)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 5)) (2.25.1)\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 6)) (1.26.6)\n",
      "Requirement already satisfied: dbldatagen in /home/cdsw/.local/lib/python3.7/site-packages (from -r requirements.txt (line 7)) (0.2.1)\n",
      "Requirement already satisfied: PyArrow in /home/cdsw/.local/lib/python3.7/site-packages (from -r requirements.txt (line 8)) (12.0.0)\n",
      "Requirement already satisfied: requests-kerberos==0.12.0 in /home/cdsw/.local/lib/python3.7/site-packages (from cmlbootstrap->-r requirements.txt (line 4)) (0.12.0)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.62 in /home/cdsw/.local/lib/python3.7/site-packages (from boto3->-r requirements.txt (line 3)) (1.20.112)\n",
      "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /home/cdsw/.local/lib/python3.7/site-packages (from boto3->-r requirements.txt (line 3)) (0.4.2)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/cdsw/.local/lib/python3.7/site-packages (from boto3->-r requirements.txt (line 3)) (0.10.0)\n",
      "Requirement already satisfied: cryptography>=1.3 in /home/cdsw/.local/lib/python3.7/site-packages (from requests-kerberos==0.12.0->cmlbootstrap->-r requirements.txt (line 4)) (40.0.2)\n",
      "Requirement already satisfied: pykerberos<2.0.0,>=1.1.8 in /home/cdsw/.local/lib/python3.7/site-packages (from requests-kerberos==0.12.0->cmlbootstrap->-r requirements.txt (line 4)) (1.2.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests->-r requirements.txt (line 5)) (2020.11.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests->-r requirements.txt (line 5)) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests->-r requirements.txt (line 5)) (2.10)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/site-packages (from botocore<1.21.0,>=1.20.62->boto3->-r requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/site-packages (from cryptography>=1.3->requests-kerberos==0.12.0->cmlbootstrap->-r requirements.txt (line 4)) (1.14.4)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/site-packages (from cffi>=1.12->cryptography>=1.3->requests-kerberos==0.12.0->cmlbootstrap->-r requirements.txt (line 4)) (2.20)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.62->boto3->-r requirements.txt (line 3)) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 1)) (2021.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd15c6f-e31d-440c-bc48-966385b7c5ec",
   "metadata": {},
   "source": [
    "#### Launching a Spark Session with Iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f139b6a-eed6-4848-9e82-593aabad58b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 03:00:41 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "23/09/04 03:00:41 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "Setting spark.hadoop.yarn.resourcemanager.principal to pauldefusco\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/04 03:00:42 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "23/09/04 03:00:42 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "23/09/04 03:00:42 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "23/09/04 03:00:43 WARN Utils: Service 'SparkUI' could not bind on port 20049. Attempting port 20050.\n",
      "23/09/04 03:00:43 WARN Utils: Service 'SparkUI' could not bind on port 20050. Attempting port 20051.\n",
      "23/09/04 03:00:44 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "23/09/04 03:00:45 WARN HiveServer2CredentialProvider: Failed to get HS2 delegation token\n",
      "java.util.NoSuchElementException: spark.sql.hive.hiveserver2.jdbc.url\n",
      "\tat org.apache.spark.SparkConf.$anonfun$get$1(SparkConf.scala:245)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.SparkConf.get(SparkConf.scala:245)\n",
      "\tat com.hortonworks.spark.deploy.yarn.security.HiveServer2CredentialProvider.obtainDelegationTokens(HiveServer2CredentialProvider.scala:64)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager.$anonfun$obtainDelegationTokens$2(HadoopDelegationTokenManager.scala:164)\n",
      "\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat scala.collection.MapLike$DefaultValuesIterable.foreach(MapLike.scala:213)\n",
      "\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)\n",
      "\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)\n",
      "\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager.org$apache$spark$deploy$security$HadoopDelegationTokenManager$$obtainDelegationTokens(HadoopDelegationTokenManager.scala:162)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager$$anon$4.run(HadoopDelegationTokenManager.scala:226)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager$$anon$4.run(HadoopDelegationTokenManager.scala:224)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager.obtainTokensAndScheduleRenewal(HadoopDelegationTokenManager.scala:224)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager.org$apache$spark$deploy$security$HadoopDelegationTokenManager$$updateTokensTask(HadoopDelegationTokenManager.scala:198)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager.start(HadoopDelegationTokenManager.scala:123)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.$anonfun$start$1(CoarseGrainedSchedulerBackend.scala:552)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.$anonfun$start$1$adapted(CoarseGrainedSchedulerBackend.scala:549)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.start(CoarseGrainedSchedulerBackend.scala:549)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.KubernetesClusterSchedulerBackend.start(KubernetesClusterSchedulerBackend.scala:95)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:220)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:581)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/04 03:00:47 WARN ExecutorAllocationManager: Dynamic allocation without a shuffle service is an experimental feature.\n",
      "23/09/04 03:00:49 WARN HiveClientImpl: Detected HiveConf hive.execution.engine is 'tez' and will be reset to 'mr' to disable useless hive logic\n",
      "Hive Session ID = 031533c2-793b-418a-a3a0-fa5407cf5d99\n",
      "23/09/04 03:00:50 WARN HiveMetaStoreClient: Failed to connect to the MetaStore Server...\n",
      "23/09/04 03:00:56 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|         01_car_data|\n",
      "|           01_car_dw|\n",
      "|              adb101|\n",
      "|            airlines|\n",
      "|        airlines_csv|\n",
      "|    airlines_iceberg|\n",
      "|airlines_iceberg_...|\n",
      "|      airlines_mjain|\n",
      "|          airquality|\n",
      "|                ajvp|\n",
      "|          atlas_demo|\n",
      "|            bankdemo|\n",
      "|          bca_jps_l0|\n",
      "|        cde_workshop|\n",
      "|             cdedemo|\n",
      "|        cdp_overview|\n",
      "|      ceht_open_data|\n",
      "|        ceht_scratch|\n",
      "| ceht_transportation|\n",
      "|        cgsifacebook|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cml.data_v1 as cmldata\n",
    "\n",
    "CONNECTION_NAME = \"go01-aw-dl\"\n",
    "conn = cmldata.get_connection(CONNECTION_NAME)\n",
    "spark = conn.get_spark_session()\n",
    "\n",
    "# Sample usage to run query through spark\n",
    "EXAMPLE_SQL_QUERY = \"show databases\"\n",
    "spark.sql(EXAMPLE_SQL_QUERY).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3bbf1bf-a0a1-4864-8c98-88dfb994060a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 03:00:56 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "23/09/04 03:00:56 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('spark.kubernetes.driver.pod.name', '5esiffpo4nllmdwy'),\n",
       " ('spark.eventLog.enabled', 'true'),\n",
       " ('spark.network.crypto.enabled', 'true'),\n",
       " ('spark.sql.hive.hwc.execution.mode', 'spark'),\n",
       " ('spark.jars',\n",
       "  '/opt/spark/optional-lib/hive-warehouse-connector-assembly.jar,/opt/spark/optional-lib/iceberg-hive-runtime.jar,/opt/spark/optional-lib/iceberg-spark-runtime.jar'),\n",
       " ('spark.kerberos.renewal.credentials', 'ccache'),\n",
       " ('spark.sql.catalog.spark_catalog',\n",
       "  'org.apache.iceberg.spark.SparkSessionCatalog'),\n",
       " ('spark.dynamicAllocation.maxExecutors', '49'),\n",
       " ('spark.eventLog.dir', 'file:///sparkeventlogs'),\n",
       " ('spark.hadoop.yarn.resourcemanager.principal', 'pauldefusco'),\n",
       " ('spark.app.startTime', '1693796442306'),\n",
       " ('spark.ui.port', '20049'),\n",
       " ('spark.app.id', 'spark-application-1693796444041'),\n",
       " ('spark.driver.bindAddress', '100.100.216.56'),\n",
       " ('spark.kubernetes.driver.annotation.cluster-autoscaler.kubernetes.io/safe-to-evict',\n",
       "  'false'),\n",
       " ('spark.yarn.access.hadoopFileSystems',\n",
       "  's3a://go01-demo/warehouse/tablespace/external/hive'),\n",
       " ('spark.sql.extensions',\n",
       "  'com.qubole.spark.hiveacid.HiveAcidAutoConvertExtension,org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions'),\n",
       " ('spark.kubernetes.executor.annotation.cluster-autoscaler.kubernetes.io/safe-to-evict',\n",
       "  'false'),\n",
       " ('spark.driver.memory', '3051m'),\n",
       " ('spark.app.initial.jar.urls',\n",
       "  'spark://100.100.216.56:45179/jars/iceberg-hive-runtime-1.1.0.1.19.7215.0-118.jar,spark://100.100.216.56:45179/jars/iceberg-spark-runtime-3.2_2.12-1.1.0.1.19.7215.0-118.jar,spark://100.100.216.56:45179/jars/hive-warehouse-connector-assembly-1.0.0.1.19.7215.0-118.jar'),\n",
       " ('spark.io.encryption.enabled', 'true'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.driver.host', '100.100.216.56'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.master', 'k8s://https://172.20.0.1:443'),\n",
       " ('spark.ui.allowFramingFrom',\n",
       "  'https://ml-b74f8940-b97.go01-dem.ylcu-atmi.cloudera.site'),\n",
       " ('spark.kubernetes.executor.podTemplateFile', '/tmp/spark-executor.json'),\n",
       " ('spark.kubernetes.container.image',\n",
       "  'docker.repository.cloudera.com/cloudera/cdsw/ml-runtime-jupyterlab-python3.7-standard:2022.11.1-b2'),\n",
       " ('spark.dynamicAllocation.shuffleTracking.enabled', 'true'),\n",
       " ('spark.kubernetes.namespace', 'mlx-user-16'),\n",
       " ('spark.ui.proxyRedirectUri',\n",
       "  'https://spark-5esiffpo4nllmdwy.ml-b74f8940-b97.go01-dem.ylcu-atmi.cloudera.site'),\n",
       " ('spark.kubernetes.executor.config.dir', '/var/spark/conf'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  's3a://go01-demo/warehouse/tablespace/external/hive'),\n",
       " ('spark.hadoop.iceberg.engine.hive.enabled', 'true'),\n",
       " ('spark.driver.port', '45179'),\n",
       " ('spark.repl.local.jars',\n",
       "  'file:///runtime-addons/spark323-19-hf2-g3aapf/opt/spark/optional-lib/hive-warehouse-connector-assembly-1.0.0.1.19.7215.0-118.jar,file:///runtime-addons/spark323-19-hf2-g3aapf/opt/spark/optional-lib/iceberg-hive-runtime-1.1.0.1.19.7215.0-118.jar,file:///runtime-addons/spark323-19-hf2-g3aapf/opt/spark/optional-lib/iceberg-spark-runtime-3.2_2.12-1.1.0.1.19.7215.0-118.jar'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.yarn.rmProxy.enabled', 'false'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.kryo.registrator',\n",
       "  'com.qubole.spark.hiveacid.util.HiveAcidKyroRegistrator'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.dynamicAllocation.enabled', 'true'),\n",
       " ('spark.deploy.mode', 'client'),\n",
       " ('spark.app.name', 'None'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.sql.catalog.spark_catalog.type', 'hive'),\n",
       " ('spark.authenticate', 'true'),\n",
       " ('spark.kubernetes.executor.podNamePrefix', 'cdsw-5esiffpo4nllmdwy')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa007770-01e4-4c2e-a784-6955c448c952",
   "metadata": {},
   "source": [
    "### Iceberg Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad5615b-c7e6-4528-bd48-db0841d38c39",
   "metadata": {},
   "source": [
    "![alt text](../img/iceberg-metadata.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c3c93c-4065-422b-9697-609ae50687c7",
   "metadata": {},
   "source": [
    "#### Iceberg Catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30001dd5-c3ef-4223-84ca-e63552c4f879",
   "metadata": {},
   "source": [
    "Iceberg comes with catalogs that enable SQL commands to manage tables and load them by name. Catalogs are configured using properties under spark.sql.catalog.(catalog_name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3819a08-8e00-4184-bdc6-79b3d8507d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+\n",
      "|      catalog|namespace|\n",
      "+-------------+---------+\n",
      "|spark_catalog|  default|\n",
      "+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show catalog and database\n",
    "spark.sql(\"SHOW CURRENT NAMESPACE\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1119faa-93a7-4ebe-85b1-3ec87eb54b7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new database\n",
    "#spark.sql(\"DROP DATABASE IF EXISTS spark_catalog.lakehouse\")\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS spark_catalog.lakehouse\")\n",
    "spark.sql(\"USE spark_catalog.lakehouse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "834ca50c-9c2f-43ac-9ec0-9cf28e8507d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+\n",
      "|      catalog|namespace|\n",
      "+-------------+---------+\n",
      "|spark_catalog|lakehouse|\n",
      "+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show catalog and database\n",
    "spark.sql(\"SHOW CURRENT NAMESPACE\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6cecda-848a-4cbe-b7a8-d73096cab11f",
   "metadata": {},
   "source": [
    "#### Create an Iceberg Table with Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c5daa62-cde3-440a-ad91-0c6f9b41ef3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 03:00:56 WARN HiveMetaStoreClient: Failed to connect to the MetaStore Server...\n",
      "23/09/04 03:01:00 WARN ExecutorPodsSnapshotsStoreImpl: Exception when notifying snapshot subscriber.\n",
      "io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://172.20.0.1/api/v1/namespaces/mlx-user-16/pods. Message: pods \"cdsw-5esiffpo4nllmdwy-exec-1\" already exists. Received status: Status(apiVersion=v1, code=409, details=StatusDetails(causes=[], group=null, kind=pods, name=cdsw-5esiffpo4nllmdwy-exec-1, retryAfterSeconds=null, uid=null, additionalProperties={}), kind=Status, message=pods \"cdsw-5esiffpo4nllmdwy-exec-1\" already exists, metadata=ListMeta(_continue=null, remainingItemCount=null, resourceVersion=null, selfLink=null, additionalProperties={}), reason=AlreadyExists, status=Failure, additionalProperties={}).\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:681)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.assertResponseCode(OperationSupport.java:620)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:560)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:521)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleCreate(OperationSupport.java:309)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleCreate(BaseOperation.java:773)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.BaseOperation.create(BaseOperation.java:348)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.BaseOperation.create(BaseOperation.java:88)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$requestNewExecutors$1(ExecutorPodsAllocator.scala:420)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.requestNewExecutors(ExecutorPodsAllocator.scala:402)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$onNewSnapshots$36(ExecutorPodsAllocator.scala:366)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$onNewSnapshots$36$adapted(ExecutorPodsAllocator.scala:351)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.onNewSnapshots(ExecutorPodsAllocator.scala:351)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$start$3(ExecutorPodsAllocator.scala:129)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$start$3$adapted(ExecutorPodsAllocator.scala:129)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsSnapshotsStoreImpl$SnapshotsSubscriber.org$apache$spark$scheduler$cluster$k8s$ExecutorPodsSnapshotsStoreImpl$SnapshotsSubscriber$$processSnapshotsInternal(ExecutorPodsSnapshotsStoreImpl.scala:138)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsSnapshotsStoreImpl$SnapshotsSubscriber.processSnapshots(ExecutorPodsSnapshotsStoreImpl.scala:126)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsSnapshotsStoreImpl$$anon$1.run(ExecutorPodsSnapshotsStoreImpl.scala:90)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/04 03:01:16 WARN ExecutorPodsSnapshotsStoreImpl: Exception when notifying snapshot subscriber.\n",
      "io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://172.20.0.1/api/v1/namespaces/mlx-user-16/pods. Message: pods \"cdsw-5esiffpo4nllmdwy-exec-3\" already exists. Received status: Status(apiVersion=v1, code=409, details=StatusDetails(causes=[], group=null, kind=pods, name=cdsw-5esiffpo4nllmdwy-exec-3, retryAfterSeconds=null, uid=null, additionalProperties={}), kind=Status, message=pods \"cdsw-5esiffpo4nllmdwy-exec-3\" already exists, metadata=ListMeta(_continue=null, remainingItemCount=null, resourceVersion=null, selfLink=null, additionalProperties={}), reason=AlreadyExists, status=Failure, additionalProperties={}).\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:681)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.assertResponseCode(OperationSupport.java:620)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:560)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:521)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleCreate(OperationSupport.java:309)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleCreate(BaseOperation.java:773)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.BaseOperation.create(BaseOperation.java:348)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.BaseOperation.create(BaseOperation.java:88)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$requestNewExecutors$1(ExecutorPodsAllocator.scala:420)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.requestNewExecutors(ExecutorPodsAllocator.scala:402)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$onNewSnapshots$36(ExecutorPodsAllocator.scala:366)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$onNewSnapshots$36$adapted(ExecutorPodsAllocator.scala:351)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.onNewSnapshots(ExecutorPodsAllocator.scala:351)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$start$3(ExecutorPodsAllocator.scala:129)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$start$3$adapted(ExecutorPodsAllocator.scala:129)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsSnapshotsStoreImpl$SnapshotsSubscriber.org$apache$spark$scheduler$cluster$k8s$ExecutorPodsSnapshotsStoreImpl$SnapshotsSubscriber$$processSnapshotsInternal(ExecutorPodsSnapshotsStoreImpl.scala:138)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsSnapshotsStoreImpl$SnapshotsSubscriber.processSnapshots(ExecutorPodsSnapshotsStoreImpl.scala:126)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsSnapshotsStoreImpl$$anon$1.run(ExecutorPodsSnapshotsStoreImpl.scala:90)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/04 03:01:17 WARN ExecutorPodsSnapshotsStoreImpl: Exception when notifying snapshot subscriber.\n",
      "io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://172.20.0.1/api/v1/namespaces/mlx-user-16/pods. Message: pods \"cdsw-5esiffpo4nllmdwy-exec-4\" already exists. Received status: Status(apiVersion=v1, code=409, details=StatusDetails(causes=[], group=null, kind=pods, name=cdsw-5esiffpo4nllmdwy-exec-4, retryAfterSeconds=null, uid=null, additionalProperties={}), kind=Status, message=pods \"cdsw-5esiffpo4nllmdwy-exec-4\" already exists, metadata=ListMeta(_continue=null, remainingItemCount=null, resourceVersion=null, selfLink=null, additionalProperties={}), reason=AlreadyExists, status=Failure, additionalProperties={}).\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:681)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.assertResponseCode(OperationSupport.java:620)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:560)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:521)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleCreate(OperationSupport.java:309)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleCreate(BaseOperation.java:773)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.BaseOperation.create(BaseOperation.java:348)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.BaseOperation.create(BaseOperation.java:88)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$requestNewExecutors$1(ExecutorPodsAllocator.scala:420)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.requestNewExecutors(ExecutorPodsAllocator.scala:402)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$onNewSnapshots$36(ExecutorPodsAllocator.scala:366)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$onNewSnapshots$36$adapted(ExecutorPodsAllocator.scala:351)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.onNewSnapshots(ExecutorPodsAllocator.scala:351)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$start$3(ExecutorPodsAllocator.scala:129)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$start$3$adapted(ExecutorPodsAllocator.scala:129)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsSnapshotsStoreImpl$SnapshotsSubscriber.org$apache$spark$scheduler$cluster$k8s$ExecutorPodsSnapshotsStoreImpl$SnapshotsSubscriber$$processSnapshotsInternal(ExecutorPodsSnapshotsStoreImpl.scala:138)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsSnapshotsStoreImpl$SnapshotsSubscriber.processSnapshots(ExecutorPodsSnapshotsStoreImpl.scala:126)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsSnapshotsStoreImpl.$anonfun$addSubscriber$1(ExecutorPodsSnapshotsStoreImpl.scala:81)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/04 03:01:17 WARN ExecutorPodsSnapshotsStoreImpl: Exception when notifying snapshot subscriber.\n",
      "io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://172.20.0.1/api/v1/namespaces/mlx-user-16/pods. Message: pods \"cdsw-5esiffpo4nllmdwy-exec-5\" already exists. Received status: Status(apiVersion=v1, code=409, details=StatusDetails(causes=[], group=null, kind=pods, name=cdsw-5esiffpo4nllmdwy-exec-5, retryAfterSeconds=null, uid=null, additionalProperties={}), kind=Status, message=pods \"cdsw-5esiffpo4nllmdwy-exec-5\" already exists, metadata=ListMeta(_continue=null, remainingItemCount=null, resourceVersion=null, selfLink=null, additionalProperties={}), reason=AlreadyExists, status=Failure, additionalProperties={}).\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:681)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.assertResponseCode(OperationSupport.java:620)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:560)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:521)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleCreate(OperationSupport.java:309)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleCreate(BaseOperation.java:773)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.BaseOperation.create(BaseOperation.java:348)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.BaseOperation.create(BaseOperation.java:88)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$requestNewExecutors$1(ExecutorPodsAllocator.scala:420)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.requestNewExecutors(ExecutorPodsAllocator.scala:402)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$onNewSnapshots$36(ExecutorPodsAllocator.scala:366)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$onNewSnapshots$36$adapted(ExecutorPodsAllocator.scala:351)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.onNewSnapshots(ExecutorPodsAllocator.scala:351)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$start$3(ExecutorPodsAllocator.scala:129)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$start$3$adapted(ExecutorPodsAllocator.scala:129)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsSnapshotsStoreImpl$SnapshotsSubscriber.org$apache$spark$scheduler$cluster$k8s$ExecutorPodsSnapshotsStoreImpl$SnapshotsSubscriber$$processSnapshotsInternal(ExecutorPodsSnapshotsStoreImpl.scala:138)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsSnapshotsStoreImpl$SnapshotsSubscriber.processSnapshots(ExecutorPodsSnapshotsStoreImpl.scala:126)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsSnapshotsStoreImpl$$anon$1.run(ExecutorPodsSnapshotsStoreImpl.scala:90)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/04 03:01:18 WARN ExecutorPodsSnapshotsStoreImpl: Exception when notifying snapshot subscriber.\n",
      "io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://172.20.0.1/api/v1/namespaces/mlx-user-16/pods. Message: pods \"cdsw-5esiffpo4nllmdwy-exec-7\" already exists. Received status: Status(apiVersion=v1, code=409, details=StatusDetails(causes=[], group=null, kind=pods, name=cdsw-5esiffpo4nllmdwy-exec-7, retryAfterSeconds=null, uid=null, additionalProperties={}), kind=Status, message=pods \"cdsw-5esiffpo4nllmdwy-exec-7\" already exists, metadata=ListMeta(_continue=null, remainingItemCount=null, resourceVersion=null, selfLink=null, additionalProperties={}), reason=AlreadyExists, status=Failure, additionalProperties={}).\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:681)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.assertResponseCode(OperationSupport.java:620)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:560)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:521)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleCreate(OperationSupport.java:309)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleCreate(BaseOperation.java:773)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.BaseOperation.create(BaseOperation.java:348)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.BaseOperation.create(BaseOperation.java:88)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$requestNewExecutors$1(ExecutorPodsAllocator.scala:420)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.requestNewExecutors(ExecutorPodsAllocator.scala:402)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$onNewSnapshots$36(ExecutorPodsAllocator.scala:366)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$onNewSnapshots$36$adapted(ExecutorPodsAllocator.scala:351)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.onNewSnapshots(ExecutorPodsAllocator.scala:351)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$start$3(ExecutorPodsAllocator.scala:129)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$start$3$adapted(ExecutorPodsAllocator.scala:129)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsSnapshotsStoreImpl$SnapshotsSubscriber.org$apache$spark$scheduler$cluster$k8s$ExecutorPodsSnapshotsStoreImpl$SnapshotsSubscriber$$processSnapshotsInternal(ExecutorPodsSnapshotsStoreImpl.scala:138)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsSnapshotsStoreImpl$SnapshotsSubscriber.processSnapshots(ExecutorPodsSnapshotsStoreImpl.scala:126)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsSnapshotsStoreImpl.$anonfun$addSubscriber$1(ExecutorPodsSnapshotsStoreImpl.scala:81)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/04 03:01:19 WARN ExecutorPodsSnapshotsStoreImpl: Exception when notifying snapshot subscriber.\n",
      "io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://172.20.0.1/api/v1/namespaces/mlx-user-16/pods. Message: pods \"cdsw-5esiffpo4nllmdwy-exec-8\" already exists. Received status: Status(apiVersion=v1, code=409, details=StatusDetails(causes=[], group=null, kind=pods, name=cdsw-5esiffpo4nllmdwy-exec-8, retryAfterSeconds=null, uid=null, additionalProperties={}), kind=Status, message=pods \"cdsw-5esiffpo4nllmdwy-exec-8\" already exists, metadata=ListMeta(_continue=null, remainingItemCount=null, resourceVersion=null, selfLink=null, additionalProperties={}), reason=AlreadyExists, status=Failure, additionalProperties={}).\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:681)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.assertResponseCode(OperationSupport.java:620)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:560)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:521)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleCreate(OperationSupport.java:309)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleCreate(BaseOperation.java:773)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.BaseOperation.create(BaseOperation.java:348)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.BaseOperation.create(BaseOperation.java:88)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$requestNewExecutors$1(ExecutorPodsAllocator.scala:420)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.requestNewExecutors(ExecutorPodsAllocator.scala:402)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$onNewSnapshots$36(ExecutorPodsAllocator.scala:366)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$onNewSnapshots$36$adapted(ExecutorPodsAllocator.scala:351)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.onNewSnapshots(ExecutorPodsAllocator.scala:351)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$start$3(ExecutorPodsAllocator.scala:129)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$start$3$adapted(ExecutorPodsAllocator.scala:129)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsSnapshotsStoreImpl$SnapshotsSubscriber.org$apache$spark$scheduler$cluster$k8s$ExecutorPodsSnapshotsStoreImpl$SnapshotsSubscriber$$processSnapshotsInternal(ExecutorPodsSnapshotsStoreImpl.scala:138)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsSnapshotsStoreImpl$SnapshotsSubscriber$$anon$2.run(ExecutorPodsSnapshotsStoreImpl.scala:158)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/04 03:01:20 WARN ExecutorPodsSnapshotsStoreImpl: Exception when notifying snapshot subscriber.\n",
      "io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://172.20.0.1/api/v1/namespaces/mlx-user-16/pods. Message: pods \"cdsw-5esiffpo4nllmdwy-exec-9\" already exists. Received status: Status(apiVersion=v1, code=409, details=StatusDetails(causes=[], group=null, kind=pods, name=cdsw-5esiffpo4nllmdwy-exec-9, retryAfterSeconds=null, uid=null, additionalProperties={}), kind=Status, message=pods \"cdsw-5esiffpo4nllmdwy-exec-9\" already exists, metadata=ListMeta(_continue=null, remainingItemCount=null, resourceVersion=null, selfLink=null, additionalProperties={}), reason=AlreadyExists, status=Failure, additionalProperties={}).\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:681)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.assertResponseCode(OperationSupport.java:620)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:560)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:521)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleCreate(OperationSupport.java:309)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleCreate(BaseOperation.java:773)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.BaseOperation.create(BaseOperation.java:348)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.BaseOperation.create(BaseOperation.java:88)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$requestNewExecutors$1(ExecutorPodsAllocator.scala:420)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.requestNewExecutors(ExecutorPodsAllocator.scala:402)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$onNewSnapshots$36(ExecutorPodsAllocator.scala:366)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$onNewSnapshots$36$adapted(ExecutorPodsAllocator.scala:351)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.onNewSnapshots(ExecutorPodsAllocator.scala:351)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$start$3(ExecutorPodsAllocator.scala:129)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$start$3$adapted(ExecutorPodsAllocator.scala:129)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsSnapshotsStoreImpl$SnapshotsSubscriber.org$apache$spark$scheduler$cluster$k8s$ExecutorPodsSnapshotsStoreImpl$SnapshotsSubscriber$$processSnapshotsInternal(ExecutorPodsSnapshotsStoreImpl.scala:138)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsSnapshotsStoreImpl$SnapshotsSubscriber.processSnapshots(ExecutorPodsSnapshotsStoreImpl.scala:126)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsSnapshotsStoreImpl$$anon$1.run(ExecutorPodsSnapshotsStoreImpl.scala:90)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS lakehouse.coffees_table_2 PURGE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b10e6c-efcc-46b5-a164-893879b79327",
   "metadata": {},
   "source": [
    "# TEST 2 - MOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b19e0de9-29c4-43dd-ae5c-c2bd69558032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE IF NOT EXISTS coffees_table_2 (coffee_id BIGINT, coffee_size STRING, coffee_sale_ts TIMESTAMP)\\\n",
    "          USING ICEBERG\\\n",
    "          PARTITIONED BY (months(coffee_sale_ts))\\\n",
    "          TBLPROPERTIES ('write.delete.mode'='merge-on-read',\\\n",
    "                          'write.update.mode'='merge-on-read',\\\n",
    "                          'write.merge.mode'='merge-on-read',\\\n",
    "                          'format-version' = '2')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eea95e7-e221-4997-9071-208cbc9f9fc5",
   "metadata": {},
   "source": [
    "#### Verify that a Metadata JSON file has been created under the Metadata directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2668955d-14e9-43c9-a359-61cd9ae5bff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = \"warehouse/tablespace/external/hive/lakehouse.db/coffees_table_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfa6cae7-8598-4088-8de8-751341c257b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table_2/metadata/00000-ae122cd1-2725-4c19-b834-7461c9121690.metadata.json\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "my_bucket = s3.Bucket(\"go01-demo\")\n",
    "\n",
    "for object_summary in my_bucket.objects.filter(Prefix=metadata_path):\n",
    "    #print(object_summary.key)\n",
    "    metadata_file = object_summary.key\n",
    "    \n",
    "print(\"Metadata File Path: {}\".format(metadata_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e03d542a-f982-4b9a-bc19-84a26d504cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>current-schema-id</th>\n",
       "      <th>current-snapshot-id</th>\n",
       "      <th>default-sort-order-id</th>\n",
       "      <th>default-spec-id</th>\n",
       "      <th>format-version</th>\n",
       "      <th>last-column-id</th>\n",
       "      <th>last-partition-id</th>\n",
       "      <th>last-sequence-number</th>\n",
       "      <th>last-updated-ms</th>\n",
       "      <th>location</th>\n",
       "      <th>metadata-log</th>\n",
       "      <th>partition-specs</th>\n",
       "      <th>properties</th>\n",
       "      <th>schemas</th>\n",
       "      <th>snapshot-log</th>\n",
       "      <th>snapshots</th>\n",
       "      <th>sort-orders</th>\n",
       "      <th>statistics</th>\n",
       "      <th>table-uuid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>1693796481590</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[([Row(field-id=1000, name='coffee_sale_ts_mon...</td>\n",
       "      <td>(pauldefusco, merge-on-read, merge-on-read, me...</td>\n",
       "      <td>[([Row(id=1, name='coffee_id', required=False,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[([], 0)]</td>\n",
       "      <td>[]</td>\n",
       "      <td>6cc4ed40-098f-4f0b-b5d8-e92fbbe8cc4b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   current-schema-id  current-snapshot-id  default-sort-order-id  \\\n",
       "0                  0                   -1                      0   \n",
       "\n",
       "   default-spec-id  format-version  last-column-id  last-partition-id  \\\n",
       "0                0               2               3               1000   \n",
       "\n",
       "   last-sequence-number  last-updated-ms  \\\n",
       "0                     0    1693796481590   \n",
       "\n",
       "                                            location metadata-log  \\\n",
       "0  s3a://go01-demo/warehouse/tablespace/external/...           []   \n",
       "\n",
       "                                     partition-specs  \\\n",
       "0  [([Row(field-id=1000, name='coffee_sale_ts_mon...   \n",
       "\n",
       "                                          properties  \\\n",
       "0  (pauldefusco, merge-on-read, merge-on-read, me...   \n",
       "\n",
       "                                             schemas snapshot-log snapshots  \\\n",
       "0  [([Row(id=1, name='coffee_id', required=False,...           []        []   \n",
       "\n",
       "  sort-orders statistics                            table-uuid  \n",
       "0   [([], 0)]         []  6cc4ed40-098f-4f0b-b5d8-e92fbbe8cc4b  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "spark.read.option(\"multiline\",\"true\").json(\"s3a://go01-demo/\" + \"warehouse/tablespace/external/hive/lakehouse.db/coffees_table_2/metadata\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357d7e23-9be4-4b92-bbb0-cb69da6f1476",
   "metadata": {},
   "source": [
    "![alt text](../img/s3_metadata.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f6b96e-d878-4e2e-91a0-1ef785833479",
   "metadata": {},
   "source": [
    "#### Notice that no snapshots or other files have been created as data has not yet been inserted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "972dced2-83a3-4c4f-a63d-2936a54534d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+---------+-------------------+\n",
      "|made_current_at|snapshot_id|parent_id|is_current_ancestor|\n",
      "+---------------+-----------+---------+-------------------+\n",
      "+---------------+-----------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM lakehouse.coffees_table_2.history\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d726f825-38b0-4342-a0ea-0f932ce20355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+---------+---------+-------------+-------+\n",
      "|committed_at|snapshot_id|parent_id|operation|manifest_list|summary|\n",
      "+------------+-----------+---------+---------+-------------+-------+\n",
      "+------------+-----------+---------+---------+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM lakehouse.coffees_table_2.snapshots;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68f5ad55-0adb-4ab3-8574-da44c3af6b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----------+-------+---------+------------+------------------+------------+------------+-----------------+----------------+------------+------------+------------+-------------+------------+-------------+\n",
      "|content|file_path|file_format|spec_id|partition|record_count|file_size_in_bytes|column_sizes|value_counts|null_value_counts|nan_value_counts|lower_bounds|upper_bounds|key_metadata|split_offsets|equality_ids|sort_order_id|\n",
      "+-------+---------+-----------+-------+---------+------------+------------------+------------+------------+-----------------+----------------+------------+------------+------------+-------------+------------+-------------+\n",
      "+-------+---------+-----------+-------+---------+------------+------------------+------------+------------+-----------------+----------------+------------+------------+------------+-------------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM lakehouse.coffees_table_2.files;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "636820a4-528b-42f2-9153-2180206420de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+-----------------+-----------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+-------------------+\n",
      "|content|path|length|partition_spec_id|added_snapshot_id|added_data_files_count|existing_data_files_count|deleted_data_files_count|added_delete_files_count|existing_delete_files_count|deleted_delete_files_count|partition_summaries|\n",
      "+-------+----+------+-----------------+-----------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+-------------------+\n",
      "+-------+----+------+-----------------+-----------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM lakehouse.coffees_table_2.manifests;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8544499-8ba1-4156-b641-89c10c281926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----------+-------+---------+------------+------------------+------------+------------+-----------------+----------------+------------+------------+------------+-------------+------------+-------------+\n",
      "|content|file_path|file_format|spec_id|partition|record_count|file_size_in_bytes|column_sizes|value_counts|null_value_counts|nan_value_counts|lower_bounds|upper_bounds|key_metadata|split_offsets|equality_ids|sort_order_id|\n",
      "+-------+---------+-----------+-------+---------+------------+------------------+------------+------------+-----------------+----------------+------------+------------+------------+-------------+------------+-------------+\n",
      "+-------+---------+-----------+-------+---------+------------+------------------+------------+------------+-----------------+----------------+------------+------------+------------+-------------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM lakehouse.coffees_table_2.all_data_files;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "323639fc-1ed3-418d-ae0e-8c1674a30ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+-----------------+-----------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+-------------------+---------------------+\n",
      "|content|path|length|partition_spec_id|added_snapshot_id|added_data_files_count|existing_data_files_count|deleted_data_files_count|added_delete_files_count|existing_delete_files_count|deleted_delete_files_count|partition_summaries|reference_snapshot_id|\n",
      "+-------+----+------+-----------------+-----------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+-------------------+---------------------+\n",
      "+-------+----+------+-----------------+-----------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+-------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM lakehouse.coffees_table_2.all_manifests;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24046132-45da-4f28-a60b-f0227b8fb7cc",
   "metadata": {},
   "source": [
    "### Table Insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "028f245c-e167-4418-9e96-fd098e0cb6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f2d7725-fe94-4a8c-82d0-319c1cefef8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Coffee_id = 1, Coffee_size = venti, coffee_sale_ts = 2023-09-01\n",
    "\n",
    "spark.sql(\"INSERT INTO lakehouse.coffees_table_2 VALUES (1, 'venti', cast(date_format('2023-09-01 10:00:00', 'yyyy-MM-dd HH:mm:ss') as timestamp))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f0d668-9e68-4b1c-a881-5d1265f999c7",
   "metadata": {},
   "source": [
    "#### Data has been added to the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da0242b8-93b9-4471-9aae-7a8f162a2320",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = \"select h.made_current_at,\\\n",
    "            s.operation,\\\n",
    "            h.snapshot_id,\\\n",
    "            h.is_current_ancestor,\\\n",
    "            s.summary['spark.app.id']\\\n",
    "        from lakehouse.coffees_table.history h\\\n",
    "        join lakehouse.coffees_table.snapshots s\\\n",
    "            on h.snapshot_id = s.snapshot_id\\\n",
    "            order by made_current_at;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76eaa58b-c77b-437a-979d-feaa4ac9b536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 03:01:31 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>made_current_at</th>\n",
       "      <th>operation</th>\n",
       "      <th>snapshot_id</th>\n",
       "      <th>is_current_ancestor</th>\n",
       "      <th>summary[spark.app.id]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-09-04 02:47:21.766</td>\n",
       "      <td>append</td>\n",
       "      <td>6464031631726591033</td>\n",
       "      <td>True</td>\n",
       "      <td>spark-application-1693795583203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-09-04 02:47:43.813</td>\n",
       "      <td>overwrite</td>\n",
       "      <td>5876718107486774833</td>\n",
       "      <td>True</td>\n",
       "      <td>spark-application-1693795583203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          made_current_at  operation          snapshot_id  \\\n",
       "0 2023-09-04 02:47:21.766     append  6464031631726591033   \n",
       "1 2023-09-04 02:47:43.813  overwrite  5876718107486774833   \n",
       "\n",
       "   is_current_ancestor            summary[spark.app.id]  \n",
       "0                 True  spark-application-1693795583203  \n",
       "1                 True  spark-application-1693795583203  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(QUERY).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3728c7dc-ac75-4859-8790-501210b43381",
   "metadata": {},
   "source": [
    "#### Notice there are now two json files and two avro files. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44f25b0-3aea-4b67-a5e5-bed3bf2b6a9d",
   "metadata": {},
   "source": [
    "The first json file is the metadata file created when the table was created. This is the metata file prefixed by 00000. The second json file is the new metadata file reflecting the insert of one row. This is the metadata file prefixed by 00001."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f46899f-2ecd-4651-9dcf-7e6c4d912273",
   "metadata": {},
   "source": [
    "The avro file with the \"snap\" prefix is the manifest list. The other avro file created is the corresponding manifest file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8968ed18-4759-4cf0-9e18-98f537517a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Metadata Files: \n",
      "\n",
      "warehouse/tablespace/external/hive/lakehouse.db/coffees_table_2/metadata/00000-ae122cd1-2725-4c19-b834-7461c9121690.metadata.json\n",
      "\n",
      "warehouse/tablespace/external/hive/lakehouse.db/coffees_table_2/metadata/00001-f20716ed-0e61-42e5-8db0-51f1d984a719.metadata.json\n",
      "\n",
      "warehouse/tablespace/external/hive/lakehouse.db/coffees_table_2/metadata/52754ebc-cb8c-41ba-8fd0-0330469eb626-m0.avro\n",
      "\n",
      "warehouse/tablespace/external/hive/lakehouse.db/coffees_table_2/metadata/snap-476467335916624763-1-52754ebc-cb8c-41ba-8fd0-0330469eb626.avro\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket(\"go01-demo\")\n",
    "\n",
    "metadata_file_list = []\n",
    "\n",
    "print(\"Current Metadata Files: \\n\")\n",
    "for object_summary in my_bucket.objects.filter(Prefix=metadata_path+\"/metadata\"):\n",
    "    print(object_summary.key +\"\\n\")\n",
    "    metadata_file_list.append(object_summary.key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f44c26-c842-4843-a126-2f96df9b4496",
   "metadata": {},
   "source": [
    "Showing Metadata Files (JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7cb383ed-5604-4d35-a525-bd7fb3501c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing warehouse/tablespace/external/hive/lakehouse.db/coffees_table_2/metadata/00000-ae122cd1-2725-4c19-b834-7461c9121690.metadata.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>current-schema-id</th>\n",
       "      <th>current-snapshot-id</th>\n",
       "      <th>default-sort-order-id</th>\n",
       "      <th>default-spec-id</th>\n",
       "      <th>format-version</th>\n",
       "      <th>last-column-id</th>\n",
       "      <th>last-partition-id</th>\n",
       "      <th>last-sequence-number</th>\n",
       "      <th>last-updated-ms</th>\n",
       "      <th>location</th>\n",
       "      <th>metadata-log</th>\n",
       "      <th>partition-specs</th>\n",
       "      <th>properties</th>\n",
       "      <th>schemas</th>\n",
       "      <th>snapshot-log</th>\n",
       "      <th>snapshots</th>\n",
       "      <th>sort-orders</th>\n",
       "      <th>statistics</th>\n",
       "      <th>table-uuid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>1693796481590</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[([Row(field-id=1000, name='coffee_sale_ts_mon...</td>\n",
       "      <td>(pauldefusco, merge-on-read, merge-on-read, me...</td>\n",
       "      <td>[([Row(id=1, name='coffee_id', required=False,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[([], 0)]</td>\n",
       "      <td>[]</td>\n",
       "      <td>6cc4ed40-098f-4f0b-b5d8-e92fbbe8cc4b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   current-schema-id  current-snapshot-id  default-sort-order-id  \\\n",
       "0                  0                   -1                      0   \n",
       "\n",
       "   default-spec-id  format-version  last-column-id  last-partition-id  \\\n",
       "0                0               2               3               1000   \n",
       "\n",
       "   last-sequence-number  last-updated-ms  \\\n",
       "0                     0    1693796481590   \n",
       "\n",
       "                                            location metadata-log  \\\n",
       "0  s3a://go01-demo/warehouse/tablespace/external/...           []   \n",
       "\n",
       "                                     partition-specs  \\\n",
       "0  [([Row(field-id=1000, name='coffee_sale_ts_mon...   \n",
       "\n",
       "                                          properties  \\\n",
       "0  (pauldefusco, merge-on-read, merge-on-read, me...   \n",
       "\n",
       "                                             schemas snapshot-log snapshots  \\\n",
       "0  [([Row(id=1, name='coffee_id', required=False,...           []        []   \n",
       "\n",
       "  sort-orders statistics                            table-uuid  \n",
       "0   [([], 0)]         []  6cc4ed40-098f-4f0b-b5d8-e92fbbe8cc4b  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"Showing \" + metadata_file_list[0])\n",
    "spark.read.option(\"multiline\",\"true\").json(\"s3a://go01-demo/\" + metadata_file_list[0]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77983734-5f46-41bc-892e-9295ca6bb6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing warehouse/tablespace/external/hive/lakehouse.db/coffees_table_2/metadata/00001-f20716ed-0e61-42e5-8db0-51f1d984a719.metadata.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>current-schema-id</th>\n",
       "      <th>current-snapshot-id</th>\n",
       "      <th>default-sort-order-id</th>\n",
       "      <th>default-spec-id</th>\n",
       "      <th>format-version</th>\n",
       "      <th>last-column-id</th>\n",
       "      <th>last-partition-id</th>\n",
       "      <th>last-sequence-number</th>\n",
       "      <th>last-updated-ms</th>\n",
       "      <th>location</th>\n",
       "      <th>metadata-log</th>\n",
       "      <th>partition-specs</th>\n",
       "      <th>properties</th>\n",
       "      <th>refs</th>\n",
       "      <th>schemas</th>\n",
       "      <th>snapshot-log</th>\n",
       "      <th>snapshots</th>\n",
       "      <th>sort-orders</th>\n",
       "      <th>statistics</th>\n",
       "      <th>table-uuid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>476467335916624763</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>1693796490216</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>[(s3a://go01-demo/warehouse/tablespace/externa...</td>\n",
       "      <td>[([Row(field-id=1000, name='coffee_sale_ts_mon...</td>\n",
       "      <td>(pauldefusco, merge-on-read, merge-on-read, me...</td>\n",
       "      <td>((476467335916624763, branch),)</td>\n",
       "      <td>[([Row(id=1, name='coffee_id', required=False,...</td>\n",
       "      <td>[(476467335916624763, 1693796490216)]</td>\n",
       "      <td>[(s3a://go01-demo/warehouse/tablespace/externa...</td>\n",
       "      <td>[([], 0)]</td>\n",
       "      <td>[]</td>\n",
       "      <td>6cc4ed40-098f-4f0b-b5d8-e92fbbe8cc4b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   current-schema-id  current-snapshot-id  default-sort-order-id  \\\n",
       "0                  0   476467335916624763                      0   \n",
       "\n",
       "   default-spec-id  format-version  last-column-id  last-partition-id  \\\n",
       "0                0               2               3               1000   \n",
       "\n",
       "   last-sequence-number  last-updated-ms  \\\n",
       "0                     1    1693796490216   \n",
       "\n",
       "                                            location  \\\n",
       "0  s3a://go01-demo/warehouse/tablespace/external/...   \n",
       "\n",
       "                                        metadata-log  \\\n",
       "0  [(s3a://go01-demo/warehouse/tablespace/externa...   \n",
       "\n",
       "                                     partition-specs  \\\n",
       "0  [([Row(field-id=1000, name='coffee_sale_ts_mon...   \n",
       "\n",
       "                                          properties  \\\n",
       "0  (pauldefusco, merge-on-read, merge-on-read, me...   \n",
       "\n",
       "                              refs  \\\n",
       "0  ((476467335916624763, branch),)   \n",
       "\n",
       "                                             schemas  \\\n",
       "0  [([Row(id=1, name='coffee_id', required=False,...   \n",
       "\n",
       "                            snapshot-log  \\\n",
       "0  [(476467335916624763, 1693796490216)]   \n",
       "\n",
       "                                           snapshots sort-orders statistics  \\\n",
       "0  [(s3a://go01-demo/warehouse/tablespace/externa...   [([], 0)]         []   \n",
       "\n",
       "                             table-uuid  \n",
       "0  6cc4ed40-098f-4f0b-b5d8-e92fbbe8cc4b  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Showing \" + metadata_file_list[1])\n",
    "spark.read.option(\"multiline\",\"true\").json(\"s3a://go01-demo/\" + metadata_file_list[1]).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833c5078-51ca-4d2b-a877-344197cee888",
   "metadata": {},
   "source": [
    "Showing Manifest List (AVRO - prefixed by \"SNAP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1eb30215-5e39-429d-b0a7-6284c2b25a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing warehouse/tablespace/external/hive/lakehouse.db/coffees_table_2/metadata/snap-476467335916624763-1-52754ebc-cb8c-41ba-8fd0-0330469eb626.avro\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>manifest_path</th>\n",
       "      <th>manifest_length</th>\n",
       "      <th>partition_spec_id</th>\n",
       "      <th>content</th>\n",
       "      <th>sequence_number</th>\n",
       "      <th>min_sequence_number</th>\n",
       "      <th>added_snapshot_id</th>\n",
       "      <th>added_data_files_count</th>\n",
       "      <th>existing_data_files_count</th>\n",
       "      <th>deleted_data_files_count</th>\n",
       "      <th>added_rows_count</th>\n",
       "      <th>existing_rows_count</th>\n",
       "      <th>deleted_rows_count</th>\n",
       "      <th>partitions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>7069</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>476467335916624763</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(False, False, [132, 2, 0, 0], [132, 2, 0, 0])]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       manifest_path  manifest_length  \\\n",
       "0  s3a://go01-demo/warehouse/tablespace/external/...             7069   \n",
       "\n",
       "   partition_spec_id  content  sequence_number  min_sequence_number  \\\n",
       "0                  0        0                1                    1   \n",
       "\n",
       "    added_snapshot_id  added_data_files_count  existing_data_files_count  \\\n",
       "0  476467335916624763                       1                          0   \n",
       "\n",
       "   deleted_data_files_count  added_rows_count  existing_rows_count  \\\n",
       "0                         0                 1                    0   \n",
       "\n",
       "   deleted_rows_count                                        partitions  \n",
       "0                   0  [(False, False, [132, 2, 0, 0], [132, 2, 0, 0])]  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Showing \" + metadata_file_list[3])\n",
    "spark.read.format(\"avro\").load(\"s3a://go01-demo/\" + metadata_file_list[3]).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f050bd1-b1a6-4b1e-9e19-76d8abc1cdd3",
   "metadata": {},
   "source": [
    "Showing Manifest Files (Avro) i.e. which shows data file locations according to each snapshot_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f09c777a-d717-4c8c-a098-eca4972e0f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing warehouse/tablespace/external/hive/lakehouse.db/coffees_table_2/metadata/52754ebc-cb8c-41ba-8fd0-0330469eb626-m0.avro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>snapshot_id</th>\n",
       "      <th>sequence_number</th>\n",
       "      <th>file_sequence_number</th>\n",
       "      <th>data_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>476467335916624763</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(0, s3a://go01-demo/warehouse/tablespace/exter...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   status         snapshot_id  sequence_number  file_sequence_number  \\\n",
       "0       1  476467335916624763              NaN                   NaN   \n",
       "\n",
       "                                           data_file  \n",
       "0  (0, s3a://go01-demo/warehouse/tablespace/exter...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Showing \" + metadata_file_list[2])\n",
    "spark.read.format(\"avro\").load(\"s3a://go01-demo/\" + metadata_file_list[2]).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f023c1ca-48c8-4d4d-b329-4825d473bf4b",
   "metadata": {},
   "source": [
    "### Table Merge Into"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bef6e0-6ea1-4f75-aeac-fd84b0b7f2f7",
   "metadata": {},
   "source": [
    "Create a staging table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8287cfa-9f8e-40fb-a880-a2dabcdbd2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 03:01:46 WARN ExecutorPodsSnapshotsStoreImpl: Exception when notifying snapshot subscriber.\n",
      "io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://172.20.0.1/api/v1/namespaces/mlx-user-16/pods. Message: pods \"cdsw-5esiffpo4nllmdwy-exec-11\" already exists. Received status: Status(apiVersion=v1, code=409, details=StatusDetails(causes=[], group=null, kind=pods, name=cdsw-5esiffpo4nllmdwy-exec-11, retryAfterSeconds=null, uid=null, additionalProperties={}), kind=Status, message=pods \"cdsw-5esiffpo4nllmdwy-exec-11\" already exists, metadata=ListMeta(_continue=null, remainingItemCount=null, resourceVersion=null, selfLink=null, additionalProperties={}), reason=AlreadyExists, status=Failure, additionalProperties={}).\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:681)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.assertResponseCode(OperationSupport.java:620)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:560)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:521)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleCreate(OperationSupport.java:309)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleCreate(BaseOperation.java:773)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.BaseOperation.create(BaseOperation.java:348)\n",
      "\tat io.fabric8.kubernetes.client.dsl.base.BaseOperation.create(BaseOperation.java:88)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$requestNewExecutors$1(ExecutorPodsAllocator.scala:420)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.requestNewExecutors(ExecutorPodsAllocator.scala:402)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$onNewSnapshots$36(ExecutorPodsAllocator.scala:366)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$onNewSnapshots$36$adapted(ExecutorPodsAllocator.scala:351)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.onNewSnapshots(ExecutorPodsAllocator.scala:351)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$start$3(ExecutorPodsAllocator.scala:129)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$start$3$adapted(ExecutorPodsAllocator.scala:129)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsSnapshotsStoreImpl$SnapshotsSubscriber.org$apache$spark$scheduler$cluster$k8s$ExecutorPodsSnapshotsStoreImpl$SnapshotsSubscriber$$processSnapshotsInternal(ExecutorPodsSnapshotsStoreImpl.scala:138)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsSnapshotsStoreImpl$SnapshotsSubscriber.processSnapshots(ExecutorPodsSnapshotsStoreImpl.scala:126)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsSnapshotsStoreImpl$$anon$1.run(ExecutorPodsSnapshotsStoreImpl.scala:90)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS lakehouse.coffee_staging_2 PURGE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a742c99-a211-4fa6-bdd5-e63f8079f9d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE IF NOT EXISTS lakehouse.coffee_staging_2\\\n",
    "            (coffee_id BIGINT, coffee_size STRING, coffee_sale_ts TIMESTAMP)\\\n",
    "            USING iceberg\\\n",
    "            PARTITIONED BY (months(coffee_sale_ts))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d4dc4207-34e0-4bb3-a147-dfcd4e601990",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"INSERT INTO lakehouse.coffee_staging_2\\\n",
    "            VALUES (1, 'venti', cast(date_format('2023-07-01 11:00:00', 'yyyy-MM-dd HH:mm:ss') as timestamp)),\\\n",
    "            (2, 'grande', cast(date_format('2023-07-01 11:10:00', 'yyyy-MM-dd HH:mm:ss') as timestamp)),\\\n",
    "            (3, 'tall', cast(date_format('2023-07-01 12:01:00', 'yyyy-MM-dd HH:mm:ss') as timestamp))\")\n",
    "\n",
    "#Row: Coffee_id = 1, coffee_size = venti, coffee_sale_ts = 2023-07-01\n",
    "#Row: Coffee_id = 2, coffee_size = grande, coffee_sale_ts = 2023-07-01\n",
    "#Row: Coffee_id = 3, coffee_size = tall, coffee_sale_ts = 2023-04-01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c3720c-3011-4a15-924f-de39a525501d",
   "metadata": {},
   "source": [
    "Merge Into Customers Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b62e634-935e-4219-b240-fdb8411b2984",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"MERGE INTO lakehouse.coffees_table_2 c\\\n",
    "            USING (SELECT * FROM lakehouse.coffee_staging_2) s\\\n",
    "            ON c.coffee_id = s.coffee_id \\\n",
    "            WHEN MATCHED THEN UPDATE SET c.coffee_size = s.coffee_size \\\n",
    "            WHEN NOT MATCHED THEN INSERT *\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c94305e6-c808-4d67-8903-864a4d80b5dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>committed_at</th>\n",
       "      <th>snapshot_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>operation</th>\n",
       "      <th>manifest_list</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-09-04 03:01:30.216</td>\n",
       "      <td>476467335916624763</td>\n",
       "      <td>NaN</td>\n",
       "      <td>append</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>{'spark.app.id': 'spark-application-1693796444...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-09-04 03:02:14.727</td>\n",
       "      <td>1376882192344568144</td>\n",
       "      <td>4.764673e+17</td>\n",
       "      <td>overwrite</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>{'added-data-files': '2', 'added-position-dele...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             committed_at          snapshot_id     parent_id  operation  \\\n",
       "0 2023-09-04 03:01:30.216   476467335916624763           NaN     append   \n",
       "1 2023-09-04 03:02:14.727  1376882192344568144  4.764673e+17  overwrite   \n",
       "\n",
       "                                       manifest_list  \\\n",
       "0  s3a://go01-demo/warehouse/tablespace/external/...   \n",
       "1  s3a://go01-demo/warehouse/tablespace/external/...   \n",
       "\n",
       "                                             summary  \n",
       "0  {'spark.app.id': 'spark-application-1693796444...  \n",
       "1  {'added-data-files': '2', 'added-position-dele...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM lakehouse.coffees_table_2.snapshots;\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d2d2db3-6d38-428e-bc8d-51286ff2e11c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>path</th>\n",
       "      <th>length</th>\n",
       "      <th>partition_spec_id</th>\n",
       "      <th>added_snapshot_id</th>\n",
       "      <th>added_data_files_count</th>\n",
       "      <th>existing_data_files_count</th>\n",
       "      <th>deleted_data_files_count</th>\n",
       "      <th>added_delete_files_count</th>\n",
       "      <th>existing_delete_files_count</th>\n",
       "      <th>deleted_delete_files_count</th>\n",
       "      <th>partition_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>7174</td>\n",
       "      <td>0</td>\n",
       "      <td>1376882192344568144</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(False, False, 2023-07, 2023-09)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>7069</td>\n",
       "      <td>0</td>\n",
       "      <td>476467335916624763</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(False, False, 2023-09, 2023-09)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>7098</td>\n",
       "      <td>0</td>\n",
       "      <td>1376882192344568144</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(False, False, 2023-09, 2023-09)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   content                                               path  length  \\\n",
       "0        0  s3a://go01-demo/warehouse/tablespace/external/...    7174   \n",
       "1        0  s3a://go01-demo/warehouse/tablespace/external/...    7069   \n",
       "2        1  s3a://go01-demo/warehouse/tablespace/external/...    7098   \n",
       "\n",
       "   partition_spec_id    added_snapshot_id  added_data_files_count  \\\n",
       "0                  0  1376882192344568144                       2   \n",
       "1                  0   476467335916624763                       1   \n",
       "2                  0  1376882192344568144                       0   \n",
       "\n",
       "   existing_data_files_count  deleted_data_files_count  \\\n",
       "0                          0                         0   \n",
       "1                          0                         0   \n",
       "2                          0                         0   \n",
       "\n",
       "   added_delete_files_count  existing_delete_files_count  \\\n",
       "0                         0                            0   \n",
       "1                         0                            0   \n",
       "2                         1                            0   \n",
       "\n",
       "   deleted_delete_files_count                 partition_summaries  \n",
       "0                           0  [(False, False, 2023-07, 2023-09)]  \n",
       "1                           0  [(False, False, 2023-09, 2023-09)]  \n",
       "2                           0  [(False, False, 2023-09, 2023-09)]  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM lakehouse.coffees_table_2.manifests;\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db59e20d-b4fa-455c-8a34-b5538d06b2ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>file_path</th>\n",
       "      <th>file_format</th>\n",
       "      <th>spec_id</th>\n",
       "      <th>partition</th>\n",
       "      <th>record_count</th>\n",
       "      <th>file_size_in_bytes</th>\n",
       "      <th>column_sizes</th>\n",
       "      <th>value_counts</th>\n",
       "      <th>null_value_counts</th>\n",
       "      <th>nan_value_counts</th>\n",
       "      <th>lower_bounds</th>\n",
       "      <th>upper_bounds</th>\n",
       "      <th>key_metadata</th>\n",
       "      <th>split_offsets</th>\n",
       "      <th>equality_ids</th>\n",
       "      <th>sort_order_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>PARQUET</td>\n",
       "      <td>0</td>\n",
       "      <td>(644,)</td>\n",
       "      <td>1</td>\n",
       "      <td>992</td>\n",
       "      <td>{1: 39, 2: 40, 3: 39}</td>\n",
       "      <td>{1: 1, 2: 1, 3: 1}</td>\n",
       "      <td>{1: 0, 2: 0, 3: 0}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{1: [1, 0, 0, 0, 0, 0, 0, 0], 2: [118, 101, 11...</td>\n",
       "      <td>{1: [1, 0, 0, 0, 0, 0, 0, 0], 2: [118, 101, 11...</td>\n",
       "      <td>None</td>\n",
       "      <td>[4]</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>PARQUET</td>\n",
       "      <td>0</td>\n",
       "      <td>(642,)</td>\n",
       "      <td>2</td>\n",
       "      <td>1002</td>\n",
       "      <td>{1: 46, 2: 49, 3: 47}</td>\n",
       "      <td>{1: 2, 2: 2, 3: 2}</td>\n",
       "      <td>{1: 0, 2: 0, 3: 0}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{1: [2, 0, 0, 0, 0, 0, 0, 0], 2: [103, 114, 97...</td>\n",
       "      <td>{1: [3, 0, 0, 0, 0, 0, 0, 0], 2: [116, 97, 108...</td>\n",
       "      <td>None</td>\n",
       "      <td>[4]</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>PARQUET</td>\n",
       "      <td>0</td>\n",
       "      <td>(644,)</td>\n",
       "      <td>1</td>\n",
       "      <td>976</td>\n",
       "      <td>{1: 33, 2: 34, 3: 39}</td>\n",
       "      <td>{1: 1, 2: 1, 3: 1}</td>\n",
       "      <td>{1: 0, 2: 0, 3: 0}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{1: [1, 0, 0, 0, 0, 0, 0, 0], 2: [118, 101, 11...</td>\n",
       "      <td>{1: [1, 0, 0, 0, 0, 0, 0, 0], 2: [118, 101, 11...</td>\n",
       "      <td>None</td>\n",
       "      <td>[4]</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   content                                          file_path file_format  \\\n",
       "0        0  s3a://go01-demo/warehouse/tablespace/external/...     PARQUET   \n",
       "1        0  s3a://go01-demo/warehouse/tablespace/external/...     PARQUET   \n",
       "2        0  s3a://go01-demo/warehouse/tablespace/external/...     PARQUET   \n",
       "\n",
       "   spec_id partition  record_count  file_size_in_bytes           column_sizes  \\\n",
       "0        0    (644,)             1                 992  {1: 39, 2: 40, 3: 39}   \n",
       "1        0    (642,)             2                1002  {1: 46, 2: 49, 3: 47}   \n",
       "2        0    (644,)             1                 976  {1: 33, 2: 34, 3: 39}   \n",
       "\n",
       "         value_counts   null_value_counts nan_value_counts  \\\n",
       "0  {1: 1, 2: 1, 3: 1}  {1: 0, 2: 0, 3: 0}               {}   \n",
       "1  {1: 2, 2: 2, 3: 2}  {1: 0, 2: 0, 3: 0}               {}   \n",
       "2  {1: 1, 2: 1, 3: 1}  {1: 0, 2: 0, 3: 0}               {}   \n",
       "\n",
       "                                        lower_bounds  \\\n",
       "0  {1: [1, 0, 0, 0, 0, 0, 0, 0], 2: [118, 101, 11...   \n",
       "1  {1: [2, 0, 0, 0, 0, 0, 0, 0], 2: [103, 114, 97...   \n",
       "2  {1: [1, 0, 0, 0, 0, 0, 0, 0], 2: [118, 101, 11...   \n",
       "\n",
       "                                        upper_bounds key_metadata  \\\n",
       "0  {1: [1, 0, 0, 0, 0, 0, 0, 0], 2: [118, 101, 11...         None   \n",
       "1  {1: [3, 0, 0, 0, 0, 0, 0, 0], 2: [116, 97, 108...         None   \n",
       "2  {1: [1, 0, 0, 0, 0, 0, 0, 0], 2: [118, 101, 11...         None   \n",
       "\n",
       "  split_offsets equality_ids  sort_order_id  \n",
       "0           [4]         None              0  \n",
       "1           [4]         None              0  \n",
       "2           [4]         None              0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM lakehouse.coffees_table_2.all_data_files;\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9486357-7744-4068-998c-7bb4a9edf5da",
   "metadata": {},
   "source": [
    "#### There is a new metadata file (json) prefixed by 0002."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9308c678-d334-4a0b-8b14-94c0164c9be5",
   "metadata": {},
   "source": [
    "#### There is a new manifest list file (avro) prefixed by \"snap\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1685d9f-422a-44dc-a969-e4e3d5b0718c",
   "metadata": {},
   "source": [
    "#### There is a new manifest file (avro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "91fd6085-7047-437b-9b5d-08a5a43dc13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Metadata Files: \n",
      "\n",
      "warehouse/tablespace/external/hive/lakehouse.db/coffees_table_2/metadata/00000-ae122cd1-2725-4c19-b834-7461c9121690.metadata.json\n",
      "warehouse/tablespace/external/hive/lakehouse.db/coffees_table_2/metadata/00001-f20716ed-0e61-42e5-8db0-51f1d984a719.metadata.json\n",
      "warehouse/tablespace/external/hive/lakehouse.db/coffees_table_2/metadata/00002-aa09db85-7ce4-49cd-9cfe-f0f0547bedbf.metadata.json\n",
      "warehouse/tablespace/external/hive/lakehouse.db/coffees_table_2/metadata/1f6b71e2-d5a8-45ad-934b-f98506c29875-m0.avro\n",
      "warehouse/tablespace/external/hive/lakehouse.db/coffees_table_2/metadata/1f6b71e2-d5a8-45ad-934b-f98506c29875-m1.avro\n",
      "warehouse/tablespace/external/hive/lakehouse.db/coffees_table_2/metadata/52754ebc-cb8c-41ba-8fd0-0330469eb626-m0.avro\n",
      "warehouse/tablespace/external/hive/lakehouse.db/coffees_table_2/metadata/snap-1376882192344568144-1-1f6b71e2-d5a8-45ad-934b-f98506c29875.avro\n",
      "warehouse/tablespace/external/hive/lakehouse.db/coffees_table_2/metadata/snap-476467335916624763-1-52754ebc-cb8c-41ba-8fd0-0330469eb626.avro\n",
      "There is a total of 8 files in the Metadata layer\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket(\"go01-demo\")\n",
    "\n",
    "metadata_file_list = []\n",
    "\n",
    "print(\"Current Metadata Files: \\n\")\n",
    "\n",
    "for object_summary in my_bucket.objects.filter(Prefix=metadata_path+\"/metadata\"):\n",
    "    #print(object_summary.key +\"\\n\")\n",
    "    metadata_file_list.append(object_summary.key)\n",
    "    \n",
    "print(*metadata_file_list, sep = \"\\n\")\n",
    "\n",
    "print(\"There is a total of \" + str(len(metadata_file_list)) + \" files in the Metadata layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ae4a83-5c55-4ebc-a344-4363e9585ff3",
   "metadata": {},
   "source": [
    "Showing Latest (Current) Metadata File (JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df26dc95-0c4b-419b-b561-3ca5242de499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing warehouse/tablespace/external/hive/lakehouse.db/coffees_table_2/metadata/00002-aa09db85-7ce4-49cd-9cfe-f0f0547bedbf.metadata.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>current-schema-id</th>\n",
       "      <th>current-snapshot-id</th>\n",
       "      <th>default-sort-order-id</th>\n",
       "      <th>default-spec-id</th>\n",
       "      <th>format-version</th>\n",
       "      <th>last-column-id</th>\n",
       "      <th>last-partition-id</th>\n",
       "      <th>last-sequence-number</th>\n",
       "      <th>last-updated-ms</th>\n",
       "      <th>location</th>\n",
       "      <th>metadata-log</th>\n",
       "      <th>partition-specs</th>\n",
       "      <th>properties</th>\n",
       "      <th>schemas</th>\n",
       "      <th>snapshot-log</th>\n",
       "      <th>snapshots</th>\n",
       "      <th>sort-orders</th>\n",
       "      <th>statistics</th>\n",
       "      <th>table-uuid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>1693796481590</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[([Row(field-id=1000, name='coffee_sale_ts_mon...</td>\n",
       "      <td>(pauldefusco, merge-on-read, merge-on-read, me...</td>\n",
       "      <td>[([Row(id=1, name='coffee_id', required=False,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[([], 0)]</td>\n",
       "      <td>[]</td>\n",
       "      <td>6cc4ed40-098f-4f0b-b5d8-e92fbbe8cc4b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   current-schema-id  current-snapshot-id  default-sort-order-id  \\\n",
       "0                  0                   -1                      0   \n",
       "\n",
       "   default-spec-id  format-version  last-column-id  last-partition-id  \\\n",
       "0                0               2               3               1000   \n",
       "\n",
       "   last-sequence-number  last-updated-ms  \\\n",
       "0                     0    1693796481590   \n",
       "\n",
       "                                            location metadata-log  \\\n",
       "0  s3a://go01-demo/warehouse/tablespace/external/...           []   \n",
       "\n",
       "                                     partition-specs  \\\n",
       "0  [([Row(field-id=1000, name='coffee_sale_ts_mon...   \n",
       "\n",
       "                                          properties  \\\n",
       "0  (pauldefusco, merge-on-read, merge-on-read, me...   \n",
       "\n",
       "                                             schemas snapshot-log snapshots  \\\n",
       "0  [([Row(id=1, name='coffee_id', required=False,...           []        []   \n",
       "\n",
       "  sort-orders statistics                            table-uuid  \n",
       "0   [([], 0)]         []  6cc4ed40-098f-4f0b-b5d8-e92fbbe8cc4b  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Showing \" + metadata_file_list[2])\n",
    "spark.read.option(\"multiline\",\"true\").json(\"s3a://go01-demo/\" + metadata_file_list[0]).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba3cfee-e9d4-4d30-b209-6fde71d57479",
   "metadata": {},
   "source": [
    "Showing Latest Manifest List (AVRO - prefixed by \"SNAP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2f34aa3-7763-4f58-9c33-b08f1fab32b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing warehouse/tablespace/external/hive/lakehouse.db/coffees_table_2/metadata/snap-1376882192344568144-1-1f6b71e2-d5a8-45ad-934b-f98506c29875.avro\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>manifest_path</th>\n",
       "      <th>manifest_length</th>\n",
       "      <th>partition_spec_id</th>\n",
       "      <th>content</th>\n",
       "      <th>sequence_number</th>\n",
       "      <th>min_sequence_number</th>\n",
       "      <th>added_snapshot_id</th>\n",
       "      <th>added_data_files_count</th>\n",
       "      <th>existing_data_files_count</th>\n",
       "      <th>deleted_data_files_count</th>\n",
       "      <th>added_rows_count</th>\n",
       "      <th>existing_rows_count</th>\n",
       "      <th>deleted_rows_count</th>\n",
       "      <th>partitions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>7174</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1376882192344568144</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(False, False, [130, 2, 0, 0], [132, 2, 0, 0])]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>7069</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>476467335916624763</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(False, False, [132, 2, 0, 0], [132, 2, 0, 0])]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>7098</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1376882192344568144</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(False, False, [132, 2, 0, 0], [132, 2, 0, 0])]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       manifest_path  manifest_length  \\\n",
       "0  s3a://go01-demo/warehouse/tablespace/external/...             7174   \n",
       "1  s3a://go01-demo/warehouse/tablespace/external/...             7069   \n",
       "2  s3a://go01-demo/warehouse/tablespace/external/...             7098   \n",
       "\n",
       "   partition_spec_id  content  sequence_number  min_sequence_number  \\\n",
       "0                  0        0                2                    2   \n",
       "1                  0        0                1                    1   \n",
       "2                  0        1                2                    2   \n",
       "\n",
       "     added_snapshot_id  added_data_files_count  existing_data_files_count  \\\n",
       "0  1376882192344568144                       2                          0   \n",
       "1   476467335916624763                       1                          0   \n",
       "2  1376882192344568144                       1                          0   \n",
       "\n",
       "   deleted_data_files_count  added_rows_count  existing_rows_count  \\\n",
       "0                         0                 3                    0   \n",
       "1                         0                 1                    0   \n",
       "2                         0                 1                    0   \n",
       "\n",
       "   deleted_rows_count                                        partitions  \n",
       "0                   0  [(False, False, [130, 2, 0, 0], [132, 2, 0, 0])]  \n",
       "1                   0  [(False, False, [132, 2, 0, 0], [132, 2, 0, 0])]  \n",
       "2                   0  [(False, False, [132, 2, 0, 0], [132, 2, 0, 0])]  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Showing \" + metadata_file_list[6])\n",
    "spark.read.format(\"avro\").load(\"s3a://go01-demo/\" + metadata_file_list[6]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f99f9bf9-8d6b-4a46-97bd-c869391d16ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing warehouse/tablespace/external/hive/lakehouse.db/coffees_table_2/metadata/snap-476467335916624763-1-52754ebc-cb8c-41ba-8fd0-0330469eb626.avro\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>manifest_path</th>\n",
       "      <th>manifest_length</th>\n",
       "      <th>partition_spec_id</th>\n",
       "      <th>content</th>\n",
       "      <th>sequence_number</th>\n",
       "      <th>min_sequence_number</th>\n",
       "      <th>added_snapshot_id</th>\n",
       "      <th>added_data_files_count</th>\n",
       "      <th>existing_data_files_count</th>\n",
       "      <th>deleted_data_files_count</th>\n",
       "      <th>added_rows_count</th>\n",
       "      <th>existing_rows_count</th>\n",
       "      <th>deleted_rows_count</th>\n",
       "      <th>partitions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>7069</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>476467335916624763</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(False, False, [132, 2, 0, 0], [132, 2, 0, 0])]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       manifest_path  manifest_length  \\\n",
       "0  s3a://go01-demo/warehouse/tablespace/external/...             7069   \n",
       "\n",
       "   partition_spec_id  content  sequence_number  min_sequence_number  \\\n",
       "0                  0        0                1                    1   \n",
       "\n",
       "    added_snapshot_id  added_data_files_count  existing_data_files_count  \\\n",
       "0  476467335916624763                       1                          0   \n",
       "\n",
       "   deleted_data_files_count  added_rows_count  existing_rows_count  \\\n",
       "0                         0                 1                    0   \n",
       "\n",
       "   deleted_rows_count                                        partitions  \n",
       "0                   0  [(False, False, [132, 2, 0, 0], [132, 2, 0, 0])]  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Showing \" + metadata_file_list[7])\n",
    "spark.read.format(\"avro\").load(\"s3a://go01-demo/\" + metadata_file_list[7]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b0c48b1e-9f65-47b2-a219-c81e69cf80c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing warehouse/tablespace/external/hive/lakehouse.db/coffees_table_2/metadata/snap-1376882192344568144-1-1f6b71e2-d5a8-45ad-934b-f98506c29875.avro\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table_2/metadata/1f6b71e2-d5a8-45ad-934b-f98506c29875-m0.avro'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Showing \" + metadata_file_list[6])\n",
    "spark.read.format(\"avro\").load(\"s3a://go01-demo/\" + metadata_file_list[6]).toPandas()['manifest_path'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ea6b32a-6ef8-40f3-a38d-33b78a37f2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing warehouse/tablespace/external/hive/lakehouse.db/coffees_table_2/metadata/snap-1376882192344568144-1-1f6b71e2-d5a8-45ad-934b-f98506c29875.avro\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table_2/metadata/52754ebc-cb8c-41ba-8fd0-0330469eb626-m0.avro'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Showing \" + metadata_file_list[6])\n",
    "spark.read.format(\"avro\").load(\"s3a://go01-demo/\" + metadata_file_list[6]).toPandas()['manifest_path'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ad15d0-0d60-4d02-8ace-2d2339bac782",
   "metadata": {},
   "source": [
    "Showing Manifest Files (Avro) i.e. list of table partitions mapped to snapshot ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fec68656-4531-4dd6-99d7-b85249a491a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing warehouse/tablespace/external/hive/lakehouse.db/coffees_table_2/metadata/1f6b71e2-d5a8-45ad-934b-f98506c29875-m0.avro\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>snapshot_id</th>\n",
       "      <th>sequence_number</th>\n",
       "      <th>file_sequence_number</th>\n",
       "      <th>data_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1376882192344568144</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(0, s3a://go01-demo/warehouse/tablespace/exter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1376882192344568144</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(0, s3a://go01-demo/warehouse/tablespace/exter...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   status          snapshot_id  sequence_number  file_sequence_number  \\\n",
       "0       1  1376882192344568144              NaN                   NaN   \n",
       "1       1  1376882192344568144              NaN                   NaN   \n",
       "\n",
       "                                           data_file  \n",
       "0  (0, s3a://go01-demo/warehouse/tablespace/exter...  \n",
       "1  (0, s3a://go01-demo/warehouse/tablespace/exter...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Showing \" + metadata_file_list[3])\n",
    "spark.read.format(\"avro\").load(\"s3a://go01-demo/\" + metadata_file_list[3]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4f3b9d9a-955c-4474-821f-974cd9decda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing warehouse/tablespace/external/hive/lakehouse.db/coffees_table_2/metadata/1f6b71e2-d5a8-45ad-934b-f98506c29875-m1.avro\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>snapshot_id</th>\n",
       "      <th>sequence_number</th>\n",
       "      <th>file_sequence_number</th>\n",
       "      <th>data_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1376882192344568144</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(1, s3a://go01-demo/warehouse/tablespace/exter...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   status          snapshot_id  sequence_number  file_sequence_number  \\\n",
       "0       1  1376882192344568144              NaN                   NaN   \n",
       "\n",
       "                                           data_file  \n",
       "0  (1, s3a://go01-demo/warehouse/tablespace/exter...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Showing \" + metadata_file_list[4])\n",
    "spark.read.format(\"avro\").load(\"s3a://go01-demo/\" + metadata_file_list[4]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3837a142-1dc7-46fe-871d-1b3ff2ec6dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing warehouse/tablespace/external/hive/lakehouse.db/coffees_table_2/metadata/52754ebc-cb8c-41ba-8fd0-0330469eb626-m0.avro\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>snapshot_id</th>\n",
       "      <th>sequence_number</th>\n",
       "      <th>file_sequence_number</th>\n",
       "      <th>data_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>476467335916624763</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(0, s3a://go01-demo/warehouse/tablespace/exter...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   status         snapshot_id  sequence_number  file_sequence_number  \\\n",
       "0       1  476467335916624763              NaN                   NaN   \n",
       "\n",
       "                                           data_file  \n",
       "0  (0, s3a://go01-demo/warehouse/tablespace/exter...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Showing \" + metadata_file_list[5])\n",
    "spark.read.format(\"avro\").load(\"s3a://go01-demo/\" + metadata_file_list[5]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "43f94648-66db-48d9-8f96-62447cea9054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing warehouse/tablespace/external/hive/lakehouse.db/coffees_table_2/metadata/1f6b71e2-d5a8-45ad-934b-f98506c29875-m0.avro\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(content=0, file_path='s3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table_2/data/coffee_sale_ts_month=2023-09/00008-831-1665f868-1b85-4142-b416-ba62d6e7be4c-00001.parquet', file_format='PARQUET', partition=Row(coffee_sale_ts_month=644), record_count=1, file_size_in_bytes=992, column_sizes=[Row(key=1, value=39), Row(key=2, value=40), Row(key=3, value=39)], value_counts=[Row(key=1, value=1), Row(key=2, value=1), Row(key=3, value=1)], null_value_counts=[Row(key=1, value=0), Row(key=2, value=0), Row(key=3, value=0)], nan_value_counts=[], lower_bounds=[Row(key=1, value=bytearray(b'\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00')), Row(key=2, value=bytearray(b'venti')), Row(key=3, value=bytearray(b'\\x00\\xc8A9I\\x04\\x06\\x00'))], upper_bounds=[Row(key=1, value=bytearray(b'\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00')), Row(key=2, value=bytearray(b'venti')), Row(key=3, value=bytearray(b'\\x00\\xc8A9I\\x04\\x06\\x00'))], key_metadata=None, split_offsets=[4], equality_ids=None, sort_order_id=0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Showing \" + metadata_file_list[3])\n",
    "spark.read.format(\"avro\").load(\"s3a://go01-demo/\" + metadata_file_list[3]).toPandas()['data_file'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fe86be50-94d6-4f0e-b45a-50bce41796e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 52:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------------+\n",
      "|coffee_id|coffee_size|     coffee_sale_ts|\n",
      "+---------+-----------+-------------------+\n",
      "|        1|      venti|2023-09-01 10:00:00|\n",
      "+---------+-----------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_file_path = spark.read.format(\"avro\").load(\"s3a://go01-demo/\" + metadata_file_list[3]).toPandas()['data_file'][0][1]\n",
    "spark.read.parquet(data_file_path).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1cfe5925-f8cd-41fa-8fd3-65b5456e7810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing warehouse/tablespace/external/hive/lakehouse.db/coffees_table_2/metadata/1f6b71e2-d5a8-45ad-934b-f98506c29875-m1.avro\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(content=1, file_path='s3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table_2/data/coffee_sale_ts_month=2023-09/00008-831-48079c9c-66f2-47c9-82a4-a757cabfeb0b-00001.parquet', file_format='PARQUET', partition=Row(coffee_sale_ts_month=644), record_count=1, file_size_in_bytes=1794, column_sizes=[Row(key=2147483546, value=199), Row(key=2147483545, value=33)], value_counts=[Row(key=2147483546, value=1), Row(key=2147483545, value=1)], null_value_counts=[Row(key=2147483546, value=0), Row(key=2147483545, value=0)], nan_value_counts=[], lower_bounds=[Row(key=2147483546, value=bytearray(b's3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table_2/data/coffee_sale_ts_month=2023-09/00072-212-473f28c6-00c3-478e-8685-eedf508153f6-00001.parquet')), Row(key=2147483545, value=bytearray(b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'))], upper_bounds=[Row(key=2147483546, value=bytearray(b's3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table_2/data/coffee_sale_ts_month=2023-09/00072-212-473f28c6-00c3-478e-8685-eedf508153f6-00001.parquet')), Row(key=2147483545, value=bytearray(b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'))], key_metadata=None, split_offsets=None, equality_ids=None, sort_order_id=None)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Showing \" + metadata_file_list[4])\n",
    "spark.read.format(\"avro\").load(\"s3a://go01-demo/\" + metadata_file_list[4]).toPandas()['data_file'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a2c467e6-e90d-4a39-bfb3-ee2a84cb95fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|           file_path|pos|\n",
      "+--------------------+---+\n",
      "|s3a://go01-demo/w...|  0|\n",
      "+--------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_file_path = spark.read.format(\"avro\").load(\"s3a://go01-demo/\" + metadata_file_list[4]).toPandas()['data_file'][0][1]\n",
    "spark.read.parquet(data_file_path).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8a73df6a-5135-475e-a74e-28551fa55bf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table_2/data/coffee_sale_ts_month=2023-09/00072-212-473f28c6-00c3-478e-8685-eedf508153f6-00001.parquet'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 03:36:41 WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed (this is expected if the application is shutting down.)\n",
      "io.fabric8.kubernetes.client.WatcherException: too old resource version: 13819492 (13823117)\n",
      "\tat io.fabric8.kubernetes.client.dsl.internal.AbstractWatchManager.onStatus(AbstractWatchManager.java:265)\n",
      "\tat io.fabric8.kubernetes.client.dsl.internal.AbstractWatchManager.onMessage(AbstractWatchManager.java:249)\n",
      "\tat io.fabric8.kubernetes.client.dsl.internal.WatcherWebSocketListener.onMessage(WatcherWebSocketListener.java:93)\n",
      "\tat okhttp3.internal.ws.RealWebSocket.onReadMessage(RealWebSocket.java:323)\n",
      "\tat okhttp3.internal.ws.WebSocketReader.readMessageFrame(WebSocketReader.java:219)\n",
      "\tat okhttp3.internal.ws.WebSocketReader.processNextFrame(WebSocketReader.java:105)\n",
      "\tat okhttp3.internal.ws.RealWebSocket.loopReader(RealWebSocket.java:274)\n",
      "\tat okhttp3.internal.ws.RealWebSocket$2.onResponse(RealWebSocket.java:214)\n",
      "\tat okhttp3.RealCall$AsyncCall.execute(RealCall.java:203)\n",
      "\tat okhttp3.internal.NamedRunnable.run(NamedRunnable.java:32)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: io.fabric8.kubernetes.client.KubernetesClientException: too old resource version: 13819492 (13823117)\n",
      "\t... 13 more\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(data_file_path).toPandas()['file_path'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e2577bac-5439-4426-915f-f63081522c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_data_file_path = 's3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table_2/data/coffee_sale_ts_month=2023-09/00072-212-473f28c6-00c3-478e-8685-eedf508153f6-00001.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0058140c-a9d5-4239-8270-fa13fce4dda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------------+\n",
      "|coffee_id|coffee_size|     coffee_sale_ts|\n",
      "+---------+-----------+-------------------+\n",
      "|        1|      venti|2023-09-01 10:00:00|\n",
      "+---------+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(sub_data_file_path).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62108a25-7170-4760-88a9-bbe2a80581b3",
   "metadata": {},
   "source": [
    "### Time Travel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c566c5c6-8b5d-4aba-9c20-409233c4827b",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshots_df = spark.sql(\"SELECT * FROM lakehouse.customer_table.snapshots;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0852473c-3639-43c1-a446-8920be731f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_snapshot = snapshots_df.select(\"snapshot_id\").head(1)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb18fe8c-fb4f-4669-a52e-fbe024979e4b",
   "metadata": {},
   "source": [
    "#### Validate that the output dataframe only includes one row per the original insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7c0cd767-66f2-4393-8a2f-0d1d89e6a973",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>dob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>CA</td>\n",
       "      <td>USA</td>\n",
       "      <td>2000-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id state country        dob\n",
       "0   1    CA     USA 2000-01-01"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read\\\n",
    "    .option(\"snapshot-id\", first_snapshot)\\\n",
    "    .format(\"iceberg\")\\\n",
    "    .load(\"lakehouse.customer_table\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bbbc2468-98ab-4e80-a7cd-717cb515b045",
   "metadata": {},
   "outputs": [],
   "source": [
    "avro_tempdf = spark.read.format(\"avro\").load(\"s3a://go01-demo/\" + metadata_file_list[6]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1e45b8d9-0368-4f66-80e6-2e5aa2766144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['manifest_path', 'manifest_length', 'partition_spec_id',\n",
       "       'added_snapshot_id', 'added_data_files_count',\n",
       "       'existing_data_files_count', 'deleted_data_files_count', 'partitions',\n",
       "       'added_rows_count', 'existing_rows_count', 'deleted_rows_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avro_tempdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6dda7fb9-9e96-4a4d-89fc-e6f0c1197213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [(False, False, [56, 3, 4, 0], [56, 3, 4, 0])]\n",
       "Name: partitions, dtype: object"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avro_tempdf['partitions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7990d583-eedb-4959-a90c-1151920ab963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "Name: added_rows_count, dtype: int64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avro_tempdf['added_rows_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d808a437-8439-4b0a-afeb-33d5817f0636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "Name: existing_rows_count, dtype: int64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avro_tempdf['existing_rows_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "74f5a019-bbf7-431a-96e3-984e79a249a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "Name: added_data_files_count, dtype: int32"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avro_tempdf['added_data_files_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1d2ef22c-733c-42b7-a022-2955407ec2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing warehouse/tablespace/external/hive/lakehouse.db/customer_table/metadata/00002-30665850-e97b-46dd-a4f1-2dee67bbe972.metadata.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Showing \" + metadata_file_list[2])\n",
    "json_tempdf = spark.read.option(\"multiline\",\"true\").json(\"s3a://go01-demo/\" + metadata_file_list[2]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "abb05bf2-2a10-4964-a75c-f2c732d09682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['current-schema-id', 'current-snapshot-id', 'default-sort-order-id',\n",
       "       'default-spec-id', 'format-version', 'last-column-id',\n",
       "       'last-partition-id', 'last-updated-ms', 'location', 'metadata-log',\n",
       "       'partition-spec', 'partition-specs', 'properties', 'refs', 'schema',\n",
       "       'schemas', 'snapshot-log', 'snapshots', 'sort-orders', 'table-uuid'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_tempdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f44b9003-ec1e-4a66-a7f1-aa90e2fcf8c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "Name: current-schema-id, dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_tempdf['current-schema-id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4cd86e88-aec0-4602-b8db-8f96cb053a45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Row(manifest-list='s3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/customer_table/metadata/snap-3986362509610264284-1-385f35b6-3a5a-4d30-a7a1-156fa6a8c7b2.avro', parent-snapshot-id=None, schema-id=0, snapshot-id=3986362509610264284, summary=Row(added-data-files='1', added-files-size='1106', added-records='1', changed-partition-count='1', deleted-data-files=None, deleted-records=None, operation='append', removed-files-size=None, spark.app.id='spark-application-1684536915149', total-data-files='1', total-delete-files='0', total-equality-deletes='0', total-files-size='1106', total-position-deletes='0', total-records='1'), timestamp-ms=1684537347083),\n",
       "  Row(manifest-list='s3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/customer_table/metadata/snap-6274129147237535833-1-021df821-94c9-424f-af1a-51fa31667010.avro', parent-snapshot-id=3986362509610264284, schema-id=0, snapshot-id=6274129147237535833, summary=Row(added-data-files='4', added-files-size='4760', added-records='9', changed-partition-count='4', deleted-data-files='1', deleted-records='1', operation='overwrite', removed-files-size='1106', spark.app.id='spark-application-1684536915149', total-data-files='4', total-delete-files='0', total-equality-deletes='0', total-files-size='4760', total-position-deletes='0', total-records='9'), timestamp-ms=1684537604014)]]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(json_tempdf['snapshots'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "340197e4-5203-483d-9700-a3fd982ceedc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [(1000, dob_hour, 4, hour)]\n",
       "Name: partition-spec, dtype: object"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_tempdf['partition-spec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2001b532-b800-4377-bfd9-e7fe5799d9f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Row(fields=[Row(field-id=1000, name='dob_hour', source-id=4, transform='hour')], spec-id=0)]]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(json_tempdf['partition-specs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156c215d-ba62-4046-82f8-6a8e21a96001",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM lakehouse.coffees_table_2.all_data_files;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3a962b-9da5-4256-b48e-b5cd17690323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c23bd6a-9e1a-4909-9f09-7caeb5524647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca0e554-abdf-4fbd-9a56-840c7602a57f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b08cb0-0cf9-4a87-ab17-5b4286023c05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d86f6c6-682c-45d6-a26e-54c93430a0d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a00307f-365a-492a-a710-692510544b69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fadb12-a5fc-4138-89e8-455459b7cab9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe197cc5-122b-4fa0-ba41-6d3889a466b3",
   "metadata": {},
   "source": [
    "### Partition Evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63961d6-9c2a-451d-b772-adc382e1c0da",
   "metadata": {},
   "source": [
    "Spark partitioning is a way to split the data into multiple partitions so that you can execute transformations on multiple partitions in parallel which allows completing the job faster. You can also write partitioned data into a file system (multiple sub-directories) for faster reads by downstream systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0463d053-187c-4f32-8bbe-53c4ece328dc",
   "metadata": {},
   "source": [
    "Spark has several partitioning methods to achieve parallelism, based on your need, you should choose which one to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb47968-03c8-400b-93c4-2101ac1f14cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "738a10ca-a464-40f6-baa6-2aafdfb2d2bb",
   "metadata": {},
   "source": [
    "Creating New Data to Test Partition Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b096c653-df75-4ba4-bf13-5bcbfacd9f43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c3851f4-d681-44cc-965c-835667ffd4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Version : VersionInfo(major='0', minor='2', patch='1', release='', build='')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, device_id: string, country: string, manufacturer: string, model_line: string, event_type: string, event_ts: timestamp]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import LongType, IntegerType, StringType\n",
    "\n",
    "import dbldatagen as dg\n",
    "\n",
    "shuffle_partitions_requested = 20\n",
    "device_population = 100000\n",
    "data_rows = 20 * 1000000\n",
    "#partitions_requested = 20\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", shuffle_partitions_requested)\n",
    "\n",
    "country_codes = [\n",
    "    \"CN\", \"US\", \"FR\", \"CA\", \"IN\", \"JM\", \"IE\", \"PK\", \"GB\", \"IL\", \"AU\", \n",
    "    \"SG\", \"ES\", \"GE\", \"MX\", \"ET\", \"SA\", \"LB\", \"NL\", \"IT\"\n",
    "]\n",
    "#country_weights = [\n",
    "#    1300, 365, 67, 38, 1300, 3, 7, 212, 67, 9, 25, 6, 47, 83, \n",
    "#    126, 109, 58, 8, 17,\n",
    "#]\n",
    "\n",
    "manufacturers = [\n",
    "    \"Delta corp\", \"Xyzzy Inc.\", \"Lakehouse Ltd\", \"Acme Corp\", \"Embanks Devices\",\n",
    "]\n",
    "\n",
    "lines = [\"delta\", \"xyzzy\", \"lakehouse\", \"gadget\", \"droid\"]\n",
    "\n",
    "testDataSpec = (\n",
    "    dg.DataGenerator(spark, name=\"device_data_set\", rows=data_rows) \n",
    "                     #,partitions=partitions_requested)\n",
    "    .withIdOutput()\n",
    "    # we'll use hash of the base field to generate the ids to\n",
    "    # avoid a simple incrementing sequence\n",
    "    .withColumn(\"internal_device_id\", \"long\", minValue=0x1000000000000, \n",
    "                uniqueValues=device_population, omit=True, baseColumnType=\"hash\",\n",
    "    )\n",
    "    # note for format strings, we must use \"%lx\" not \"%x\" as the\n",
    "    # underlying value is a long\n",
    "    .withColumn(\n",
    "        \"device_id\", \"string\", format=\"0x%013x\", baseColumn=\"internal_device_id\"\n",
    "    )\n",
    "    # the device / user attributes will be the same for the same device id\n",
    "    # so lets use the internal device id as the base column for these attribute\n",
    "    .withColumn(\"country\", \"string\", values=country_codes, #weights=country_weights, \n",
    "                baseColumn=\"internal_device_id\")\n",
    "    .withColumn(\"manufacturer\", \"string\", values=manufacturers, \n",
    "                baseColumn=\"internal_device_id\", )\n",
    "    # use omit = True if you don't want a column to appear in the final output\n",
    "    # but just want to use it as part of generation of another column\n",
    "    .withColumn(\"line\", \"string\", values=lines, baseColumn=\"manufacturer\", \n",
    "                baseColumnType=\"hash\", omit=True )\n",
    "    .withColumn(\"model_ser\", \"integer\", minValue=1, maxValue=11, baseColumn=\"device_id\", \n",
    "                baseColumnType=\"hash\", omit=True, )\n",
    "    .withColumn(\"model_line\", \"string\", expr=\"concat(line, '#', model_ser)\", \n",
    "                baseColumn=[\"line\", \"model_ser\"] )\n",
    "    .withColumn(\"event_type\", \"string\", \n",
    "                values=[\"activation\", \"deactivation\", \"plan change\", \"telecoms activity\", \n",
    "                        \"internet activity\", \"device error\", ],\n",
    "                random=True)\n",
    "    .withColumn(\"event_ts\", \"timestamp\", begin=\"2020-01-01 01:00:00\", \n",
    "                end=\"2020-12-31 23:59:00\", \n",
    "                interval=\"1 minute\", random=True )\n",
    ")\n",
    "\n",
    "dfTestData = testDataSpec.build()\n",
    "\n",
    "display(dfTestData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37a41d6d-fd68-4fa0-914d-a84bbf298fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(id=0, device_id='0x100000001281d', country='US', manufacturer='Xyzzy Inc.', model_line='lakehouse#10', event_type='internet activity', event_ts=datetime.datetime(2020, 11, 4, 3, 44))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTestData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bbedb46-48c7-472f-a1db-2e2df8ce4adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.sources.partitionColumnTypeInference.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842ee72a-ec1a-4ed4-8aba-3a2a4baf5863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84cb0635-e535-4758-919f-d4f04bacf0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS spark_catalog.lakehouse.partition_evol_tbl PURGE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "559589db-5dd5-4e49-ba44-b8b8143d12b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfTestData.groupBy(\"country\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa4ed5eb-90ae-4960-b9f6-83a3df408409",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfTestData.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0594e7-4e60-45cf-a7e9-ac26c9478e87",
   "metadata": {},
   "source": [
    "Iceberg requires the data to be sorted according to the partition spec per task (Spark partition) in prior to write against partitioned table. This applies both Writing with SQL and Writing with DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50873884-250f-4872-8df8-befeadf6b797",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfTestData.sortWithinPartitions(\"country\").writeTo(\"spark_catalog.lakehouse.p_evol_tbl\").partitionedBy(\"country\").using(\"iceberg\").create()#.append()#replace()#overwritePartitions()#create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85761304-fcbb-405c-aab8-221c6fad602b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"SELECT * FROM spark_catalog.lakehouse.part_evol_tbl.PARTITIONS\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04502b8-874e-49ec-b3c3-aac117ce29ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"SELECT * FROM spark_catalog.lakehouse.part_evol_tbl.files\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f76c097-d1c1-4339-af78-c97c245ea00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"SELECT * FROM spark_catalog.lakehouse.part_evol_tbl.manifests\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e6a8be-01f8-4be9-beb1-f953c0ad7e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"SELECT * FROM spark_catalog.lakehouse.part_evol_tbl.all_manifests\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f2bf66-d219-4e65-a58e-260bf891e2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"SELECT * FROM spark_catalog.lakehouse.part_evol_tbl.all_data_files\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88b8694-61bb-4507-af92-7b288b50e4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"SELECT * FROM spark_catalog.lakehouse.part_evol_tbl.snapshots\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684e0727-c58d-4f37-a682-7748f566e205",
   "metadata": {},
   "source": [
    "Adding a partition field is a metadata operation and does not change any of the existing table data. New data will be written with the new partitioning, but existing data will remain in the old partition layout. Old data files will have null values for the new partition fields in metadata tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd2c9fe1-7594-4909-8744-59dde048eb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TABLE PARTITIONS BEFORE ALTER PARTITION STATEMENT: \n",
      "+---------+------------+----------+-------+\n",
      "|partition|record_count|file_count|spec_id|\n",
      "+---------+------------+----------+-------+\n",
      "|     {CN}|     1000100|        10|      0|\n",
      "|     {CA}|     1000301|        10|      0|\n",
      "|     {IT}|      999401|        10|      0|\n",
      "|     {IL}|     1000474|        10|      0|\n",
      "|     {NL}|     1000498|        10|      0|\n",
      "|     {AU}|     1000859|        10|      0|\n",
      "|     {GB}|      999318|        10|      0|\n",
      "|     {US}|     1000098|        10|      0|\n",
      "|     {SG}|      999532|        10|      0|\n",
      "|     {LB}|      998789|        10|      0|\n",
      "|     {IN}|      999328|        10|      0|\n",
      "|     {IE}|      998731|        10|      0|\n",
      "|     {FR}|      999338|        10|      0|\n",
      "|     {ES}|     1000299|        10|      0|\n",
      "|     {GE}|      999322|        10|      0|\n",
      "|     {MX}|     1001198|        10|      0|\n",
      "|     {PK}|     1002144|        10|      0|\n",
      "|     {JM}|     1000840|        10|      0|\n",
      "|     {SA}|      998932|        10|      0|\n",
      "|     {ET}|     1000498|        10|      0|\n",
      "+---------+------------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"TABLE PARTITIONS BEFORE ALTER PARTITION STATEMENT: \")\n",
    "spark.sql(\"SELECT * FROM spark_catalog.lakehouse.p_evol_tbl.PARTITIONS\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56dd4330-e06c-46fa-87b4-3624555daa2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADD PARTITION BY BUCKETING DEVICE ID:\n",
      "ALTER TABLE spark_catalog.lakehouse.p_evol_tbl ADD PARTITION FIELD months(event_ts)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"ADD PARTITION BY EVENT TIMESTAMP MONTHS: \")\n",
    "print(\"ALTER TABLE spark_catalog.lakehouse.p_evol_tbl ADD PARTITION FIELD months(event_ts)\")\n",
    "spark.sql(\"ALTER TABLE spark_catalog.lakehouse.p_evol_tbl ADD PARTITION FIELD months(event_ts)\")\n",
    "#spark.sql(\"ALTER TABLE spark_catalog.lakehouse.part_evol_tbl REPLACE PARTITION FIELD hours(dob) WITH state\")\n",
    "#spark.sql(\"ALTER TABLE prod.db.sample ADD PARTITION FIELD month\")\n",
    "\n",
    "#ALTER TABLE spark_catalog.lakehouse.part_evol_tbl ADD PARTITION FIELD days(event_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bee150f5-9d59-4b23-8df3-e6f5d70e2fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TABLE PARTITIONS AFTER ALTER PARTITION STATEMENT: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+----------+-------+\n",
      "|partition|record_count|file_count|spec_id|\n",
      "+---------+------------+----------+-------+\n",
      "|     {US}|     1000098|        10|      0|\n",
      "|     {LB}|      998789|        10|      0|\n",
      "|     {FR}|      999338|        10|      0|\n",
      "|     {GE}|      999322|        10|      0|\n",
      "|     {IN}|      999328|        10|      0|\n",
      "|     {SG}|      999532|        10|      0|\n",
      "|     {IE}|      998731|        10|      0|\n",
      "|     {JM}|     1000840|        10|      0|\n",
      "|     {CN}|     1000100|        10|      0|\n",
      "|     {SA}|      998932|        10|      0|\n",
      "|     {AU}|     1000859|        10|      0|\n",
      "|     {IT}|      999401|        10|      0|\n",
      "|     {ET}|     1000498|        10|      0|\n",
      "|     {PK}|     1002144|        10|      0|\n",
      "|     {GB}|      999318|        10|      0|\n",
      "|     {MX}|     1001198|        10|      0|\n",
      "|     {NL}|     1000498|        10|      0|\n",
      "|     {CA}|     1000301|        10|      0|\n",
      "|     {IL}|     1000474|        10|      0|\n",
      "|     {ES}|     1000299|        10|      0|\n",
      "+---------+------------+----------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"TABLE PARTITIONS AFTER ALTER PARTITION STATEMENT: \")\n",
    "spark.sql(\"SELECT * FROM spark_catalog.lakehouse.p_evol_tbl.PARTITIONS\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "baa64157-5a82-4c6d-82f8-f94bd08e8601",
   "metadata": {},
   "outputs": [],
   "source": [
    "appendDf = dfTestData.sample(fraction=0.3, seed=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "494691b4-e56a-41d5-98e3-0f033f26d59b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'bigint'),\n",
       " ('device_id', 'string'),\n",
       " ('country', 'string'),\n",
       " ('manufacturer', 'string'),\n",
       " ('model_line', 'string'),\n",
       " ('event_type', 'string'),\n",
       " ('event_ts', 'timestamp')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "appendDf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11f2ca29-d54b-4aba-a9f8-ad1019616b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "appendDf.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f33af64b-c2ca-447f-a0b7-185e223d973c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+-------+---------------+------------+-----------------+-------------------+\n",
      "| id|      device_id|country|   manufacturer|  model_line|       event_type|           event_ts|\n",
      "+---+---------------+-------+---------------+------------+-----------------+-------------------+\n",
      "|  0|0x100000001281d|     US|     Xyzzy Inc.|lakehouse#10|internet activity|2020-11-04 03:44:00|\n",
      "|  4|0x1000000003674|     SA|     Xyzzy Inc.| lakehouse#6|       activation|2020-07-05 01:45:00|\n",
      "|  5|0x100000001492c|     IN|Embanks Devices| lakehouse#7|telecoms activity|2020-01-24 22:31:00|\n",
      "| 17|0x1000000010aa2|     MX|Embanks Devices| lakehouse#1|     deactivation|2020-09-29 01:01:00|\n",
      "| 20|0x1000000001f27|     SG|     Xyzzy Inc.|lakehouse#11|      plan change|2020-10-26 00:21:00|\n",
      "| 23|0x10000000039b4|     GB|      Acme Corp|    droid#10|     deactivation|2020-11-11 02:00:00|\n",
      "| 28|0x1000000003a10|     CN|     Delta corp|    gadget#1|       activation|2020-10-28 03:30:00|\n",
      "| 29|0x1000000014cd0|     SA|     Xyzzy Inc.| lakehouse#1|      plan change|2020-08-01 09:42:00|\n",
      "| 31|0x1000000012c1c|     IN|Embanks Devices|lakehouse#11|internet activity|2020-09-06 09:02:00|\n",
      "| 42|0x100000000ca38|     IN|Embanks Devices| lakehouse#4|      plan change|2020-04-14 02:11:00|\n",
      "| 50|0x1000000010f2a|     MX|Embanks Devices| lakehouse#2|      plan change|2020-05-01 11:39:00|\n",
      "| 52|0x1000000002cbf|     SG|     Xyzzy Inc.| lakehouse#7|telecoms activity|2020-04-03 05:12:00|\n",
      "| 53|0x100000000464b|     SG|     Xyzzy Inc.| lakehouse#2|telecoms activity|2020-05-05 16:50:00|\n",
      "| 54|0x100000000f48c|     CN|     Delta corp|    gadget#7|telecoms activity|2020-12-12 16:42:00|\n",
      "| 56|0x1000000003acb|     PK|  Lakehouse Ltd|     xyzzy#6|     device error|2020-04-15 02:19:00|\n",
      "| 58|0x1000000001b87|     CA|      Acme Corp|     droid#7|internet activity|2020-01-11 03:37:00|\n",
      "| 64|0x1000000004ede|     IE|     Xyzzy Inc.| lakehouse#6|     deactivation|2020-04-28 23:09:00|\n",
      "| 71|0x100000000ec1b|     IT|Embanks Devices| lakehouse#8|internet activity|2020-04-19 05:19:00|\n",
      "| 73|0x1000000006ae8|     IN|Embanks Devices| lakehouse#2|internet activity|2020-06-02 09:15:00|\n",
      "| 74|0x100000000572b|     SG|     Xyzzy Inc.|lakehouse#11|     deactivation|2020-09-28 03:25:00|\n",
      "+---+---------------+-------+---------------+------------+-----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "appendDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ea0b278-5cb4-4bc3-aff9-f988b338c9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+-------+------------+----------+-----------------+-------------------+\n",
      "|  id|      device_id|country|manufacturer|model_line|       event_type|           event_ts|\n",
      "+----+---------------+-------+------------+----------+-----------------+-------------------+\n",
      "| 132|0x1000000008146|     AU|  Delta corp|  gadget#2|     device error|2020-04-24 10:00:00|\n",
      "| 150|0x100000001540e|     AU|  Delta corp| gadget#11|internet activity|2020-10-11 06:21:00|\n",
      "| 217|0x100000000582e|     AU|  Delta corp|  gadget#2|telecoms activity|2020-04-11 14:24:00|\n",
      "| 279|0x10000000183b6|     AU|  Delta corp| gadget#10|      plan change|2020-03-31 04:31:00|\n",
      "| 368|0x100000000c2d2|     AU|  Delta corp|  gadget#1|       activation|2020-03-25 11:11:00|\n",
      "| 489|0x10000000185d2|     AU|  Delta corp|  gadget#8|      plan change|2020-12-11 13:21:00|\n",
      "| 515|0x100000001729a|     AU|  Delta corp|  gadget#3|telecoms activity|2020-04-12 14:19:00|\n",
      "| 804|0x100000000d1be|     AU|  Delta corp|  gadget#2|      plan change|2020-11-16 08:48:00|\n",
      "| 943|0x1000000014702|     AU|  Delta corp|  gadget#8|telecoms activity|2020-12-24 16:58:00|\n",
      "| 947|0x1000000011106|     AU|  Delta corp| gadget#11|     deactivation|2020-12-04 21:39:00|\n",
      "|1008|0x100000001716e|     AU|  Delta corp|  gadget#3|telecoms activity|2020-12-19 17:25:00|\n",
      "|1014|0x10000000067ba|     AU|  Delta corp|  gadget#6|      plan change|2020-06-29 10:26:00|\n",
      "|1288|0x1000000009852|     AU|  Delta corp|  gadget#6|       activation|2020-04-15 17:19:00|\n",
      "|1332|0x10000000171e6|     AU|  Delta corp|  gadget#6|     deactivation|2020-01-22 00:35:00|\n",
      "|1356|0x100000000e0be|     AU|  Delta corp|  gadget#8|telecoms activity|2020-07-21 04:27:00|\n",
      "|1436|0x10000000002a2|     AU|  Delta corp|  gadget#2|      plan change|2020-02-18 01:33:00|\n",
      "|1445|0x1000000008fce|     AU|  Delta corp|  gadget#4|internet activity|2020-06-30 07:55:00|\n",
      "|1464|0x10000000062e2|     AU|  Delta corp|  gadget#2|telecoms activity|2020-09-19 17:37:00|\n",
      "|1482|0x10000000089a2|     AU|  Delta corp|  gadget#7|      plan change|2020-12-11 08:23:00|\n",
      "|1483|0x100000000bd1e|     AU|  Delta corp|  gadget#1|telecoms activity|2020-09-02 12:48:00|\n",
      "+----+---------------+-------+------------+----------+-----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "appendDf.sortWithinPartitions(\"country\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fdf4dcea-b8d5-43a9-ae67-b3aebf07d938",
   "metadata": {},
   "outputs": [],
   "source": [
    "#appendDf.sortWithinPartitions(\"country\", \"month(event_ts)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60e1c1ea-6949-49e1-a6ea-5022033abc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "appendDf.sortWithinPartitions(\"country\").writeTo(\"spark_catalog.lakehouse.p_evol_tbl\").using(\"iceberg\").append() #.append()#replace()#overwritePartitions()#create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef167cb7-a230-4d45-8619-87090d850789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TABLE PARTITIONS AFTER APPEND: \n",
      "+----------+------------+----------+-------+\n",
      "| partition|record_count|file_count|spec_id|\n",
      "+----------+------------+----------+-------+\n",
      "| {MX, 610}|       24539|        10|      1|\n",
      "| {CA, 606}|       25261|        10|      1|\n",
      "| {ES, 603}|       24582|        10|      1|\n",
      "| {MX, 601}|       23816|        10|      1|\n",
      "|{SG, null}|      999532|        10|      0|\n",
      "| {FR, 600}|       25141|        10|      1|\n",
      "| {IE, 600}|       25480|        10|      1|\n",
      "| {IE, 603}|       24541|        10|      1|\n",
      "| {SG, 600}|       25147|        10|      1|\n",
      "| {SG, 604}|       25366|        10|      1|\n",
      "| {IL, 601}|       23763|        10|      1|\n",
      "| {MX, 611}|       25176|        10|      1|\n",
      "| {IE, 605}|       24326|        10|      1|\n",
      "| {LB, 607}|       25535|        10|      1|\n",
      "| {IN, 611}|       25312|        10|      1|\n",
      "| {JM, 601}|       24042|        10|      1|\n",
      "|{IE, null}|      998731|        10|      0|\n",
      "| {GE, 611}|       25223|        10|      1|\n",
      "| {CA, 608}|       24660|        10|      1|\n",
      "| {MX, 605}|       24805|        10|      1|\n",
      "|{ET, null}|     1000498|        10|      0|\n",
      "| {CA, 610}|       24829|        10|      1|\n",
      "| {PK, 602}|       25631|        10|      1|\n",
      "| {FR, 604}|       25312|        10|      1|\n",
      "| {LB, 600}|       25272|        10|      1|\n",
      "| {ES, 611}|       25477|        10|      1|\n",
      "| {IE, 607}|       25201|        10|      1|\n",
      "| {SA, 603}|       24362|        10|      1|\n",
      "| {SG, 608}|       24696|        10|      1|\n",
      "|{GE, null}|      999322|        10|      0|\n",
      "| {ET, 602}|       25334|        10|      1|\n",
      "| {IT, 603}|       24673|        10|      1|\n",
      "| {CN, 602}|       25433|        10|      1|\n",
      "| {ET, 603}|       24606|        10|      1|\n",
      "| {GE, 602}|       25056|        10|      1|\n",
      "| {IT, 606}|       25561|        10|      1|\n",
      "|{GB, null}|      999318|        10|      0|\n",
      "| {CN, 606}|       25412|        10|      1|\n",
      "| {ES, 601}|       23577|        10|      1|\n",
      "| {CN, 608}|       24475|        10|      1|\n",
      "| {JM, 608}|       24913|        10|      1|\n",
      "| {SA, 611}|       25435|        10|      1|\n",
      "| {PK, 606}|       25220|        10|      1|\n",
      "| {CA, 603}|       24276|        10|      1|\n",
      "| {FR, 607}|       25340|        10|      1|\n",
      "| {NL, 602}|       25440|        10|      1|\n",
      "| {JM, 607}|       25413|        10|      1|\n",
      "| {AU, 600}|       25445|        10|      1|\n",
      "| {GB, 605}|       24702|        10|      1|\n",
      "| {SG, 602}|       25433|        10|      1|\n",
      "|{IT, null}|      999401|        10|      0|\n",
      "| {GE, 600}|       25201|        10|      1|\n",
      "|{IL, null}|     1000474|        10|      0|\n",
      "| {IL, 609}|       25477|        10|      1|\n",
      "|{ES, null}|     1000299|        10|      0|\n",
      "| {GB, 606}|       25386|        10|      1|\n",
      "| {CN, 600}|       25211|        10|      1|\n",
      "| {IT, 605}|       24661|        10|      1|\n",
      "| {JM, 605}|       24465|        10|      1|\n",
      "| {IN, 608}|       24547|        10|      1|\n",
      "| {IT, 601}|       23792|        10|      1|\n",
      "| {ET, 608}|       24625|        10|      1|\n",
      "| {US, 603}|       24536|        10|      1|\n",
      "| {CN, 603}|       24473|        10|      1|\n",
      "| {ES, 602}|       25370|        10|      1|\n",
      "| {NL, 605}|       24614|        10|      1|\n",
      "| {NL, 600}|       25313|        10|      1|\n",
      "| {IL, 600}|       25472|        10|      1|\n",
      "| {GE, 610}|       24551|        10|      1|\n",
      "| {US, 607}|       25313|        10|      1|\n",
      "|{CA, null}|     1000301|        10|      0|\n",
      "| {IL, 606}|       25497|        10|      1|\n",
      "| {PK, 609}|       25381|        10|      1|\n",
      "| {IT, 602}|       25203|        10|      1|\n",
      "| {GE, 603}|       24654|        10|      1|\n",
      "| {JM, 611}|       25369|        10|      1|\n",
      "| {IN, 605}|       24998|        10|      1|\n",
      "| {FR, 608}|       24704|        10|      1|\n",
      "| {GE, 609}|       25463|        10|      1|\n",
      "| {MX, 600}|       25231|        10|      1|\n",
      "| {GB, 607}|       25562|        10|      1|\n",
      "| {GE, 604}|       25214|        10|      1|\n",
      "| {IN, 603}|       24599|        10|      1|\n",
      "| {SA, 604}|       25545|        10|      1|\n",
      "| {FR, 603}|       24293|        10|      1|\n",
      "| {IN, 609}|       25335|        10|      1|\n",
      "|{PK, null}|     1002144|        10|      0|\n",
      "| {AU, 610}|       24791|        10|      1|\n",
      "| {SG, 610}|       24620|        10|      1|\n",
      "| {CN, 611}|       25069|        10|      1|\n",
      "| {AU, 602}|       25426|        10|      1|\n",
      "| {IT, 611}|       25503|        10|      1|\n",
      "| {LB, 605}|       24879|        10|      1|\n",
      "| {MX, 603}|       24707|        10|      1|\n",
      "| {ET, 604}|       25041|        10|      1|\n",
      "| {ET, 610}|       24557|        10|      1|\n",
      "| {LB, 603}|       24656|        10|      1|\n",
      "| {PK, 611}|       25437|        10|      1|\n",
      "| {JM, 606}|       25184|        10|      1|\n",
      "| {PK, 610}|       24623|        10|      1|\n",
      "+----------+------------+----------+-------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"TABLE PARTITIONS AFTER APPEND: \")\n",
    "spark.sql(\"SELECT * FROM spark_catalog.lakehouse.p_evol_tbl.PARTITIONS\").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ef51fc-950b-4a96-85c3-6cdb7c472e37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5406060-fea0-4002-a915-be5bd29aa08b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b3860d-d5b9-47f5-b494-0ffe5f0d190e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a009cb8e-e564-40aa-bebf-e1229b4c5341",
   "metadata": {},
   "source": [
    "Dropping a partition field is a metadata operation and does not change any of the existing table data. New data will be written with the new partitioning, but existing data will remain in the old partition layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ea7e0fb9-374e-4965-93da-776e91c2b787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"ALTER TABLE spark_catalog.lakehouse.part_evol_tbl DROP PARTITION FIELD bucket(16, device_id)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5340bace-0227-48b8-b984-d0136381f672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAR SALES TABLE PARTITIONS AFTER ALTER PARTITION STATEMENT: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 80:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-------+\n",
      "| partition|record_count|file_count|spec_id|\n",
      "+----------+------------+----------+-------+\n",
      "|{CN, null}|    10000790|        10|      0|\n",
      "|{IT, null}|    10002099|        10|      0|\n",
      "|{US, null}|     9998801|        10|      0|\n",
      "|{AU, null}|     9997695|        10|      0|\n",
      "|{GE, null}|    10005212|        10|      0|\n",
      "|{PK, null}|    10002237|        10|      0|\n",
      "|{ES, null}|    10001299|        10|      0|\n",
      "|{SA, null}|     9998077|        10|      0|\n",
      "|{ET, null}|    10001822|        10|      0|\n",
      "|{NL, null}|     9998304|        10|      0|\n",
      "|{IN, null}|    10004181|        10|      0|\n",
      "|{MX, null}|    10002210|        10|      0|\n",
      "|{GB, null}|     9993405|        10|      0|\n",
      "|{IE, null}|     9998066|        10|      0|\n",
      "|{CA, null}|     9997659|        10|      0|\n",
      "|{LB, null}|     9996707|        10|      0|\n",
      "|{JM, null}|    10000921|        10|      0|\n",
      "|{IL, null}|     9998131|        10|      0|\n",
      "|{FR, null}|    10003914|        10|      0|\n",
      "|{SG, null}|     9998470|        10|      0|\n",
      "+----------+------------+----------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"TABLE PARTITIONS AFTER ALTER PARTITION STATEMENT: \")\n",
    "spark.sql(\"SELECT * FROM spark_catalog.lakehouse.part_evol_tbl.PARTITIONS\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068a2029-59ec-44dd-bbe0-1ba04777548e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8512c690-0009-47a8-8046-7477ed7b99b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144ebaf9-0d8c-4ff0-90fb-516b9b2f97a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e57e5ac-497b-4692-afb3-d28f4d54d5fb",
   "metadata": {},
   "source": [
    "##### Only json files have been added (one per each time you repartitioned) but Avro files have stayed the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "489132be-d671-46a6-92c5-6dc291638f84",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'boto3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_200/3277444492.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ms3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboto3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m's3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmy_bucket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBucket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"go01-demo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmetadata_file_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'boto3' is not defined"
     ]
    }
   ],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket(\"go01-demo\")\n",
    "\n",
    "metadata_file_list = []\n",
    "\n",
    "print(\"Current Metadata Files: \\n\")\n",
    "\n",
    "for object_summary in my_bucket.objects.filter(Prefix=metadata_path+\"/metadata\"):\n",
    "    #print(object_summary.key +\"\\n\")\n",
    "    metadata_file_list.append(object_summary.key)\n",
    "    \n",
    "metadata_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6af64da-19fc-47b3-afd8-5a938704ee11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1b5aed-4bba-482d-ac22-8e0f30cb970a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abd06d2-fb84-4a96-ab3d-9c9697cd85ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b998fc45-091b-4776-81d0-402aeb8673e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162a1ede-72a3-40bf-9cbf-1ae4a410832f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc464b86-ebea-41a7-9afe-6917f262b39c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd33dd11-1439-408e-912b-dc65d7f12fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE TABLE IF NOT EXISTS customer_table (id BIGINT, state STRING, country STRING, dob TIMESTAMP) USING iceberg PARTITIONED BY ( hours(dob))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7930a494-94d8-4fee-b873-8120257fbbac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|hour(dob)|\n",
      "+---------+\n",
      "|        0|\n",
      "|        0|\n",
      "|        0|\n",
      "|        0|\n",
      "|        0|\n",
      "|        0|\n",
      "|        0|\n",
      "|        0|\n",
      "|        0|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT HOUR(dob) FROM spark_catalog.lakehouse.customer_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "485b48f5-4eaf-4edf-ad66-2a30a19b6c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|day(dob)|\n",
      "+--------+\n",
      "|       1|\n",
      "|       1|\n",
      "|       1|\n",
      "|       1|\n",
      "|       2|\n",
      "|       3|\n",
      "|       3|\n",
      "|       4|\n",
      "|       4|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT DAY(dob) FROM spark_catalog.lakehouse.customer_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178a933e-5ee3-4a64-93e0-85fac5dc544c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0086fbd-2b28-4df6-baf9-17768c5a7b9b",
   "metadata": {},
   "source": [
    "### Dropping Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abab5842-1722-470e-8066-c58e7659192e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS lakehouse.staging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f903baae-7c19-45a9-95d3-79d929491ed2",
   "metadata": {},
   "source": [
    "Validate that the metadata folder is now empty but the data folder still retains parquet files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fef6a1-ca58-48d2-83c2-e4e66dc2e3b1",
   "metadata": {},
   "source": [
    "![alt text](../img/s3_droptable_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eba9f0-a03c-4c17-bb4f-d1b9e6404a97",
   "metadata": {},
   "source": [
    "![alt text](../img/s3_droptable_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de2b0a5-8b24-4055-8250-d2ce689949c3",
   "metadata": {},
   "source": [
    "![alt text](../img/s3_droptable_3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43739d1f-8682-4575-98de-58f4ca4416f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"ALTER TABLE lakehouse.customers_table\\\n",
    "            SET TBLPROPERTIES ('format-version' = '2')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f38b4f0c-f054-4721-9789-5e3968be2985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warehouse/tablespace/external/hive/lakehouse_catalog.db/customers_table/metadata/00000-8f9c2e19-d640-404e-8fc8-c4287b667740.metadata.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket(\"go01-demo\")\n",
    "\n",
    "metadata_file_list = []\n",
    "\n",
    "for object_summary in my_bucket.objects.filter(Prefix=metadata_path):\n",
    "    print(object_summary.key +\"\\n\")\n",
    "    metadata_file_list.append(object_summary.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e0ba146b-fdfb-41e3-bc75-65ce5c4c5bc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_378/2700701042.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Showing \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmetadata_file_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multiline\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"s3a://go01-demo/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmetadata_file_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(\"Showing \" + metadata_file_list[3])\n",
    "spark.read.option(\"multiline\",\"true\").json(\"s3a://go01-demo/\" + metadata_file_list[3]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdce88a-df1c-4e93-960e-fcf732bb9df0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
