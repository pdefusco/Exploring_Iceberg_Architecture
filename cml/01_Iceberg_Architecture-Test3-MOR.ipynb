{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0942afe9-7629-4b3e-b747-b795a6454a2b",
   "metadata": {},
   "source": [
    "## Introduction to Iceberg Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f7e7d5e-5a02-414b-9177-49cdfe38fddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cmlbootstrap\n",
      "  Cloning https://github.com/fastforwardlabs/cmlbootstrap to /tmp/pip-install-tb34dxdk/cmlbootstrap_c4102002e2d549098c3141d566e609e9\n",
      "  Running command git clone -q https://github.com/fastforwardlabs/cmlbootstrap /tmp/pip-install-tb34dxdk/cmlbootstrap_c4102002e2d549098c3141d566e609e9\n",
      "Requirement already satisfied: pandas in /home/cdsw/.local/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (1.3.5)\n",
      "Requirement already satisfied: numpy==1.19.0 in /home/cdsw/.local/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (1.19.0)\n",
      "Requirement already satisfied: boto3 in /home/cdsw/.local/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (1.17.62)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 5)) (2.25.1)\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 6)) (1.26.6)\n",
      "Requirement already satisfied: dbldatagen in /home/cdsw/.local/lib/python3.7/site-packages (from -r requirements.txt (line 7)) (0.2.1)\n",
      "Requirement already satisfied: PyArrow in /home/cdsw/.local/lib/python3.7/site-packages (from -r requirements.txt (line 8)) (12.0.0)\n",
      "Requirement already satisfied: requests-kerberos==0.12.0 in /home/cdsw/.local/lib/python3.7/site-packages (from cmlbootstrap->-r requirements.txt (line 4)) (0.12.0)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.62 in /home/cdsw/.local/lib/python3.7/site-packages (from boto3->-r requirements.txt (line 3)) (1.20.112)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/cdsw/.local/lib/python3.7/site-packages (from boto3->-r requirements.txt (line 3)) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /home/cdsw/.local/lib/python3.7/site-packages (from boto3->-r requirements.txt (line 3)) (0.4.2)\n",
      "Requirement already satisfied: cryptography>=1.3 in /home/cdsw/.local/lib/python3.7/site-packages (from requests-kerberos==0.12.0->cmlbootstrap->-r requirements.txt (line 4)) (40.0.2)\n",
      "Requirement already satisfied: pykerberos<2.0.0,>=1.1.8 in /home/cdsw/.local/lib/python3.7/site-packages (from requests-kerberos==0.12.0->cmlbootstrap->-r requirements.txt (line 4)) (1.2.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests->-r requirements.txt (line 5)) (2020.11.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests->-r requirements.txt (line 5)) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests->-r requirements.txt (line 5)) (2.10)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/site-packages (from botocore<1.21.0,>=1.20.62->boto3->-r requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/site-packages (from cryptography>=1.3->requests-kerberos==0.12.0->cmlbootstrap->-r requirements.txt (line 4)) (1.14.4)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/site-packages (from cffi>=1.12->cryptography>=1.3->requests-kerberos==0.12.0->cmlbootstrap->-r requirements.txt (line 4)) (2.20)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.62->boto3->-r requirements.txt (line 3)) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 1)) (2021.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd15c6f-e31d-440c-bc48-966385b7c5ec",
   "metadata": {},
   "source": [
    "#### Launching a Spark Session with Iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f139b6a-eed6-4848-9e82-593aabad58b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 03:58:03 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "23/09/04 03:58:03 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "Setting spark.hadoop.yarn.resourcemanager.principal to pauldefusco\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/04 03:58:03 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "23/09/04 03:58:03 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "23/09/04 03:58:03 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "23/09/04 03:58:05 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:135)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/04 03:58:05 WARN HttpChannel: /jobs/\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:135)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/04 03:58:06 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "23/09/04 03:58:06 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/04 03:58:06 WARN HttpChannel: /jobs/\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/04 03:58:07 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/04 03:58:07 WARN HttpChannel: /jobs/\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/04 03:58:08 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/04 03:58:08 WARN HttpChannel: /jobs/\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/04 03:58:09 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/04 03:58:09 WARN HttpChannel: /jobs/\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/04 03:58:10 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/04 03:58:10 WARN HttpChannel: /jobs/\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/04 03:58:11 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/04 03:58:11 WARN HttpChannel: /jobs/\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/04 03:58:12 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/04 03:58:12 WARN HttpChannel: /jobs/\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/04 03:58:13 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/04 03:58:13 WARN HttpChannel: /jobs/\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/04 03:58:13 WARN HiveServer2CredentialProvider: Failed to get HS2 delegation token\n",
      "java.util.NoSuchElementException: spark.sql.hive.hiveserver2.jdbc.url\n",
      "\tat org.apache.spark.SparkConf.$anonfun$get$1(SparkConf.scala:245)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.SparkConf.get(SparkConf.scala:245)\n",
      "\tat com.hortonworks.spark.deploy.yarn.security.HiveServer2CredentialProvider.obtainDelegationTokens(HiveServer2CredentialProvider.scala:64)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager.$anonfun$obtainDelegationTokens$2(HadoopDelegationTokenManager.scala:164)\n",
      "\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat scala.collection.MapLike$DefaultValuesIterable.foreach(MapLike.scala:213)\n",
      "\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)\n",
      "\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)\n",
      "\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager.org$apache$spark$deploy$security$HadoopDelegationTokenManager$$obtainDelegationTokens(HadoopDelegationTokenManager.scala:162)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager$$anon$4.run(HadoopDelegationTokenManager.scala:226)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager$$anon$4.run(HadoopDelegationTokenManager.scala:224)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager.obtainTokensAndScheduleRenewal(HadoopDelegationTokenManager.scala:224)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager.org$apache$spark$deploy$security$HadoopDelegationTokenManager$$updateTokensTask(HadoopDelegationTokenManager.scala:198)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager.start(HadoopDelegationTokenManager.scala:123)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.$anonfun$start$1(CoarseGrainedSchedulerBackend.scala:552)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.$anonfun$start$1$adapted(CoarseGrainedSchedulerBackend.scala:549)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.start(CoarseGrainedSchedulerBackend.scala:549)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.KubernetesClusterSchedulerBackend.start(KubernetesClusterSchedulerBackend.scala:95)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:220)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:581)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/04 03:58:14 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/04 03:58:14 WARN HttpChannel: /jobs/\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/04 03:58:15 WARN ExecutorAllocationManager: Dynamic allocation without a shuffle service is an experimental feature.\n",
      "23/09/04 03:58:18 WARN HiveClientImpl: Detected HiveConf hive.execution.engine is 'tez' and will be reset to 'mr' to disable useless hive logic\n",
      "Hive Session ID = 1ebdc555-69af-4d41-8bfb-5056ed42eb51\n",
      "23/09/04 03:58:24 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|         01_car_data|\n",
      "|           01_car_dw|\n",
      "|              adb101|\n",
      "|            airlines|\n",
      "|        airlines_csv|\n",
      "|    airlines_iceberg|\n",
      "|airlines_iceberg_...|\n",
      "|      airlines_mjain|\n",
      "|          airquality|\n",
      "|                ajvp|\n",
      "|          atlas_demo|\n",
      "|            bankdemo|\n",
      "|          bca_jps_l0|\n",
      "|        cde_workshop|\n",
      "|             cdedemo|\n",
      "|        cdp_overview|\n",
      "|      ceht_open_data|\n",
      "|        ceht_scratch|\n",
      "| ceht_transportation|\n",
      "|        cgsifacebook|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cml.data_v1 as cmldata\n",
    "\n",
    "CONNECTION_NAME = \"go01-aw-dl\"\n",
    "conn = cmldata.get_connection(CONNECTION_NAME)\n",
    "spark = conn.get_spark_session()\n",
    "\n",
    "# Sample usage to run query through spark\n",
    "EXAMPLE_SQL_QUERY = \"show databases\"\n",
    "spark.sql(EXAMPLE_SQL_QUERY).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3bbf1bf-a0a1-4864-8c98-88dfb994060a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 03:58:25 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "23/09/04 03:58:25 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('spark.eventLog.enabled', 'true'),\n",
       " ('spark.network.crypto.enabled', 'true'),\n",
       " ('spark.sql.hive.hwc.execution.mode', 'spark'),\n",
       " ('spark.jars',\n",
       "  '/opt/spark/optional-lib/hive-warehouse-connector-assembly.jar,/opt/spark/optional-lib/iceberg-hive-runtime.jar,/opt/spark/optional-lib/iceberg-spark-runtime.jar'),\n",
       " ('spark.app.startTime', '1693799883971'),\n",
       " ('spark.kerberos.renewal.credentials', 'ccache'),\n",
       " ('spark.sql.catalog.spark_catalog',\n",
       "  'org.apache.iceberg.spark.SparkSessionCatalog'),\n",
       " ('spark.dynamicAllocation.maxExecutors', '49'),\n",
       " ('spark.app.id', 'spark-application-1693799885897'),\n",
       " ('spark.eventLog.dir', 'file:///sparkeventlogs'),\n",
       " ('spark.hadoop.yarn.resourcemanager.principal', 'pauldefusco'),\n",
       " ('spark.kubernetes.driver.annotation.cluster-autoscaler.kubernetes.io/safe-to-evict',\n",
       "  'false'),\n",
       " ('spark.ui.port', '20049'),\n",
       " ('spark.yarn.access.hadoopFileSystems',\n",
       "  's3a://go01-demo/warehouse/tablespace/external/hive'),\n",
       " ('spark.sql.extensions',\n",
       "  'com.qubole.spark.hiveacid.HiveAcidAutoConvertExtension,org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions'),\n",
       " ('spark.driver.bindAddress', '100.100.150.135'),\n",
       " ('spark.kubernetes.executor.annotation.cluster-autoscaler.kubernetes.io/safe-to-evict',\n",
       "  'false'),\n",
       " ('spark.driver.memory', '3051m'),\n",
       " ('spark.kubernetes.executor.podNamePrefix', 'cdsw-nyvv8j0nfp64m2bb'),\n",
       " ('spark.io.encryption.enabled', 'true'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.ui.proxyRedirectUri',\n",
       "  'https://spark-nyvv8j0nfp64m2bb.ml-b74f8940-b97.go01-dem.ylcu-atmi.cloudera.site'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.master', 'k8s://https://172.20.0.1:443'),\n",
       " ('spark.ui.allowFramingFrom',\n",
       "  'https://ml-b74f8940-b97.go01-dem.ylcu-atmi.cloudera.site'),\n",
       " ('spark.kubernetes.executor.podTemplateFile', '/tmp/spark-executor.json'),\n",
       " ('spark.kubernetes.driver.pod.name', 'nyvv8j0nfp64m2bb'),\n",
       " ('spark.dynamicAllocation.shuffleTracking.enabled', 'true'),\n",
       " ('spark.kubernetes.container.image',\n",
       "  'docker.repository.cloudera.com/cloudera/cdsw/ml-runtime-jupyterlab-python3.7-standard:2022.11.1-b2'),\n",
       " ('spark.kubernetes.namespace', 'mlx-user-16'),\n",
       " ('spark.kubernetes.executor.config.dir', '/var/spark/conf'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  's3a://go01-demo/warehouse/tablespace/external/hive'),\n",
       " ('spark.hadoop.iceberg.engine.hive.enabled', 'true'),\n",
       " ('spark.repl.local.jars',\n",
       "  'file:///runtime-addons/spark323-19-hf2-g3aapf/opt/spark/optional-lib/hive-warehouse-connector-assembly-1.0.0.1.19.7215.0-118.jar,file:///runtime-addons/spark323-19-hf2-g3aapf/opt/spark/optional-lib/iceberg-hive-runtime-1.1.0.1.19.7215.0-118.jar,file:///runtime-addons/spark323-19-hf2-g3aapf/opt/spark/optional-lib/iceberg-spark-runtime-3.2_2.12-1.1.0.1.19.7215.0-118.jar'),\n",
       " ('spark.driver.port', '43569'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.host', '100.100.150.135'),\n",
       " ('spark.yarn.rmProxy.enabled', 'false'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.kryo.registrator',\n",
       "  'com.qubole.spark.hiveacid.util.HiveAcidKyroRegistrator'),\n",
       " ('spark.app.initial.jar.urls',\n",
       "  'spark://100.100.150.135:43569/jars/iceberg-spark-runtime-3.2_2.12-1.1.0.1.19.7215.0-118.jar,spark://100.100.150.135:43569/jars/hive-warehouse-connector-assembly-1.0.0.1.19.7215.0-118.jar,spark://100.100.150.135:43569/jars/iceberg-hive-runtime-1.1.0.1.19.7215.0-118.jar'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.dynamicAllocation.enabled', 'true'),\n",
       " ('spark.deploy.mode', 'client'),\n",
       " ('spark.app.name', 'None'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.sql.catalog.spark_catalog.type', 'hive'),\n",
       " ('spark.authenticate', 'true')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa007770-01e4-4c2e-a784-6955c448c952",
   "metadata": {},
   "source": [
    "### Iceberg Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad5615b-c7e6-4528-bd48-db0841d38c39",
   "metadata": {},
   "source": [
    "![alt text](../img/iceberg-metadata.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c3c93c-4065-422b-9697-609ae50687c7",
   "metadata": {},
   "source": [
    "#### Iceberg Catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30001dd5-c3ef-4223-84ca-e63552c4f879",
   "metadata": {},
   "source": [
    "Iceberg comes with catalogs that enable SQL commands to manage tables and load them by name. Catalogs are configured using properties under spark.sql.catalog.(catalog_name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3819a08-8e00-4184-bdc6-79b3d8507d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+\n",
      "|      catalog|namespace|\n",
      "+-------------+---------+\n",
      "|spark_catalog|  default|\n",
      "+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show catalog and database\n",
    "spark.sql(\"SHOW CURRENT NAMESPACE\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1119faa-93a7-4ebe-85b1-3ec87eb54b7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new database\n",
    "#spark.sql(\"DROP DATABASE IF EXISTS spark_catalog.lakehouse\")\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS spark_catalog.lakehouse\")\n",
    "spark.sql(\"USE spark_catalog.lakehouse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "834ca50c-9c2f-43ac-9ec0-9cf28e8507d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+\n",
      "|      catalog|namespace|\n",
      "+-------------+---------+\n",
      "|spark_catalog|lakehouse|\n",
      "+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show catalog and database\n",
    "spark.sql(\"SHOW CURRENT NAMESPACE\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6cecda-848a-4cbe-b7a8-d73096cab11f",
   "metadata": {},
   "source": [
    "#### Create an Iceberg Table with Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c5daa62-cde3-440a-ad91-0c6f9b41ef3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS lakehouse.coffees_table_3 PURGE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b10e6c-efcc-46b5-a164-893879b79327",
   "metadata": {},
   "source": [
    "# TEST 3 - COW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b19e0de9-29c4-43dd-ae5c-c2bd69558032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE IF NOT EXISTS coffees_table_3 (coffee_id BIGINT, coffee_size STRING, coffee_sale_ts TIMESTAMP)\\\n",
    "          USING ICEBERG\\\n",
    "          PARTITIONED BY (months(coffee_sale_ts))\\\n",
    "          TBLPROPERTIES ('write.delete.mode'='merge-on-read',\\\n",
    "                          'write.update.mode'='merge-on-read',\\\n",
    "                          'write.merge.mode'='merge-on-read',\\\n",
    "                          'format-version' = '2')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eea95e7-e221-4997-9071-208cbc9f9fc5",
   "metadata": {},
   "source": [
    "#### Verify that a Metadata JSON file has been created under the Metadata directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2668955d-14e9-43c9-a359-61cd9ae5bff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = \"warehouse/tablespace/external/hive/lakehouse.db/coffees_table_3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfa6cae7-8598-4088-8de8-751341c257b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata File Path: warehouse/tablespace/external/hive/lakehouse.db/coffees_table_3/metadata/00000-60da2bb9-7e09-4618-af3f-68ab7e8df9fe.metadata.json\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "my_bucket = s3.Bucket(\"go01-demo\")\n",
    "\n",
    "for object_summary in my_bucket.objects.filter(Prefix=metadata_path):\n",
    "    #print(object_summary.key)\n",
    "    metadata_file = object_summary.key\n",
    "    \n",
    "print(\"Metadata File Path: {}\".format(metadata_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e03d542a-f982-4b9a-bc19-84a26d504cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>current-schema-id</th>\n",
       "      <th>current-snapshot-id</th>\n",
       "      <th>default-sort-order-id</th>\n",
       "      <th>default-spec-id</th>\n",
       "      <th>format-version</th>\n",
       "      <th>last-column-id</th>\n",
       "      <th>last-partition-id</th>\n",
       "      <th>last-sequence-number</th>\n",
       "      <th>last-updated-ms</th>\n",
       "      <th>location</th>\n",
       "      <th>metadata-log</th>\n",
       "      <th>partition-specs</th>\n",
       "      <th>properties</th>\n",
       "      <th>schemas</th>\n",
       "      <th>snapshot-log</th>\n",
       "      <th>snapshots</th>\n",
       "      <th>sort-orders</th>\n",
       "      <th>statistics</th>\n",
       "      <th>table-uuid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>1693799933483</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[([Row(field-id=1000, name='coffee_sale_ts_mon...</td>\n",
       "      <td>(pauldefusco, merge-on-read, merge-on-read, me...</td>\n",
       "      <td>[([Row(id=1, name='coffee_id', required=False,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[([], 0)]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0d731042-eee8-4223-aa6a-404e6248d543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   current-schema-id  current-snapshot-id  default-sort-order-id  \\\n",
       "0                  0                   -1                      0   \n",
       "\n",
       "   default-spec-id  format-version  last-column-id  last-partition-id  \\\n",
       "0                0               2               3               1000   \n",
       "\n",
       "   last-sequence-number  last-updated-ms  \\\n",
       "0                     0    1693799933483   \n",
       "\n",
       "                                            location metadata-log  \\\n",
       "0  s3a://go01-demo/warehouse/tablespace/external/...           []   \n",
       "\n",
       "                                     partition-specs  \\\n",
       "0  [([Row(field-id=1000, name='coffee_sale_ts_mon...   \n",
       "\n",
       "                                          properties  \\\n",
       "0  (pauldefusco, merge-on-read, merge-on-read, me...   \n",
       "\n",
       "                                             schemas snapshot-log snapshots  \\\n",
       "0  [([Row(id=1, name='coffee_id', required=False,...           []        []   \n",
       "\n",
       "  sort-orders statistics                            table-uuid  \n",
       "0   [([], 0)]         []  0d731042-eee8-4223-aa6a-404e6248d543  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "spark.read.option(\"multiline\",\"true\").json(\"s3a://go01-demo/\" + \"warehouse/tablespace/external/hive/lakehouse.db/coffees_table_3/metadata\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357d7e23-9be4-4b92-bbb0-cb69da6f1476",
   "metadata": {},
   "source": [
    "![alt text](../img/s3_metadata.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f6b96e-d878-4e2e-91a0-1ef785833479",
   "metadata": {},
   "source": [
    "#### Notice that no snapshots or other files have been created as data has not yet been inserted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "972dced2-83a3-4c4f-a63d-2936a54534d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+---------+-------------------+\n",
      "|made_current_at|snapshot_id|parent_id|is_current_ancestor|\n",
      "+---------------+-----------+---------+-------------------+\n",
      "+---------------+-----------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM lakehouse.coffees_table_3.history\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d726f825-38b0-4342-a0ea-0f932ce20355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+---------+---------+-------------+-------+\n",
      "|committed_at|snapshot_id|parent_id|operation|manifest_list|summary|\n",
      "+------------+-----------+---------+---------+-------------+-------+\n",
      "+------------+-----------+---------+---------+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM lakehouse.coffees_table_3.snapshots;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68f5ad55-0adb-4ab3-8574-da44c3af6b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----------+-------+---------+------------+------------------+------------+------------+-----------------+----------------+------------+------------+------------+-------------+------------+-------------+\n",
      "|content|file_path|file_format|spec_id|partition|record_count|file_size_in_bytes|column_sizes|value_counts|null_value_counts|nan_value_counts|lower_bounds|upper_bounds|key_metadata|split_offsets|equality_ids|sort_order_id|\n",
      "+-------+---------+-----------+-------+---------+------------+------------------+------------+------------+-----------------+----------------+------------+------------+------------+-------------+------------+-------------+\n",
      "+-------+---------+-----------+-------+---------+------------+------------------+------------+------------+-----------------+----------------+------------+------------+------------+-------------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM lakehouse.coffees_table_3.files;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "636820a4-528b-42f2-9153-2180206420de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+-----------------+-----------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+-------------------+\n",
      "|content|path|length|partition_spec_id|added_snapshot_id|added_data_files_count|existing_data_files_count|deleted_data_files_count|added_delete_files_count|existing_delete_files_count|deleted_delete_files_count|partition_summaries|\n",
      "+-------+----+------+-----------------+-----------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+-------------------+\n",
      "+-------+----+------+-----------------+-----------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM lakehouse.coffees_table_3.manifests;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8544499-8ba1-4156-b641-89c10c281926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----------+-------+---------+------------+------------------+------------+------------+-----------------+----------------+------------+------------+------------+-------------+------------+-------------+\n",
      "|content|file_path|file_format|spec_id|partition|record_count|file_size_in_bytes|column_sizes|value_counts|null_value_counts|nan_value_counts|lower_bounds|upper_bounds|key_metadata|split_offsets|equality_ids|sort_order_id|\n",
      "+-------+---------+-----------+-------+---------+------------+------------------+------------+------------+-----------------+----------------+------------+------------+------------+-------------+------------+-------------+\n",
      "+-------+---------+-----------+-------+---------+------------+------------------+------------+------------+-----------------+----------------+------------+------------+------------+-------------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM lakehouse.coffees_table_3.all_data_files;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "323639fc-1ed3-418d-ae0e-8c1674a30ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+-----------------+-----------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+-------------------+---------------------+\n",
      "|content|path|length|partition_spec_id|added_snapshot_id|added_data_files_count|existing_data_files_count|deleted_data_files_count|added_delete_files_count|existing_delete_files_count|deleted_delete_files_count|partition_summaries|reference_snapshot_id|\n",
      "+-------+----+------+-----------------+-----------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+-------------------+---------------------+\n",
      "+-------+----+------+-----------------+-----------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+-------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM lakehouse.coffees_table_3.all_manifests;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24046132-45da-4f28-a60b-f0227b8fb7cc",
   "metadata": {},
   "source": [
    "### Table Insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "028f245c-e167-4418-9e96-fd098e0cb6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f2d7725-fe94-4a8c-82d0-319c1cefef8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Row: coffee_id = 1, coffee_size = venti, coffee_sale_ts = 2023-07-01\n",
    "#Row: coffee_id = 2, coffee_size = grande, coffee_sale_ts = 2023-07-01\n",
    "#Row: coffee_id = 3, coffee_size = tall, coffee_sale_ts = 2023-04-01\n",
    "\n",
    "spark.sql(\"INSERT INTO lakehouse.coffees_table_3 VALUES (1, 'venti', cast(date_format('2023-07-01 10:00:00', 'yyyy-MM-dd HH:mm:ss') as timestamp)),\\\n",
    "            (2, 'grande', cast(date_format('2023-07-01 10:00:00', 'yyyy-MM-dd HH:mm:ss') as timestamp)),\\\n",
    "            (3, 'tall', cast(date_format('2023-04-01 10:00:00', 'yyyy-MM-dd HH:mm:ss') as timestamp))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f0d668-9e68-4b1c-a881-5d1265f999c7",
   "metadata": {},
   "source": [
    "#### Data has been added to the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da0242b8-93b9-4471-9aae-7a8f162a2320",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = \"select h.made_current_at,\\\n",
    "            s.operation,\\\n",
    "            h.snapshot_id,\\\n",
    "            h.is_current_ancestor,\\\n",
    "            s.summary['spark.app.id']\\\n",
    "        from lakehouse.coffees_table.history h\\\n",
    "        join lakehouse.coffees_table.snapshots s\\\n",
    "            on h.snapshot_id = s.snapshot_id\\\n",
    "            order by made_current_at;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76eaa58b-c77b-437a-979d-feaa4ac9b536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 03:59:14 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>made_current_at</th>\n",
       "      <th>operation</th>\n",
       "      <th>snapshot_id</th>\n",
       "      <th>is_current_ancestor</th>\n",
       "      <th>summary[spark.app.id]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-09-04 02:47:21.766</td>\n",
       "      <td>append</td>\n",
       "      <td>6464031631726591033</td>\n",
       "      <td>True</td>\n",
       "      <td>spark-application-1693795583203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-09-04 02:47:43.813</td>\n",
       "      <td>overwrite</td>\n",
       "      <td>5876718107486774833</td>\n",
       "      <td>True</td>\n",
       "      <td>spark-application-1693795583203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          made_current_at  operation          snapshot_id  \\\n",
       "0 2023-09-04 02:47:21.766     append  6464031631726591033   \n",
       "1 2023-09-04 02:47:43.813  overwrite  5876718107486774833   \n",
       "\n",
       "   is_current_ancestor            summary[spark.app.id]  \n",
       "0                 True  spark-application-1693795583203  \n",
       "1                 True  spark-application-1693795583203  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(QUERY).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3728c7dc-ac75-4859-8790-501210b43381",
   "metadata": {},
   "source": [
    "#### Notice there are now two json files and two avro files. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44f25b0-3aea-4b67-a5e5-bed3bf2b6a9d",
   "metadata": {},
   "source": [
    "The first json file is the metadata file created when the table was created. This is the metata file prefixed by 00000. The second json file is the new metadata file reflecting the insert of one row. This is the metadata file prefixed by 00001."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f46899f-2ecd-4651-9dcf-7e6c4d912273",
   "metadata": {},
   "source": [
    "The avro file with the \"snap\" prefix is the manifest list. The other avro file created is the corresponding manifest file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8968ed18-4759-4cf0-9e18-98f537517a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Metadata Files: \n",
      "\n",
      "warehouse/tablespace/external/hive/lakehouse.db/coffees_table_3/metadata/00000-60da2bb9-7e09-4618-af3f-68ab7e8df9fe.metadata.json\n",
      "\n",
      "warehouse/tablespace/external/hive/lakehouse.db/coffees_table_3/metadata/00001-eeec84c8-a650-4033-83ef-9ef2cd6c81b7.metadata.json\n",
      "\n",
      "warehouse/tablespace/external/hive/lakehouse.db/coffees_table_3/metadata/ebfddaac-adcc-4d9b-91b6-6b41c31286c5-m0.avro\n",
      "\n",
      "warehouse/tablespace/external/hive/lakehouse.db/coffees_table_3/metadata/snap-4815467340052359539-1-ebfddaac-adcc-4d9b-91b6-6b41c31286c5.avro\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket(\"go01-demo\")\n",
    "\n",
    "metadata_file_list = []\n",
    "\n",
    "print(\"Current Metadata Files: \\n\")\n",
    "for object_summary in my_bucket.objects.filter(Prefix=metadata_path+\"/metadata\"):\n",
    "    print(object_summary.key +\"\\n\")\n",
    "    metadata_file_list.append(object_summary.key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f44c26-c842-4843-a126-2f96df9b4496",
   "metadata": {},
   "source": [
    "Showing Metadata Files (JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7cb383ed-5604-4d35-a525-bd7fb3501c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing warehouse/tablespace/external/hive/lakehouse.db/coffees_table_3/metadata/00000-60da2bb9-7e09-4618-af3f-68ab7e8df9fe.metadata.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>current-schema-id</th>\n",
       "      <th>current-snapshot-id</th>\n",
       "      <th>default-sort-order-id</th>\n",
       "      <th>default-spec-id</th>\n",
       "      <th>format-version</th>\n",
       "      <th>last-column-id</th>\n",
       "      <th>last-partition-id</th>\n",
       "      <th>last-sequence-number</th>\n",
       "      <th>last-updated-ms</th>\n",
       "      <th>location</th>\n",
       "      <th>metadata-log</th>\n",
       "      <th>partition-specs</th>\n",
       "      <th>properties</th>\n",
       "      <th>schemas</th>\n",
       "      <th>snapshot-log</th>\n",
       "      <th>snapshots</th>\n",
       "      <th>sort-orders</th>\n",
       "      <th>statistics</th>\n",
       "      <th>table-uuid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>1693799933483</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[([Row(field-id=1000, name='coffee_sale_ts_mon...</td>\n",
       "      <td>(pauldefusco, merge-on-read, merge-on-read, me...</td>\n",
       "      <td>[([Row(id=1, name='coffee_id', required=False,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[([], 0)]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0d731042-eee8-4223-aa6a-404e6248d543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   current-schema-id  current-snapshot-id  default-sort-order-id  \\\n",
       "0                  0                   -1                      0   \n",
       "\n",
       "   default-spec-id  format-version  last-column-id  last-partition-id  \\\n",
       "0                0               2               3               1000   \n",
       "\n",
       "   last-sequence-number  last-updated-ms  \\\n",
       "0                     0    1693799933483   \n",
       "\n",
       "                                            location metadata-log  \\\n",
       "0  s3a://go01-demo/warehouse/tablespace/external/...           []   \n",
       "\n",
       "                                     partition-specs  \\\n",
       "0  [([Row(field-id=1000, name='coffee_sale_ts_mon...   \n",
       "\n",
       "                                          properties  \\\n",
       "0  (pauldefusco, merge-on-read, merge-on-read, me...   \n",
       "\n",
       "                                             schemas snapshot-log snapshots  \\\n",
       "0  [([Row(id=1, name='coffee_id', required=False,...           []        []   \n",
       "\n",
       "  sort-orders statistics                            table-uuid  \n",
       "0   [([], 0)]         []  0d731042-eee8-4223-aa6a-404e6248d543  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"Showing \" + metadata_file_list[0])\n",
    "spark.read.option(\"multiline\",\"true\").json(\"s3a://go01-demo/\" + metadata_file_list[0]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77983734-5f46-41bc-892e-9295ca6bb6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing warehouse/tablespace/external/hive/lakehouse.db/coffees_table_3/metadata/00001-eeec84c8-a650-4033-83ef-9ef2cd6c81b7.metadata.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>current-schema-id</th>\n",
       "      <th>current-snapshot-id</th>\n",
       "      <th>default-sort-order-id</th>\n",
       "      <th>default-spec-id</th>\n",
       "      <th>format-version</th>\n",
       "      <th>last-column-id</th>\n",
       "      <th>last-partition-id</th>\n",
       "      <th>last-sequence-number</th>\n",
       "      <th>last-updated-ms</th>\n",
       "      <th>location</th>\n",
       "      <th>metadata-log</th>\n",
       "      <th>partition-specs</th>\n",
       "      <th>properties</th>\n",
       "      <th>refs</th>\n",
       "      <th>schemas</th>\n",
       "      <th>snapshot-log</th>\n",
       "      <th>snapshots</th>\n",
       "      <th>sort-orders</th>\n",
       "      <th>statistics</th>\n",
       "      <th>table-uuid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4815467340052359539</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>1693799952760</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>[(s3a://go01-demo/warehouse/tablespace/externa...</td>\n",
       "      <td>[([Row(field-id=1000, name='coffee_sale_ts_mon...</td>\n",
       "      <td>(pauldefusco, merge-on-read, merge-on-read, me...</td>\n",
       "      <td>((4815467340052359539, branch),)</td>\n",
       "      <td>[([Row(id=1, name='coffee_id', required=False,...</td>\n",
       "      <td>[(4815467340052359539, 1693799952760)]</td>\n",
       "      <td>[(s3a://go01-demo/warehouse/tablespace/externa...</td>\n",
       "      <td>[([], 0)]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0d731042-eee8-4223-aa6a-404e6248d543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   current-schema-id  current-snapshot-id  default-sort-order-id  \\\n",
       "0                  0  4815467340052359539                      0   \n",
       "\n",
       "   default-spec-id  format-version  last-column-id  last-partition-id  \\\n",
       "0                0               2               3               1000   \n",
       "\n",
       "   last-sequence-number  last-updated-ms  \\\n",
       "0                     1    1693799952760   \n",
       "\n",
       "                                            location  \\\n",
       "0  s3a://go01-demo/warehouse/tablespace/external/...   \n",
       "\n",
       "                                        metadata-log  \\\n",
       "0  [(s3a://go01-demo/warehouse/tablespace/externa...   \n",
       "\n",
       "                                     partition-specs  \\\n",
       "0  [([Row(field-id=1000, name='coffee_sale_ts_mon...   \n",
       "\n",
       "                                          properties  \\\n",
       "0  (pauldefusco, merge-on-read, merge-on-read, me...   \n",
       "\n",
       "                               refs  \\\n",
       "0  ((4815467340052359539, branch),)   \n",
       "\n",
       "                                             schemas  \\\n",
       "0  [([Row(id=1, name='coffee_id', required=False,...   \n",
       "\n",
       "                             snapshot-log  \\\n",
       "0  [(4815467340052359539, 1693799952760)]   \n",
       "\n",
       "                                           snapshots sort-orders statistics  \\\n",
       "0  [(s3a://go01-demo/warehouse/tablespace/externa...   [([], 0)]         []   \n",
       "\n",
       "                             table-uuid  \n",
       "0  0d731042-eee8-4223-aa6a-404e6248d543  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Showing \" + metadata_file_list[1])\n",
    "spark.read.option(\"multiline\",\"true\").json(\"s3a://go01-demo/\" + metadata_file_list[1]).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833c5078-51ca-4d2b-a877-344197cee888",
   "metadata": {},
   "source": [
    "Showing Manifest List (AVRO - prefixed by \"SNAP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1eb30215-5e39-429d-b0a7-6284c2b25a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing warehouse/tablespace/external/hive/lakehouse.db/coffees_table_3/metadata/snap-4815467340052359539-1-ebfddaac-adcc-4d9b-91b6-6b41c31286c5.avro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>manifest_path</th>\n",
       "      <th>manifest_length</th>\n",
       "      <th>partition_spec_id</th>\n",
       "      <th>content</th>\n",
       "      <th>sequence_number</th>\n",
       "      <th>min_sequence_number</th>\n",
       "      <th>added_snapshot_id</th>\n",
       "      <th>added_data_files_count</th>\n",
       "      <th>existing_data_files_count</th>\n",
       "      <th>deleted_data_files_count</th>\n",
       "      <th>added_rows_count</th>\n",
       "      <th>existing_rows_count</th>\n",
       "      <th>deleted_rows_count</th>\n",
       "      <th>partitions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>7163</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4815467340052359539</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(False, False, [127, 2, 0, 0], [130, 2, 0, 0])]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       manifest_path  manifest_length  \\\n",
       "0  s3a://go01-demo/warehouse/tablespace/external/...             7163   \n",
       "\n",
       "   partition_spec_id  content  sequence_number  min_sequence_number  \\\n",
       "0                  0        0                1                    1   \n",
       "\n",
       "     added_snapshot_id  added_data_files_count  existing_data_files_count  \\\n",
       "0  4815467340052359539                       2                          0   \n",
       "\n",
       "   deleted_data_files_count  added_rows_count  existing_rows_count  \\\n",
       "0                         0                 3                    0   \n",
       "\n",
       "   deleted_rows_count                                        partitions  \n",
       "0                   0  [(False, False, [127, 2, 0, 0], [130, 2, 0, 0])]  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Showing \" + metadata_file_list[3])\n",
    "spark.read.format(\"avro\").load(\"s3a://go01-demo/\" + metadata_file_list[3]).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f050bd1-b1a6-4b1e-9e19-76d8abc1cdd3",
   "metadata": {},
   "source": [
    "Showing Manifest Files (Avro) i.e. which shows data file locations according to each snapshot_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f09c777a-d717-4c8c-a098-eca4972e0f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing warehouse/tablespace/external/hive/lakehouse.db/coffees_table_3/metadata/ebfddaac-adcc-4d9b-91b6-6b41c31286c5-m0.avro\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>snapshot_id</th>\n",
       "      <th>sequence_number</th>\n",
       "      <th>file_sequence_number</th>\n",
       "      <th>data_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4815467340052359539</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(0, s3a://go01-demo/warehouse/tablespace/exter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4815467340052359539</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(0, s3a://go01-demo/warehouse/tablespace/exter...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   status          snapshot_id  sequence_number  file_sequence_number  \\\n",
       "0       1  4815467340052359539              NaN                   NaN   \n",
       "1       1  4815467340052359539              NaN                   NaN   \n",
       "\n",
       "                                           data_file  \n",
       "0  (0, s3a://go01-demo/warehouse/tablespace/exter...  \n",
       "1  (0, s3a://go01-demo/warehouse/tablespace/exter...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Showing \" + metadata_file_list[2])\n",
    "spark.read.format(\"avro\").load(\"s3a://go01-demo/\" + metadata_file_list[2]).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f023c1ca-48c8-4d4d-b329-4825d473bf4b",
   "metadata": {},
   "source": [
    "### Table Merge Into"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bef6e0-6ea1-4f75-aeac-fd84b0b7f2f7",
   "metadata": {},
   "source": [
    "Create a staging table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8287cfa-9f8e-40fb-a880-a2dabcdbd2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS lakehouse.coffee_staging_3 PURGE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a742c99-a211-4fa6-bdd5-e63f8079f9d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE IF NOT EXISTS lakehouse.coffee_staging_3\\\n",
    "            (coffee_id BIGINT, coffee_size STRING, coffee_sale_ts TIMESTAMP)\\\n",
    "            USING iceberg\\\n",
    "            PARTITIONED BY (months(coffee_sale_ts))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d4dc4207-34e0-4bb3-a147-dfcd4e601990",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"INSERT INTO lakehouse.coffee_staging_3\\\n",
    "    VALUES (2, 'tall', cast(date_format('2023-08-01 11:10:00', 'yyyy-MM-dd HH:mm:ss') as timestamp)),\\\n",
    "    (3, 'venti', cast(date_format('2023-04-01 12:01:00', 'yyyy-MM-dd HH:mm:ss') as timestamp)),\\\n",
    "    (4, 'venti', cast(date_format('2023-07-01 12:01:00', 'yyyy-MM-dd HH:mm:ss') as timestamp)),\\\n",
    "    (5, 'grande', cast(date_format('2023-07-01 12:01:00', 'yyyy-MM-dd HH:mm:ss') as timestamp)),\\\n",
    "    (6, 'grande', cast(date_format('2023-07-01 12:01:00', 'yyyy-MM-dd HH:mm:ss') as timestamp)),\\\n",
    "    (7, 'venti', cast(date_format('2023-05-01 12:01:00', 'yyyy-MM-dd HH:mm:ss') as timestamp)),\\\n",
    "    (8, 'grande', cast(date_format('2023-04-01 12:01:00', 'yyyy-MM-dd HH:mm:ss') as timestamp)),\\\n",
    "    (9, 'tall', cast(date_format('2023-05-01 12:01:00', 'yyyy-MM-dd HH:mm:ss') as timestamp)),\\\n",
    "    (10, 'tall', cast(date_format('2023-05-01 12:01:00', 'yyyy-MM-dd HH:mm:ss') as timestamp))\")\n",
    "\n",
    "#Row: coffee_id = 2, coffee_size = tall, coffee_sale_ts = 2023-08-01\n",
    "#Row: coffee_id = 3, coffee_size = venti, coffee_sale_ts = 2023-04-01\n",
    "#Row: coffee_id = 4, coffee_size = venti, coffee_sale_ts = 2023-07-01\n",
    "#Row: coffee_id = 5, coffee_size = grande, coffee_sale_ts = 2023-07-01\n",
    "#Row: coffee_id = 7, coffee_size = grande, coffee_sale_ts = 2023-07-01\n",
    "#Row: coffee_id = 8, coffee_size = venti, coffee_sale_ts = 2023-07-01\n",
    "#Row: coffee_id = 9, coffee_size = grande, coffee_sale_ts = 2023-07-01\n",
    "#Row: coffee_id = 10, coffee_size = venti, coffee_sale_ts = 2023-07-01\n",
    "#Row: coffee_id = 11, coffee_size = grande, coffee_sale_ts = 2023-07-01\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c3720c-3011-4a15-924f-de39a525501d",
   "metadata": {},
   "source": [
    "Merge Into Customers Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b62e634-935e-4219-b240-fdb8411b2984",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"MERGE INTO lakehouse.coffees_table_3 c\\\n",
    "            USING (SELECT * FROM lakehouse.coffee_staging_3) s\\\n",
    "            ON c.coffee_id = s.coffee_id \\\n",
    "            WHEN MATCHED THEN UPDATE SET c.coffee_size = s.coffee_size \\\n",
    "            WHEN NOT MATCHED THEN INSERT *\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c94305e6-c808-4d67-8903-864a4d80b5dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>committed_at</th>\n",
       "      <th>snapshot_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>operation</th>\n",
       "      <th>manifest_list</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-09-04 03:59:12.760</td>\n",
       "      <td>4815467340052359539</td>\n",
       "      <td>NaN</td>\n",
       "      <td>append</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>{'spark.app.id': 'spark-application-1693799885...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-09-04 03:59:55.115</td>\n",
       "      <td>384109576346467287</td>\n",
       "      <td>4.815467e+18</td>\n",
       "      <td>overwrite</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>{'added-data-files': '5', 'added-position-dele...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             committed_at          snapshot_id     parent_id  operation  \\\n",
       "0 2023-09-04 03:59:12.760  4815467340052359539           NaN     append   \n",
       "1 2023-09-04 03:59:55.115   384109576346467287  4.815467e+18  overwrite   \n",
       "\n",
       "                                       manifest_list  \\\n",
       "0  s3a://go01-demo/warehouse/tablespace/external/...   \n",
       "1  s3a://go01-demo/warehouse/tablespace/external/...   \n",
       "\n",
       "                                             summary  \n",
       "0  {'spark.app.id': 'spark-application-1693799885...  \n",
       "1  {'added-data-files': '5', 'added-position-dele...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM lakehouse.coffees_table_3.snapshots;\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d2d2db3-6d38-428e-bc8d-51286ff2e11c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>path</th>\n",
       "      <th>length</th>\n",
       "      <th>partition_spec_id</th>\n",
       "      <th>added_snapshot_id</th>\n",
       "      <th>added_data_files_count</th>\n",
       "      <th>existing_data_files_count</th>\n",
       "      <th>deleted_data_files_count</th>\n",
       "      <th>added_delete_files_count</th>\n",
       "      <th>existing_delete_files_count</th>\n",
       "      <th>deleted_delete_files_count</th>\n",
       "      <th>partition_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>7350</td>\n",
       "      <td>0</td>\n",
       "      <td>384109576346467287</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(False, False, 2023-04, 2023-07)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>7163</td>\n",
       "      <td>0</td>\n",
       "      <td>4815467340052359539</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(False, False, 2023-04, 2023-07)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>7186</td>\n",
       "      <td>0</td>\n",
       "      <td>384109576346467287</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(False, False, 2023-04, 2023-07)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   content                                               path  length  \\\n",
       "0        0  s3a://go01-demo/warehouse/tablespace/external/...    7350   \n",
       "1        0  s3a://go01-demo/warehouse/tablespace/external/...    7163   \n",
       "2        1  s3a://go01-demo/warehouse/tablespace/external/...    7186   \n",
       "\n",
       "   partition_spec_id    added_snapshot_id  added_data_files_count  \\\n",
       "0                  0   384109576346467287                       5   \n",
       "1                  0  4815467340052359539                       2   \n",
       "2                  0   384109576346467287                       0   \n",
       "\n",
       "   existing_data_files_count  deleted_data_files_count  \\\n",
       "0                          0                         0   \n",
       "1                          0                         0   \n",
       "2                          0                         0   \n",
       "\n",
       "   added_delete_files_count  existing_delete_files_count  \\\n",
       "0                         0                            0   \n",
       "1                         0                            0   \n",
       "2                         2                            0   \n",
       "\n",
       "   deleted_delete_files_count                 partition_summaries  \n",
       "0                           0  [(False, False, 2023-04, 2023-07)]  \n",
       "1                           0  [(False, False, 2023-04, 2023-07)]  \n",
       "2                           0  [(False, False, 2023-04, 2023-07)]  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM lakehouse.coffees_table_3.manifests;\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db59e20d-b4fa-455c-8a34-b5538d06b2ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>file_path</th>\n",
       "      <th>file_format</th>\n",
       "      <th>spec_id</th>\n",
       "      <th>partition</th>\n",
       "      <th>record_count</th>\n",
       "      <th>file_size_in_bytes</th>\n",
       "      <th>column_sizes</th>\n",
       "      <th>value_counts</th>\n",
       "      <th>null_value_counts</th>\n",
       "      <th>nan_value_counts</th>\n",
       "      <th>lower_bounds</th>\n",
       "      <th>upper_bounds</th>\n",
       "      <th>key_metadata</th>\n",
       "      <th>split_offsets</th>\n",
       "      <th>equality_ids</th>\n",
       "      <th>sort_order_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>PARQUET</td>\n",
       "      <td>0</td>\n",
       "      <td>(642,)</td>\n",
       "      <td>3</td>\n",
       "      <td>1074</td>\n",
       "      <td>{1: 52, 2: 74, 3: 62}</td>\n",
       "      <td>{1: 3, 2: 3, 3: 3}</td>\n",
       "      <td>{1: 0, 2: 0, 3: 0}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{1: [4, 0, 0, 0, 0, 0, 0, 0], 2: [103, 114, 97...</td>\n",
       "      <td>{1: [6, 0, 0, 0, 0, 0, 0, 0], 2: [118, 101, 11...</td>\n",
       "      <td>None</td>\n",
       "      <td>[4]</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>PARQUET</td>\n",
       "      <td>0</td>\n",
       "      <td>(642,)</td>\n",
       "      <td>1</td>\n",
       "      <td>985</td>\n",
       "      <td>{1: 39, 2: 39, 3: 39}</td>\n",
       "      <td>{1: 1, 2: 1, 3: 1}</td>\n",
       "      <td>{1: 0, 2: 0, 3: 0}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{1: [2, 0, 0, 0, 0, 0, 0, 0], 2: [116, 97, 108...</td>\n",
       "      <td>{1: [2, 0, 0, 0, 0, 0, 0, 0], 2: [116, 97, 108...</td>\n",
       "      <td>None</td>\n",
       "      <td>[4]</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>PARQUET</td>\n",
       "      <td>0</td>\n",
       "      <td>(640,)</td>\n",
       "      <td>3</td>\n",
       "      <td>1068</td>\n",
       "      <td>{1: 52, 2: 72, 3: 62}</td>\n",
       "      <td>{1: 3, 2: 3, 3: 3}</td>\n",
       "      <td>{1: 0, 2: 0, 3: 0}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{1: [7, 0, 0, 0, 0, 0, 0, 0], 2: [116, 97, 108...</td>\n",
       "      <td>{1: [10, 0, 0, 0, 0, 0, 0, 0], 2: [118, 101, 1...</td>\n",
       "      <td>None</td>\n",
       "      <td>[4]</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>PARQUET</td>\n",
       "      <td>0</td>\n",
       "      <td>(639,)</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>{1: 39, 2: 41, 3: 39}</td>\n",
       "      <td>{1: 1, 2: 1, 3: 1}</td>\n",
       "      <td>{1: 0, 2: 0, 3: 0}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{1: [8, 0, 0, 0, 0, 0, 0, 0], 2: [103, 114, 97...</td>\n",
       "      <td>{1: [8, 0, 0, 0, 0, 0, 0, 0], 2: [103, 114, 97...</td>\n",
       "      <td>None</td>\n",
       "      <td>[4]</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>PARQUET</td>\n",
       "      <td>0</td>\n",
       "      <td>(639,)</td>\n",
       "      <td>1</td>\n",
       "      <td>992</td>\n",
       "      <td>{1: 39, 2: 40, 3: 39}</td>\n",
       "      <td>{1: 1, 2: 1, 3: 1}</td>\n",
       "      <td>{1: 0, 2: 0, 3: 0}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{1: [3, 0, 0, 0, 0, 0, 0, 0], 2: [118, 101, 11...</td>\n",
       "      <td>{1: [3, 0, 0, 0, 0, 0, 0, 0], 2: [118, 101, 11...</td>\n",
       "      <td>None</td>\n",
       "      <td>[4]</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>PARQUET</td>\n",
       "      <td>0</td>\n",
       "      <td>(642,)</td>\n",
       "      <td>2</td>\n",
       "      <td>1015</td>\n",
       "      <td>{1: 41, 2: 44, 3: 62}</td>\n",
       "      <td>{1: 2, 2: 2, 3: 2}</td>\n",
       "      <td>{1: 0, 2: 0, 3: 0}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{1: [1, 0, 0, 0, 0, 0, 0, 0], 2: [103, 114, 97...</td>\n",
       "      <td>{1: [2, 0, 0, 0, 0, 0, 0, 0], 2: [118, 101, 11...</td>\n",
       "      <td>None</td>\n",
       "      <td>[4]</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>PARQUET</td>\n",
       "      <td>0</td>\n",
       "      <td>(639,)</td>\n",
       "      <td>1</td>\n",
       "      <td>969</td>\n",
       "      <td>{1: 33, 2: 33, 3: 39}</td>\n",
       "      <td>{1: 1, 2: 1, 3: 1}</td>\n",
       "      <td>{1: 0, 2: 0, 3: 0}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{1: [3, 0, 0, 0, 0, 0, 0, 0], 2: [116, 97, 108...</td>\n",
       "      <td>{1: [3, 0, 0, 0, 0, 0, 0, 0], 2: [116, 97, 108...</td>\n",
       "      <td>None</td>\n",
       "      <td>[4]</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   content                                          file_path file_format  \\\n",
       "0        0  s3a://go01-demo/warehouse/tablespace/external/...     PARQUET   \n",
       "1        0  s3a://go01-demo/warehouse/tablespace/external/...     PARQUET   \n",
       "2        0  s3a://go01-demo/warehouse/tablespace/external/...     PARQUET   \n",
       "3        0  s3a://go01-demo/warehouse/tablespace/external/...     PARQUET   \n",
       "4        0  s3a://go01-demo/warehouse/tablespace/external/...     PARQUET   \n",
       "5        0  s3a://go01-demo/warehouse/tablespace/external/...     PARQUET   \n",
       "6        0  s3a://go01-demo/warehouse/tablespace/external/...     PARQUET   \n",
       "\n",
       "   spec_id partition  record_count  file_size_in_bytes           column_sizes  \\\n",
       "0        0    (642,)             3                1074  {1: 52, 2: 74, 3: 62}   \n",
       "1        0    (642,)             1                 985  {1: 39, 2: 39, 3: 39}   \n",
       "2        0    (640,)             3                1068  {1: 52, 2: 72, 3: 62}   \n",
       "3        0    (639,)             1                 999  {1: 39, 2: 41, 3: 39}   \n",
       "4        0    (639,)             1                 992  {1: 39, 2: 40, 3: 39}   \n",
       "5        0    (642,)             2                1015  {1: 41, 2: 44, 3: 62}   \n",
       "6        0    (639,)             1                 969  {1: 33, 2: 33, 3: 39}   \n",
       "\n",
       "         value_counts   null_value_counts nan_value_counts  \\\n",
       "0  {1: 3, 2: 3, 3: 3}  {1: 0, 2: 0, 3: 0}               {}   \n",
       "1  {1: 1, 2: 1, 3: 1}  {1: 0, 2: 0, 3: 0}               {}   \n",
       "2  {1: 3, 2: 3, 3: 3}  {1: 0, 2: 0, 3: 0}               {}   \n",
       "3  {1: 1, 2: 1, 3: 1}  {1: 0, 2: 0, 3: 0}               {}   \n",
       "4  {1: 1, 2: 1, 3: 1}  {1: 0, 2: 0, 3: 0}               {}   \n",
       "5  {1: 2, 2: 2, 3: 2}  {1: 0, 2: 0, 3: 0}               {}   \n",
       "6  {1: 1, 2: 1, 3: 1}  {1: 0, 2: 0, 3: 0}               {}   \n",
       "\n",
       "                                        lower_bounds  \\\n",
       "0  {1: [4, 0, 0, 0, 0, 0, 0, 0], 2: [103, 114, 97...   \n",
       "1  {1: [2, 0, 0, 0, 0, 0, 0, 0], 2: [116, 97, 108...   \n",
       "2  {1: [7, 0, 0, 0, 0, 0, 0, 0], 2: [116, 97, 108...   \n",
       "3  {1: [8, 0, 0, 0, 0, 0, 0, 0], 2: [103, 114, 97...   \n",
       "4  {1: [3, 0, 0, 0, 0, 0, 0, 0], 2: [118, 101, 11...   \n",
       "5  {1: [1, 0, 0, 0, 0, 0, 0, 0], 2: [103, 114, 97...   \n",
       "6  {1: [3, 0, 0, 0, 0, 0, 0, 0], 2: [116, 97, 108...   \n",
       "\n",
       "                                        upper_bounds key_metadata  \\\n",
       "0  {1: [6, 0, 0, 0, 0, 0, 0, 0], 2: [118, 101, 11...         None   \n",
       "1  {1: [2, 0, 0, 0, 0, 0, 0, 0], 2: [116, 97, 108...         None   \n",
       "2  {1: [10, 0, 0, 0, 0, 0, 0, 0], 2: [118, 101, 1...         None   \n",
       "3  {1: [8, 0, 0, 0, 0, 0, 0, 0], 2: [103, 114, 97...         None   \n",
       "4  {1: [3, 0, 0, 0, 0, 0, 0, 0], 2: [118, 101, 11...         None   \n",
       "5  {1: [2, 0, 0, 0, 0, 0, 0, 0], 2: [118, 101, 11...         None   \n",
       "6  {1: [3, 0, 0, 0, 0, 0, 0, 0], 2: [116, 97, 108...         None   \n",
       "\n",
       "  split_offsets equality_ids  sort_order_id  \n",
       "0           [4]         None              0  \n",
       "1           [4]         None              0  \n",
       "2           [4]         None              0  \n",
       "3           [4]         None              0  \n",
       "4           [4]         None              0  \n",
       "5           [4]         None              0  \n",
       "6           [4]         None              0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM lakehouse.coffees_table_3.all_data_files;\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3736265f-2336-4004-be33-3faed9c6c92b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table_3/data/coffee_sale_ts_month=2023-07/00036-1035-7c946d62-1215-44f1-9a50-7d5ef7eb35e7-00001.parquet'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM lakehouse.coffees_table_3.all_data_files;\").toPandas()['file_path'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9486357-7744-4068-998c-7bb4a9edf5da",
   "metadata": {},
   "source": [
    "#### There is a new metadata file (json) prefixed by 0002."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9308c678-d334-4a0b-8b14-94c0164c9be5",
   "metadata": {},
   "source": [
    "#### There is a new manifest list file (avro) prefixed by \"snap\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1685d9f-422a-44dc-a969-e4e3d5b0718c",
   "metadata": {},
   "source": [
    "#### There is a new manifest file (avro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "91fd6085-7047-437b-9b5d-08a5a43dc13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Metadata Files: \n",
      "\n",
      "warehouse/tablespace/external/hive/lakehouse.db/coffees_table_3/metadata/00000-60da2bb9-7e09-4618-af3f-68ab7e8df9fe.metadata.json\n",
      "warehouse/tablespace/external/hive/lakehouse.db/coffees_table_3/metadata/00001-eeec84c8-a650-4033-83ef-9ef2cd6c81b7.metadata.json\n",
      "warehouse/tablespace/external/hive/lakehouse.db/coffees_table_3/metadata/00002-f64af39b-d246-4783-afd2-2eef8c6a9941.metadata.json\n",
      "warehouse/tablespace/external/hive/lakehouse.db/coffees_table_3/metadata/92964479-485c-40b2-9b4e-e49a1f19fdc9-m0.avro\n",
      "warehouse/tablespace/external/hive/lakehouse.db/coffees_table_3/metadata/92964479-485c-40b2-9b4e-e49a1f19fdc9-m1.avro\n",
      "warehouse/tablespace/external/hive/lakehouse.db/coffees_table_3/metadata/ebfddaac-adcc-4d9b-91b6-6b41c31286c5-m0.avro\n",
      "warehouse/tablespace/external/hive/lakehouse.db/coffees_table_3/metadata/snap-384109576346467287-1-92964479-485c-40b2-9b4e-e49a1f19fdc9.avro\n",
      "warehouse/tablespace/external/hive/lakehouse.db/coffees_table_3/metadata/snap-4815467340052359539-1-ebfddaac-adcc-4d9b-91b6-6b41c31286c5.avro\n",
      "There is a total of 8 files in the Metadata layer\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket(\"go01-demo\")\n",
    "\n",
    "metadata_file_list = []\n",
    "\n",
    "print(\"Current Metadata Files: \\n\")\n",
    "\n",
    "for object_summary in my_bucket.objects.filter(Prefix=metadata_path+\"/metadata\"):\n",
    "    #print(object_summary.key +\"\\n\")\n",
    "    metadata_file_list.append(object_summary.key)\n",
    "    \n",
    "print(*metadata_file_list, sep = \"\\n\")\n",
    "\n",
    "print(\"There is a total of \" + str(len(metadata_file_list)) + \" files in the Metadata layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ae4a83-5c55-4ebc-a344-4363e9585ff3",
   "metadata": {},
   "source": [
    "Showing Latest (Current) Metadata File (JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df26dc95-0c4b-419b-b561-3ca5242de499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing warehouse/tablespace/external/hive/lakehouse.db/coffees_table_3/metadata/00002-f64af39b-d246-4783-afd2-2eef8c6a9941.metadata.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>current-schema-id</th>\n",
       "      <th>current-snapshot-id</th>\n",
       "      <th>default-sort-order-id</th>\n",
       "      <th>default-spec-id</th>\n",
       "      <th>format-version</th>\n",
       "      <th>last-column-id</th>\n",
       "      <th>last-partition-id</th>\n",
       "      <th>last-sequence-number</th>\n",
       "      <th>last-updated-ms</th>\n",
       "      <th>location</th>\n",
       "      <th>metadata-log</th>\n",
       "      <th>partition-specs</th>\n",
       "      <th>properties</th>\n",
       "      <th>schemas</th>\n",
       "      <th>snapshot-log</th>\n",
       "      <th>snapshots</th>\n",
       "      <th>sort-orders</th>\n",
       "      <th>statistics</th>\n",
       "      <th>table-uuid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>1693799933483</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[([Row(field-id=1000, name='coffee_sale_ts_mon...</td>\n",
       "      <td>(pauldefusco, merge-on-read, merge-on-read, me...</td>\n",
       "      <td>[([Row(id=1, name='coffee_id', required=False,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[([], 0)]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0d731042-eee8-4223-aa6a-404e6248d543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   current-schema-id  current-snapshot-id  default-sort-order-id  \\\n",
       "0                  0                   -1                      0   \n",
       "\n",
       "   default-spec-id  format-version  last-column-id  last-partition-id  \\\n",
       "0                0               2               3               1000   \n",
       "\n",
       "   last-sequence-number  last-updated-ms  \\\n",
       "0                     0    1693799933483   \n",
       "\n",
       "                                            location metadata-log  \\\n",
       "0  s3a://go01-demo/warehouse/tablespace/external/...           []   \n",
       "\n",
       "                                     partition-specs  \\\n",
       "0  [([Row(field-id=1000, name='coffee_sale_ts_mon...   \n",
       "\n",
       "                                          properties  \\\n",
       "0  (pauldefusco, merge-on-read, merge-on-read, me...   \n",
       "\n",
       "                                             schemas snapshot-log snapshots  \\\n",
       "0  [([Row(id=1, name='coffee_id', required=False,...           []        []   \n",
       "\n",
       "  sort-orders statistics                            table-uuid  \n",
       "0   [([], 0)]         []  0d731042-eee8-4223-aa6a-404e6248d543  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Showing \" + metadata_file_list[2])\n",
    "spark.read.option(\"multiline\",\"true\").json(\"s3a://go01-demo/\" + metadata_file_list[0]).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba3cfee-e9d4-4d30-b209-6fde71d57479",
   "metadata": {},
   "source": [
    "Showing Latest Manifest List (AVRO - prefixed by \"SNAP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f2f34aa3-7763-4f58-9c33-b08f1fab32b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing warehouse/tablespace/external/hive/lakehouse.db/coffees_table_3/metadata/snap-384109576346467287-1-92964479-485c-40b2-9b4e-e49a1f19fdc9.avro\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>manifest_path</th>\n",
       "      <th>manifest_length</th>\n",
       "      <th>partition_spec_id</th>\n",
       "      <th>content</th>\n",
       "      <th>sequence_number</th>\n",
       "      <th>min_sequence_number</th>\n",
       "      <th>added_snapshot_id</th>\n",
       "      <th>added_data_files_count</th>\n",
       "      <th>existing_data_files_count</th>\n",
       "      <th>deleted_data_files_count</th>\n",
       "      <th>added_rows_count</th>\n",
       "      <th>existing_rows_count</th>\n",
       "      <th>deleted_rows_count</th>\n",
       "      <th>partitions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>7350</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>384109576346467287</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(False, False, [127, 2, 0, 0], [130, 2, 0, 0])]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>7163</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4815467340052359539</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(False, False, [127, 2, 0, 0], [130, 2, 0, 0])]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>7186</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>384109576346467287</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(False, False, [127, 2, 0, 0], [130, 2, 0, 0])]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       manifest_path  manifest_length  \\\n",
       "0  s3a://go01-demo/warehouse/tablespace/external/...             7350   \n",
       "1  s3a://go01-demo/warehouse/tablespace/external/...             7163   \n",
       "2  s3a://go01-demo/warehouse/tablespace/external/...             7186   \n",
       "\n",
       "   partition_spec_id  content  sequence_number  min_sequence_number  \\\n",
       "0                  0        0                2                    2   \n",
       "1                  0        0                1                    1   \n",
       "2                  0        1                2                    2   \n",
       "\n",
       "     added_snapshot_id  added_data_files_count  existing_data_files_count  \\\n",
       "0   384109576346467287                       5                          0   \n",
       "1  4815467340052359539                       2                          0   \n",
       "2   384109576346467287                       2                          0   \n",
       "\n",
       "   deleted_data_files_count  added_rows_count  existing_rows_count  \\\n",
       "0                         0                 9                    0   \n",
       "1                         0                 3                    0   \n",
       "2                         0                 2                    0   \n",
       "\n",
       "   deleted_rows_count                                        partitions  \n",
       "0                   0  [(False, False, [127, 2, 0, 0], [130, 2, 0, 0])]  \n",
       "1                   0  [(False, False, [127, 2, 0, 0], [130, 2, 0, 0])]  \n",
       "2                   0  [(False, False, [127, 2, 0, 0], [130, 2, 0, 0])]  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Showing \" + metadata_file_list[6])\n",
    "spark.read.format(\"avro\").load(\"s3a://go01-demo/\" + metadata_file_list[6]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9b89c1e4-a187-43d0-a35d-eb566df31507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing warehouse/tablespace/external/hive/lakehouse.db/coffees_table_3/metadata/snap-384109576346467287-1-92964479-485c-40b2-9b4e-e49a1f19fdc9.avro\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table_3/metadata/92964479-485c-40b2-9b4e-e49a1f19fdc9-m0.avro'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Showing \" + metadata_file_list[6])\n",
    "spark.read.format(\"avro\").load(\"s3a://go01-demo/\" + metadata_file_list[6]).toPandas()['manifest_path'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5558e241-5da9-442f-b51a-360421750bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing warehouse/tablespace/external/hive/lakehouse.db/coffees_table_3/metadata/snap-384109576346467287-1-92964479-485c-40b2-9b4e-e49a1f19fdc9.avro\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table_3/metadata/ebfddaac-adcc-4d9b-91b6-6b41c31286c5-m0.avro'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Showing \" + metadata_file_list[6])\n",
    "spark.read.format(\"avro\").load(\"s3a://go01-demo/\" + metadata_file_list[6]).toPandas()['manifest_path'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "46fe5697-cb19-4b0d-aef6-6b31f86c6f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing warehouse/tablespace/external/hive/lakehouse.db/coffees_table_3/metadata/snap-384109576346467287-1-92964479-485c-40b2-9b4e-e49a1f19fdc9.avro\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table_3/metadata/92964479-485c-40b2-9b4e-e49a1f19fdc9-m1.avro'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Showing \" + metadata_file_list[6])\n",
    "spark.read.format(\"avro\").load(\"s3a://go01-demo/\" + metadata_file_list[6]).toPandas()['manifest_path'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f99f9bf9-8d6b-4a46-97bd-c869391d16ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing warehouse/tablespace/external/hive/lakehouse.db/coffees_table_3/metadata/snap-4815467340052359539-1-ebfddaac-adcc-4d9b-91b6-6b41c31286c5.avro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>manifest_path</th>\n",
       "      <th>manifest_length</th>\n",
       "      <th>partition_spec_id</th>\n",
       "      <th>content</th>\n",
       "      <th>sequence_number</th>\n",
       "      <th>min_sequence_number</th>\n",
       "      <th>added_snapshot_id</th>\n",
       "      <th>added_data_files_count</th>\n",
       "      <th>existing_data_files_count</th>\n",
       "      <th>deleted_data_files_count</th>\n",
       "      <th>added_rows_count</th>\n",
       "      <th>existing_rows_count</th>\n",
       "      <th>deleted_rows_count</th>\n",
       "      <th>partitions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>7163</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4815467340052359539</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(False, False, [127, 2, 0, 0], [130, 2, 0, 0])]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       manifest_path  manifest_length  \\\n",
       "0  s3a://go01-demo/warehouse/tablespace/external/...             7163   \n",
       "\n",
       "   partition_spec_id  content  sequence_number  min_sequence_number  \\\n",
       "0                  0        0                1                    1   \n",
       "\n",
       "     added_snapshot_id  added_data_files_count  existing_data_files_count  \\\n",
       "0  4815467340052359539                       2                          0   \n",
       "\n",
       "   deleted_data_files_count  added_rows_count  existing_rows_count  \\\n",
       "0                         0                 3                    0   \n",
       "\n",
       "   deleted_rows_count                                        partitions  \n",
       "0                   0  [(False, False, [127, 2, 0, 0], [130, 2, 0, 0])]  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Showing \" + metadata_file_list[7])\n",
    "spark.read.format(\"avro\").load(\"s3a://go01-demo/\" + metadata_file_list[7]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b0c48b1e-9f65-47b2-a219-c81e69cf80c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing warehouse/tablespace/external/hive/lakehouse.db/coffees_table_3/metadata/snap-4815467340052359539-1-ebfddaac-adcc-4d9b-91b6-6b41c31286c5.avro\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table_3/metadata/ebfddaac-adcc-4d9b-91b6-6b41c31286c5-m0.avro'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Showing \" + metadata_file_list[7])\n",
    "spark.read.format(\"avro\").load(\"s3a://go01-demo/\" + metadata_file_list[7]).toPandas()['manifest_path'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ad15d0-0d60-4d02-8ace-2d2339bac782",
   "metadata": {},
   "source": [
    "Showing Manifest Files (Avro) i.e. list of table partitions mapped to snapshot ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fec68656-4531-4dd6-99d7-b85249a491a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing warehouse/tablespace/external/hive/lakehouse.db/coffees_table_3/metadata/92964479-485c-40b2-9b4e-e49a1f19fdc9-m0.avro\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>snapshot_id</th>\n",
       "      <th>sequence_number</th>\n",
       "      <th>file_sequence_number</th>\n",
       "      <th>data_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>384109576346467287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(0, s3a://go01-demo/warehouse/tablespace/exter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>384109576346467287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(0, s3a://go01-demo/warehouse/tablespace/exter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>384109576346467287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(0, s3a://go01-demo/warehouse/tablespace/exter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>384109576346467287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(0, s3a://go01-demo/warehouse/tablespace/exter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>384109576346467287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(0, s3a://go01-demo/warehouse/tablespace/exter...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   status         snapshot_id  sequence_number  file_sequence_number  \\\n",
       "0       1  384109576346467287              NaN                   NaN   \n",
       "1       1  384109576346467287              NaN                   NaN   \n",
       "2       1  384109576346467287              NaN                   NaN   \n",
       "3       1  384109576346467287              NaN                   NaN   \n",
       "4       1  384109576346467287              NaN                   NaN   \n",
       "\n",
       "                                           data_file  \n",
       "0  (0, s3a://go01-demo/warehouse/tablespace/exter...  \n",
       "1  (0, s3a://go01-demo/warehouse/tablespace/exter...  \n",
       "2  (0, s3a://go01-demo/warehouse/tablespace/exter...  \n",
       "3  (0, s3a://go01-demo/warehouse/tablespace/exter...  \n",
       "4  (0, s3a://go01-demo/warehouse/tablespace/exter...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Showing \" + metadata_file_list[3])\n",
    "spark.read.format(\"avro\").load(\"s3a://go01-demo/\" + metadata_file_list[3]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4f3b9d9a-955c-4474-821f-974cd9decda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing warehouse/tablespace/external/hive/lakehouse.db/coffees_table_3/metadata/92964479-485c-40b2-9b4e-e49a1f19fdc9-m1.avro\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>snapshot_id</th>\n",
       "      <th>sequence_number</th>\n",
       "      <th>file_sequence_number</th>\n",
       "      <th>data_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>384109576346467287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(1, s3a://go01-demo/warehouse/tablespace/exter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>384109576346467287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(1, s3a://go01-demo/warehouse/tablespace/exter...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   status         snapshot_id  sequence_number  file_sequence_number  \\\n",
       "0       1  384109576346467287              NaN                   NaN   \n",
       "1       1  384109576346467287              NaN                   NaN   \n",
       "\n",
       "                                           data_file  \n",
       "0  (1, s3a://go01-demo/warehouse/tablespace/exter...  \n",
       "1  (1, s3a://go01-demo/warehouse/tablespace/exter...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Showing \" + metadata_file_list[4])\n",
    "spark.read.format(\"avro\").load(\"s3a://go01-demo/\" + metadata_file_list[4]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3837a142-1dc7-46fe-871d-1b3ff2ec6dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing warehouse/tablespace/external/hive/lakehouse.db/coffees_table_3/metadata/ebfddaac-adcc-4d9b-91b6-6b41c31286c5-m0.avro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>snapshot_id</th>\n",
       "      <th>sequence_number</th>\n",
       "      <th>file_sequence_number</th>\n",
       "      <th>data_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4815467340052359539</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(0, s3a://go01-demo/warehouse/tablespace/exter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4815467340052359539</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(0, s3a://go01-demo/warehouse/tablespace/exter...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   status          snapshot_id  sequence_number  file_sequence_number  \\\n",
       "0       1  4815467340052359539              NaN                   NaN   \n",
       "1       1  4815467340052359539              NaN                   NaN   \n",
       "\n",
       "                                           data_file  \n",
       "0  (0, s3a://go01-demo/warehouse/tablespace/exter...  \n",
       "1  (0, s3a://go01-demo/warehouse/tablespace/exter...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Showing \" + metadata_file_list[5])\n",
    "spark.read.format(\"avro\").load(\"s3a://go01-demo/\" + metadata_file_list[5]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "43f94648-66db-48d9-8f96-62447cea9054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing warehouse/tablespace/external/hive/lakehouse.db/coffees_table_3/metadata/92964479-485c-40b2-9b4e-e49a1f19fdc9-m0.avro\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(content=0, file_path='s3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table_3/data/coffee_sale_ts_month=2023-07/00011-839-9cc8e7da-16c5-46fa-8612-37d795596d15-00001.parquet', file_format='PARQUET', partition=Row(coffee_sale_ts_month=642), record_count=3, file_size_in_bytes=1074, column_sizes=[Row(key=1, value=52), Row(key=2, value=74), Row(key=3, value=62)], value_counts=[Row(key=1, value=3), Row(key=2, value=3), Row(key=3, value=3)], null_value_counts=[Row(key=1, value=0), Row(key=2, value=0), Row(key=3, value=0)], nan_value_counts=[], lower_bounds=[Row(key=1, value=bytearray(b'\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x00')), Row(key=2, value=bytearray(b'grande')), Row(key=3, value=bytearray(b'\\x00W\\xd3\\xafk\\xff\\x05\\x00'))], upper_bounds=[Row(key=1, value=bytearray(b'\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\x00')), Row(key=2, value=bytearray(b'venti')), Row(key=3, value=bytearray(b'\\x00W\\xd3\\xafk\\xff\\x05\\x00'))], key_metadata=None, split_offsets=[4], equality_ids=None, sort_order_id=0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Showing \" + metadata_file_list[3])\n",
    "spark.read.format(\"avro\").load(\"s3a://go01-demo/\" + metadata_file_list[3]).toPandas()['data_file'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a22e8f49-3312-4651-bbb5-d506598fe480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 51:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------------+\n",
      "|coffee_id|coffee_size|     coffee_sale_ts|\n",
      "+---------+-----------+-------------------+\n",
      "|        4|      venti|2023-07-01 12:01:00|\n",
      "|        5|     grande|2023-07-01 12:01:00|\n",
      "|        6|     grande|2023-07-01 12:01:00|\n",
      "+---------+-----------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_file_path = spark.read.format(\"avro\").load(\"s3a://go01-demo/\" + metadata_file_list[3]).toPandas()['data_file'][0][1]\n",
    "spark.read.parquet(data_file_path).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9c011c92-a731-4aae-8129-58c94527b01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 54:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------------+\n",
      "|coffee_id|coffee_size|     coffee_sale_ts|\n",
      "+---------+-----------+-------------------+\n",
      "|        2|       tall|2023-07-01 10:00:00|\n",
      "+---------+-----------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_file_path = spark.read.format(\"avro\").load(\"s3a://go01-demo/\" + metadata_file_list[3]).toPandas()['data_file'][1][1]\n",
    "spark.read.parquet(data_file_path).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2f879a9c-c1ab-4850-9d53-1a7b848e64f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------------+\n",
      "|coffee_id|coffee_size|     coffee_sale_ts|\n",
      "+---------+-----------+-------------------+\n",
      "|        7|      venti|2023-05-01 12:01:00|\n",
      "|        9|       tall|2023-05-01 12:01:00|\n",
      "|       10|       tall|2023-05-01 12:01:00|\n",
      "+---------+-----------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 04:31:07 WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed (this is expected if the application is shutting down.)\n",
      "io.fabric8.kubernetes.client.WatcherException: too old resource version: 13837705 (13839217)\n",
      "\tat io.fabric8.kubernetes.client.dsl.internal.AbstractWatchManager.onStatus(AbstractWatchManager.java:265)\n",
      "\tat io.fabric8.kubernetes.client.dsl.internal.AbstractWatchManager.onMessage(AbstractWatchManager.java:249)\n",
      "\tat io.fabric8.kubernetes.client.dsl.internal.WatcherWebSocketListener.onMessage(WatcherWebSocketListener.java:93)\n",
      "\tat okhttp3.internal.ws.RealWebSocket.onReadMessage(RealWebSocket.java:323)\n",
      "\tat okhttp3.internal.ws.WebSocketReader.readMessageFrame(WebSocketReader.java:219)\n",
      "\tat okhttp3.internal.ws.WebSocketReader.processNextFrame(WebSocketReader.java:105)\n",
      "\tat okhttp3.internal.ws.RealWebSocket.loopReader(RealWebSocket.java:274)\n",
      "\tat okhttp3.internal.ws.RealWebSocket$2.onResponse(RealWebSocket.java:214)\n",
      "\tat okhttp3.RealCall$AsyncCall.execute(RealCall.java:203)\n",
      "\tat okhttp3.internal.NamedRunnable.run(NamedRunnable.java:32)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: io.fabric8.kubernetes.client.KubernetesClientException: too old resource version: 13837705 (13839217)\n",
      "\t... 13 more\n"
     ]
    }
   ],
   "source": [
    "data_file_path = spark.read.format(\"avro\").load(\"s3a://go01-demo/\" + metadata_file_list[3]).toPandas()['data_file'][2][1]\n",
    "spark.read.parquet(data_file_path).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1cfe5925-f8cd-41fa-8fd3-65b5456e7810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing warehouse/tablespace/external/hive/lakehouse.db/coffees_table_3/metadata/92964479-485c-40b2-9b4e-e49a1f19fdc9-m1.avro\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(content=1, file_path='s3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table_3/data/coffee_sale_ts_month=2023-07/00036-1035-022f4aa1-8d4c-4ee9-812a-edb7044d29f6-00001.parquet', file_format='PARQUET', partition=Row(coffee_sale_ts_month=642), record_count=1, file_size_in_bytes=1794, column_sizes=[Row(key=2147483546, value=199), Row(key=2147483545, value=33)], value_counts=[Row(key=2147483546, value=1), Row(key=2147483545, value=1)], null_value_counts=[Row(key=2147483546, value=0), Row(key=2147483545, value=0)], nan_value_counts=[], lower_bounds=[Row(key=2147483546, value=bytearray(b's3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table_3/data/coffee_sale_ts_month=2023-07/00011-214-64d52d0e-b039-4681-89d9-6f15e487760b-00001.parquet')), Row(key=2147483545, value=bytearray(b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'))], upper_bounds=[Row(key=2147483546, value=bytearray(b's3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table_3/data/coffee_sale_ts_month=2023-07/00011-214-64d52d0e-b039-4681-89d9-6f15e487760b-00001.parquet')), Row(key=2147483545, value=bytearray(b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'))], key_metadata=None, split_offsets=None, equality_ids=None, sort_order_id=None)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Showing \" + metadata_file_list[4])\n",
    "spark.read.format(\"avro\").load(\"s3a://go01-demo/\" + metadata_file_list[4]).toPandas()['data_file'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a3aeb17d-8d67-49e1-9c6e-8fd857f5d7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|           file_path|pos|\n",
      "+--------------------+---+\n",
      "|s3a://go01-demo/w...|  0|\n",
      "+--------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_file_path = spark.read.format(\"avro\").load(\"s3a://go01-demo/\" + metadata_file_list[4]).toPandas()['data_file'][0][1]\n",
    "spark.read.parquet(data_file_path).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "66e7aa2c-40b0-4da3-be0f-ef0382b8e25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------------+\n",
      "|coffee_id|coffee_size|     coffee_sale_ts|\n",
      "+---------+-----------+-------------------+\n",
      "|        2|     grande|2023-07-01 10:00:00|\n",
      "|        1|      venti|2023-07-01 10:00:00|\n",
      "+---------+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_file_path = spark.read.parquet(data_file_path).toPandas()['file_path'][0]\n",
    "spark.read.parquet(data_file_path).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "aed80d8c-91c7-4262-aff2-122ae7e45acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|           file_path|pos|\n",
      "+--------------------+---+\n",
      "|s3a://go01-demo/w...|  0|\n",
      "+--------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_file_path = spark.read.format(\"avro\").load(\"s3a://go01-demo/\" + metadata_file_list[4]).toPandas()['data_file'][1][1]\n",
    "spark.read.parquet(data_file_path).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5263db86-b348-4271-839f-d59509e87f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 80:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------------+\n",
      "|coffee_id|coffee_size|     coffee_sale_ts|\n",
      "+---------+-----------+-------------------+\n",
      "|        3|       tall|2023-04-01 10:00:00|\n",
      "+---------+-----------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_file_path = spark.read.parquet(data_file_path).toPandas()['file_path'][0]\n",
    "spark.read.parquet(data_file_path).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a2c467e6-e90d-4a39-bfb3-ee2a84cb95fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing warehouse/tablespace/external/hive/lakehouse.db/coffees_table_3/metadata/ebfddaac-adcc-4d9b-91b6-6b41c31286c5-m0.avro\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(content=0, file_path='s3a://go01-demo/warehouse/tablespace/external/hive/lakehouse.db/coffees_table_3/data/coffee_sale_ts_month=2023-07/00011-214-64d52d0e-b039-4681-89d9-6f15e487760b-00001.parquet', file_format='PARQUET', partition=Row(coffee_sale_ts_month=642), record_count=2, file_size_in_bytes=1015, column_sizes=[Row(key=1, value=41), Row(key=2, value=44), Row(key=3, value=62)], value_counts=[Row(key=1, value=2), Row(key=2, value=2), Row(key=3, value=2)], null_value_counts=[Row(key=1, value=0), Row(key=2, value=0), Row(key=3, value=0)], nan_value_counts=[], lower_bounds=[Row(key=1, value=bytearray(b'\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00')), Row(key=2, value=bytearray(b'grande')), Row(key=3, value=bytearray(b'\\x00\\x88\\x18\\xffi\\xff\\x05\\x00'))], upper_bounds=[Row(key=1, value=bytearray(b'\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00')), Row(key=2, value=bytearray(b'venti')), Row(key=3, value=bytearray(b'\\x00\\x88\\x18\\xffi\\xff\\x05\\x00'))], key_metadata=None, split_offsets=[4], equality_ids=None, sort_order_id=0)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Showing \" + metadata_file_list[5])\n",
    "spark.read.format(\"avro\").load(\"s3a://go01-demo/\" + metadata_file_list[5]).toPandas()['data_file'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "10fc5879-cad0-4a1d-ba3a-2992cd587f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------------+\n",
      "|coffee_id|coffee_size|     coffee_sale_ts|\n",
      "+---------+-----------+-------------------+\n",
      "|        2|     grande|2023-07-01 10:00:00|\n",
      "|        1|      venti|2023-07-01 10:00:00|\n",
      "+---------+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_file_path = spark.read.format(\"avro\").load(\"s3a://go01-demo/\" + metadata_file_list[5]).toPandas()['data_file'][0][1]\n",
    "spark.read.parquet(data_file_path).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5304441e-2cf3-4442-801e-150e70e15052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------------+\n",
      "|coffee_id|coffee_size|     coffee_sale_ts|\n",
      "+---------+-----------+-------------------+\n",
      "|        3|       tall|2023-04-01 10:00:00|\n",
      "+---------+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_file_path = spark.read.format(\"avro\").load(\"s3a://go01-demo/\" + metadata_file_list[5]).toPandas()['data_file'][1][1]\n",
    "spark.read.parquet(data_file_path).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fd5e0099-f200-48d1-b4ce-b447c1de1be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/04 04:09:09 WARN HiveMetaStoreClient: Failed to connect to the MetaStore Server...\n",
      "[Stage 91:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------------+\n",
      "|coffee_id|coffee_size|     coffee_sale_ts|\n",
      "+---------+-----------+-------------------+\n",
      "|        4|      venti|2023-07-01 12:01:00|\n",
      "|        5|     grande|2023-07-01 12:01:00|\n",
      "|        6|     grande|2023-07-01 12:01:00|\n",
      "|        2|       tall|2023-07-01 10:00:00|\n",
      "|        7|      venti|2023-05-01 12:01:00|\n",
      "|        9|       tall|2023-05-01 12:01:00|\n",
      "|       10|       tall|2023-05-01 12:01:00|\n",
      "|        8|     grande|2023-04-01 12:01:00|\n",
      "|        3|      venti|2023-04-01 10:00:00|\n",
      "|        1|      venti|2023-07-01 10:00:00|\n",
      "+---------+-----------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM lakehouse.coffees_table_3\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62108a25-7170-4760-88a9-bbe2a80581b3",
   "metadata": {},
   "source": [
    "### Time Travel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c566c5c6-8b5d-4aba-9c20-409233c4827b",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshots_df = spark.sql(\"SELECT * FROM lakehouse.customer_table.snapshots;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0852473c-3639-43c1-a446-8920be731f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_snapshot = snapshots_df.select(\"snapshot_id\").head(1)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb18fe8c-fb4f-4669-a52e-fbe024979e4b",
   "metadata": {},
   "source": [
    "#### Validate that the output dataframe only includes one row per the original insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0cd767-66f2-4393-8a2f-0d1d89e6a973",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read\\\n",
    "    .option(\"snapshot-id\", first_snapshot)\\\n",
    "    .format(\"iceberg\")\\\n",
    "    .load(\"lakehouse.customer_table\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbc2468-98ab-4e80-a7cd-717cb515b045",
   "metadata": {},
   "outputs": [],
   "source": [
    "avro_tempdf = spark.read.format(\"avro\").load(\"s3a://go01-demo/\" + metadata_file_list[6]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e45b8d9-0368-4f66-80e6-2e5aa2766144",
   "metadata": {},
   "outputs": [],
   "source": [
    "avro_tempdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dda7fb9-9e96-4a4d-89fc-e6f0c1197213",
   "metadata": {},
   "outputs": [],
   "source": [
    "avro_tempdf['partitions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7990d583-eedb-4959-a90c-1151920ab963",
   "metadata": {},
   "outputs": [],
   "source": [
    "avro_tempdf['added_rows_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d808a437-8439-4b0a-afeb-33d5817f0636",
   "metadata": {},
   "outputs": [],
   "source": [
    "avro_tempdf['existing_rows_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f5a019-bbf7-431a-96e3-984e79a249a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "avro_tempdf['added_data_files_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2ef22c-733c-42b7-a022-2955407ec2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Showing \" + metadata_file_list[2])\n",
    "json_tempdf = spark.read.option(\"multiline\",\"true\").json(\"s3a://go01-demo/\" + metadata_file_list[2]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb05bf2-2a10-4964-a75c-f2c732d09682",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_tempdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44b9003-ec1e-4a66-a7f1-aa90e2fcf8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_tempdf['current-schema-id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd86e88-aec0-4602-b8db-8f96cb053a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(json_tempdf['snapshots'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340197e4-5203-483d-9700-a3fd982ceedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_tempdf['partition-spec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2001b532-b800-4377-bfd9-e7fe5799d9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(json_tempdf['partition-specs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156c215d-ba62-4046-82f8-6a8e21a96001",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM lakehouse.coffees_table_2.all_data_files;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3a962b-9da5-4256-b48e-b5cd17690323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c23bd6a-9e1a-4909-9f09-7caeb5524647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca0e554-abdf-4fbd-9a56-840c7602a57f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b08cb0-0cf9-4a87-ab17-5b4286023c05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d86f6c6-682c-45d6-a26e-54c93430a0d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a00307f-365a-492a-a710-692510544b69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fadb12-a5fc-4138-89e8-455459b7cab9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe197cc5-122b-4fa0-ba41-6d3889a466b3",
   "metadata": {},
   "source": [
    "### Partition Evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63961d6-9c2a-451d-b772-adc382e1c0da",
   "metadata": {},
   "source": [
    "Spark partitioning is a way to split the data into multiple partitions so that you can execute transformations on multiple partitions in parallel which allows completing the job faster. You can also write partitioned data into a file system (multiple sub-directories) for faster reads by downstream systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0463d053-187c-4f32-8bbe-53c4ece328dc",
   "metadata": {},
   "source": [
    "Spark has several partitioning methods to achieve parallelism, based on your need, you should choose which one to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb47968-03c8-400b-93c4-2101ac1f14cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "738a10ca-a464-40f6-baa6-2aafdfb2d2bb",
   "metadata": {},
   "source": [
    "Creating New Data to Test Partition Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b096c653-df75-4ba4-bf13-5bcbfacd9f43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3851f4-d681-44cc-965c-835667ffd4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType, IntegerType, StringType\n",
    "\n",
    "import dbldatagen as dg\n",
    "\n",
    "shuffle_partitions_requested = 20\n",
    "device_population = 100000\n",
    "data_rows = 20 * 1000000\n",
    "#partitions_requested = 20\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", shuffle_partitions_requested)\n",
    "\n",
    "country_codes = [\n",
    "    \"CN\", \"US\", \"FR\", \"CA\", \"IN\", \"JM\", \"IE\", \"PK\", \"GB\", \"IL\", \"AU\", \n",
    "    \"SG\", \"ES\", \"GE\", \"MX\", \"ET\", \"SA\", \"LB\", \"NL\", \"IT\"\n",
    "]\n",
    "#country_weights = [\n",
    "#    1300, 365, 67, 38, 1300, 3, 7, 212, 67, 9, 25, 6, 47, 83, \n",
    "#    126, 109, 58, 8, 17,\n",
    "#]\n",
    "\n",
    "manufacturers = [\n",
    "    \"Delta corp\", \"Xyzzy Inc.\", \"Lakehouse Ltd\", \"Acme Corp\", \"Embanks Devices\",\n",
    "]\n",
    "\n",
    "lines = [\"delta\", \"xyzzy\", \"lakehouse\", \"gadget\", \"droid\"]\n",
    "\n",
    "testDataSpec = (\n",
    "    dg.DataGenerator(spark, name=\"device_data_set\", rows=data_rows) \n",
    "                     #,partitions=partitions_requested)\n",
    "    .withIdOutput()\n",
    "    # we'll use hash of the base field to generate the ids to\n",
    "    # avoid a simple incrementing sequence\n",
    "    .withColumn(\"internal_device_id\", \"long\", minValue=0x1000000000000, \n",
    "                uniqueValues=device_population, omit=True, baseColumnType=\"hash\",\n",
    "    )\n",
    "    # note for format strings, we must use \"%lx\" not \"%x\" as the\n",
    "    # underlying value is a long\n",
    "    .withColumn(\n",
    "        \"device_id\", \"string\", format=\"0x%013x\", baseColumn=\"internal_device_id\"\n",
    "    )\n",
    "    # the device / user attributes will be the same for the same device id\n",
    "    # so lets use the internal device id as the base column for these attribute\n",
    "    .withColumn(\"country\", \"string\", values=country_codes, #weights=country_weights, \n",
    "                baseColumn=\"internal_device_id\")\n",
    "    .withColumn(\"manufacturer\", \"string\", values=manufacturers, \n",
    "                baseColumn=\"internal_device_id\", )\n",
    "    # use omit = True if you don't want a column to appear in the final output\n",
    "    # but just want to use it as part of generation of another column\n",
    "    .withColumn(\"line\", \"string\", values=lines, baseColumn=\"manufacturer\", \n",
    "                baseColumnType=\"hash\", omit=True )\n",
    "    .withColumn(\"model_ser\", \"integer\", minValue=1, maxValue=11, baseColumn=\"device_id\", \n",
    "                baseColumnType=\"hash\", omit=True, )\n",
    "    .withColumn(\"model_line\", \"string\", expr=\"concat(line, '#', model_ser)\", \n",
    "                baseColumn=[\"line\", \"model_ser\"] )\n",
    "    .withColumn(\"event_type\", \"string\", \n",
    "                values=[\"activation\", \"deactivation\", \"plan change\", \"telecoms activity\", \n",
    "                        \"internet activity\", \"device error\", ],\n",
    "                random=True)\n",
    "    .withColumn(\"event_ts\", \"timestamp\", begin=\"2020-01-01 01:00:00\", \n",
    "                end=\"2020-12-31 23:59:00\", \n",
    "                interval=\"1 minute\", random=True )\n",
    ")\n",
    "\n",
    "dfTestData = testDataSpec.build()\n",
    "\n",
    "display(dfTestData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a41d6d-fd68-4fa0-914d-a84bbf298fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTestData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbedb46-48c7-472f-a1db-2e2df8ce4adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.sources.partitionColumnTypeInference.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842ee72a-ec1a-4ed4-8aba-3a2a4baf5863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cb0635-e535-4758-919f-d4f04bacf0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS spark_catalog.lakehouse.partition_evol_tbl PURGE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559589db-5dd5-4e49-ba44-b8b8143d12b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfTestData.groupBy(\"country\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4ed5eb-90ae-4960-b9f6-83a3df408409",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfTestData.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0594e7-4e60-45cf-a7e9-ac26c9478e87",
   "metadata": {},
   "source": [
    "Iceberg requires the data to be sorted according to the partition spec per task (Spark partition) in prior to write against partitioned table. This applies both Writing with SQL and Writing with DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50873884-250f-4872-8df8-befeadf6b797",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTestData.sortWithinPartitions(\"country\").writeTo(\"spark_catalog.lakehouse.p_evol_tbl\").partitionedBy(\"country\").using(\"iceberg\").create()#.append()#replace()#overwritePartitions()#create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85761304-fcbb-405c-aab8-221c6fad602b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"SELECT * FROM spark_catalog.lakehouse.part_evol_tbl.PARTITIONS\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04502b8-874e-49ec-b3c3-aac117ce29ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"SELECT * FROM spark_catalog.lakehouse.part_evol_tbl.files\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f76c097-d1c1-4339-af78-c97c245ea00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"SELECT * FROM spark_catalog.lakehouse.part_evol_tbl.manifests\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e6a8be-01f8-4be9-beb1-f953c0ad7e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"SELECT * FROM spark_catalog.lakehouse.part_evol_tbl.all_manifests\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f2bf66-d219-4e65-a58e-260bf891e2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"SELECT * FROM spark_catalog.lakehouse.part_evol_tbl.all_data_files\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88b8694-61bb-4507-af92-7b288b50e4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"SELECT * FROM spark_catalog.lakehouse.part_evol_tbl.snapshots\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684e0727-c58d-4f37-a682-7748f566e205",
   "metadata": {},
   "source": [
    "Adding a partition field is a metadata operation and does not change any of the existing table data. New data will be written with the new partitioning, but existing data will remain in the old partition layout. Old data files will have null values for the new partition fields in metadata tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2c9fe1-7594-4909-8744-59dde048eb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TABLE PARTITIONS BEFORE ALTER PARTITION STATEMENT: \")\n",
    "spark.sql(\"SELECT * FROM spark_catalog.lakehouse.p_evol_tbl.PARTITIONS\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dd4330-e06c-46fa-87b4-3624555daa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ADD PARTITION BY EVENT TIMESTAMP MONTHS: \")\n",
    "print(\"ALTER TABLE spark_catalog.lakehouse.p_evol_tbl ADD PARTITION FIELD months(event_ts)\")\n",
    "spark.sql(\"ALTER TABLE spark_catalog.lakehouse.p_evol_tbl ADD PARTITION FIELD months(event_ts)\")\n",
    "#spark.sql(\"ALTER TABLE spark_catalog.lakehouse.part_evol_tbl REPLACE PARTITION FIELD hours(dob) WITH state\")\n",
    "#spark.sql(\"ALTER TABLE prod.db.sample ADD PARTITION FIELD month\")\n",
    "\n",
    "#ALTER TABLE spark_catalog.lakehouse.part_evol_tbl ADD PARTITION FIELD days(event_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee150f5-9d59-4b23-8df3-e6f5d70e2fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TABLE PARTITIONS AFTER ALTER PARTITION STATEMENT: \")\n",
    "spark.sql(\"SELECT * FROM spark_catalog.lakehouse.p_evol_tbl.PARTITIONS\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa64157-5a82-4c6d-82f8-f94bd08e8601",
   "metadata": {},
   "outputs": [],
   "source": [
    "appendDf = dfTestData.sample(fraction=0.3, seed=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494691b4-e56a-41d5-98e3-0f033f26d59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "appendDf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f2ca29-d54b-4aba-a9f8-ad1019616b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "appendDf.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33af64b-c2ca-447f-a0b7-185e223d973c",
   "metadata": {},
   "outputs": [],
   "source": [
    "appendDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea0b278-5cb4-4bc3-aff9-f988b338c9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "appendDf.sortWithinPartitions(\"country\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf4dcea-b8d5-43a9-ae67-b3aebf07d938",
   "metadata": {},
   "outputs": [],
   "source": [
    "#appendDf.sortWithinPartitions(\"country\", \"month(event_ts)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e1c1ea-6949-49e1-a6ea-5022033abc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "appendDf.sortWithinPartitions(\"country\").writeTo(\"spark_catalog.lakehouse.p_evol_tbl\").using(\"iceberg\").append() #.append()#replace()#overwritePartitions()#create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef167cb7-a230-4d45-8619-87090d850789",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TABLE PARTITIONS AFTER APPEND: \")\n",
    "spark.sql(\"SELECT * FROM spark_catalog.lakehouse.p_evol_tbl.PARTITIONS\").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ef51fc-950b-4a96-85c3-6cdb7c472e37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5406060-fea0-4002-a915-be5bd29aa08b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b3860d-d5b9-47f5-b494-0ffe5f0d190e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a009cb8e-e564-40aa-bebf-e1229b4c5341",
   "metadata": {},
   "source": [
    "Dropping a partition field is a metadata operation and does not change any of the existing table data. New data will be written with the new partitioning, but existing data will remain in the old partition layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7e0fb9-374e-4965-93da-776e91c2b787",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"ALTER TABLE spark_catalog.lakehouse.part_evol_tbl DROP PARTITION FIELD bucket(16, device_id)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5340bace-0227-48b8-b984-d0136381f672",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TABLE PARTITIONS AFTER ALTER PARTITION STATEMENT: \")\n",
    "spark.sql(\"SELECT * FROM spark_catalog.lakehouse.part_evol_tbl.PARTITIONS\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068a2029-59ec-44dd-bbe0-1ba04777548e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8512c690-0009-47a8-8046-7477ed7b99b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144ebaf9-0d8c-4ff0-90fb-516b9b2f97a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e57e5ac-497b-4692-afb3-d28f4d54d5fb",
   "metadata": {},
   "source": [
    "##### Only json files have been added (one per each time you repartitioned) but Avro files have stayed the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489132be-d671-46a6-92c5-6dc291638f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket(\"go01-demo\")\n",
    "\n",
    "metadata_file_list = []\n",
    "\n",
    "print(\"Current Metadata Files: \\n\")\n",
    "\n",
    "for object_summary in my_bucket.objects.filter(Prefix=metadata_path+\"/metadata\"):\n",
    "    #print(object_summary.key +\"\\n\")\n",
    "    metadata_file_list.append(object_summary.key)\n",
    "    \n",
    "metadata_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6af64da-19fc-47b3-afd8-5a938704ee11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1b5aed-4bba-482d-ac22-8e0f30cb970a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abd06d2-fb84-4a96-ab3d-9c9697cd85ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b998fc45-091b-4776-81d0-402aeb8673e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162a1ede-72a3-40bf-9cbf-1ae4a410832f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc464b86-ebea-41a7-9afe-6917f262b39c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd33dd11-1439-408e-912b-dc65d7f12fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE TABLE IF NOT EXISTS customer_table (id BIGINT, state STRING, country STRING, dob TIMESTAMP) USING iceberg PARTITIONED BY ( hours(dob))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7930a494-94d8-4fee-b873-8120257fbbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT HOUR(dob) FROM spark_catalog.lakehouse.customer_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485b48f5-4eaf-4edf-ad66-2a30a19b6c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT DAY(dob) FROM spark_catalog.lakehouse.customer_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178a933e-5ee3-4a64-93e0-85fac5dc544c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0086fbd-2b28-4df6-baf9-17768c5a7b9b",
   "metadata": {},
   "source": [
    "### Dropping Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abab5842-1722-470e-8066-c58e7659192e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS lakehouse.staging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f903baae-7c19-45a9-95d3-79d929491ed2",
   "metadata": {},
   "source": [
    "Validate that the metadata folder is now empty but the data folder still retains parquet files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fef6a1-ca58-48d2-83c2-e4e66dc2e3b1",
   "metadata": {},
   "source": [
    "![alt text](../img/s3_droptable_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eba9f0-a03c-4c17-bb4f-d1b9e6404a97",
   "metadata": {},
   "source": [
    "![alt text](../img/s3_droptable_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de2b0a5-8b24-4055-8250-d2ce689949c3",
   "metadata": {},
   "source": [
    "![alt text](../img/s3_droptable_3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43739d1f-8682-4575-98de-58f4ca4416f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"ALTER TABLE lakehouse.customers_table\\\n",
    "            SET TBLPROPERTIES ('format-version' = '2')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38b4f0c-f054-4721-9789-5e3968be2985",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket(\"go01-demo\")\n",
    "\n",
    "metadata_file_list = []\n",
    "\n",
    "for object_summary in my_bucket.objects.filter(Prefix=metadata_path):\n",
    "    print(object_summary.key +\"\\n\")\n",
    "    metadata_file_list.append(object_summary.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ba146b-fdfb-41e3-bc75-65ce5c4c5bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Showing \" + metadata_file_list[3])\n",
    "spark.read.option(\"multiline\",\"true\").json(\"s3a://go01-demo/\" + metadata_file_list[3]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdce88a-df1c-4e93-960e-fcf732bb9df0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
