{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "433fad8f-2310-4eed-a8e2-f615979d7dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/23 22:21:55 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "23/09/23 22:21:55 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "Setting spark.hadoop.yarn.resourcemanager.principal to pauldefusco\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/23 22:21:56 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "23/09/23 22:21:56 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "23/09/23 22:21:56 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "23/09/23 22:21:57 WARN Utils: Service 'SparkUI' could not bind on port 20049. Attempting port 20050.\n",
      "23/09/23 22:21:58 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "23/09/23 22:22:00 WARN HiveServer2CredentialProvider: Failed to get HS2 delegation token\n",
      "java.util.NoSuchElementException: spark.sql.hive.hiveserver2.jdbc.url\n",
      "\tat org.apache.spark.SparkConf.$anonfun$get$1(SparkConf.scala:245)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.SparkConf.get(SparkConf.scala:245)\n",
      "\tat com.hortonworks.spark.deploy.yarn.security.HiveServer2CredentialProvider.obtainDelegationTokens(HiveServer2CredentialProvider.scala:64)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager.$anonfun$obtainDelegationTokens$2(HadoopDelegationTokenManager.scala:164)\n",
      "\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat scala.collection.MapLike$DefaultValuesIterable.foreach(MapLike.scala:213)\n",
      "\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)\n",
      "\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)\n",
      "\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager.org$apache$spark$deploy$security$HadoopDelegationTokenManager$$obtainDelegationTokens(HadoopDelegationTokenManager.scala:162)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager$$anon$4.run(HadoopDelegationTokenManager.scala:226)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager$$anon$4.run(HadoopDelegationTokenManager.scala:224)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager.obtainTokensAndScheduleRenewal(HadoopDelegationTokenManager.scala:224)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager.org$apache$spark$deploy$security$HadoopDelegationTokenManager$$updateTokensTask(HadoopDelegationTokenManager.scala:198)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager.start(HadoopDelegationTokenManager.scala:123)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.$anonfun$start$1(CoarseGrainedSchedulerBackend.scala:552)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.$anonfun$start$1$adapted(CoarseGrainedSchedulerBackend.scala:549)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.start(CoarseGrainedSchedulerBackend.scala:549)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.KubernetesClusterSchedulerBackend.start(KubernetesClusterSchedulerBackend.scala:95)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:220)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:581)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/23 22:22:01 WARN ExecutorAllocationManager: Dynamic allocation without a shuffle service is an experimental feature.\n",
      "23/09/23 22:22:04 WARN HiveClientImpl: Detected HiveConf hive.execution.engine is 'tez' and will be reset to 'mr' to disable useless hive logic\n",
      "Hive Session ID = 80a382ea-3649-4deb-b122-f5e1d38c55de\n",
      "23/09/23 22:22:06 WARN HiveMetaStoreClient: Failed to connect to the MetaStore Server...\n",
      "23/09/23 22:22:06 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|         01_car_data|\n",
      "|           01_car_dw|\n",
      "|              adb101|\n",
      "|            airlines|\n",
      "|        airlines_csv|\n",
      "|    airlines_iceberg|\n",
      "|airlines_iceberg_...|\n",
      "|      airlines_mjain|\n",
      "|          airquality|\n",
      "|                ajvp|\n",
      "|          atlas_demo|\n",
      "|            bankdemo|\n",
      "|          bca_jps_l0|\n",
      "|        cde_workshop|\n",
      "|cde_workshop_pdef...|\n",
      "|             cdedemo|\n",
      "|        cdp_overview|\n",
      "|      ceht_open_data|\n",
      "|        ceht_scratch|\n",
      "| ceht_transportation|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cml.data_v1 as cmldata\n",
    "\n",
    "CONNECTION_NAME = \"go01-aw-dl\"\n",
    "conn = cmldata.get_connection(CONNECTION_NAME)\n",
    "spark = conn.get_spark_session()\n",
    "\n",
    "# Sample usage to run query through spark\n",
    "EXAMPLE_SQL_QUERY = \"show databases\"\n",
    "spark.sql(EXAMPLE_SQL_QUERY).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d69196d6-30dc-4542-a02a-556c608af2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/23 22:22:06 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "23/09/23 22:22:06 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('spark.eventLog.enabled', 'true'),\n",
       " ('spark.network.crypto.enabled', 'true'),\n",
       " ('spark.sql.hive.hwc.execution.mode', 'spark'),\n",
       " ('spark.jars',\n",
       "  '/opt/spark/optional-lib/hive-warehouse-connector-assembly.jar,/opt/spark/optional-lib/iceberg-hive-runtime.jar,/opt/spark/optional-lib/iceberg-spark-runtime.jar'),\n",
       " ('spark.kerberos.renewal.credentials', 'ccache'),\n",
       " ('spark.sql.catalog.spark_catalog',\n",
       "  'org.apache.iceberg.spark.SparkSessionCatalog'),\n",
       " ('spark.dynamicAllocation.maxExecutors', '49'),\n",
       " ('spark.eventLog.dir', 'file:///sparkeventlogs'),\n",
       " ('spark.hadoop.yarn.resourcemanager.principal', 'pauldefusco'),\n",
       " ('spark.kubernetes.driver.annotation.cluster-autoscaler.kubernetes.io/safe-to-evict',\n",
       "  'false'),\n",
       " ('spark.ui.port', '20049'),\n",
       " ('spark.kubernetes.driver.pod.name', 'a8r5dpdadtfatd8k'),\n",
       " ('spark.driver.bindAddress', '100.100.175.202'),\n",
       " ('spark.yarn.access.hadoopFileSystems',\n",
       "  's3a://go01-demo/warehouse/tablespace/external/hive'),\n",
       " ('spark.sql.extensions',\n",
       "  'com.qubole.spark.hiveacid.HiveAcidAutoConvertExtension,org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions'),\n",
       " ('spark.kubernetes.executor.annotation.cluster-autoscaler.kubernetes.io/safe-to-evict',\n",
       "  'false'),\n",
       " ('spark.driver.memory', '3051m'),\n",
       " ('spark.io.encryption.enabled', 'true'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.master', 'k8s://https://172.20.0.1:443'),\n",
       " ('spark.ui.allowFramingFrom',\n",
       "  'https://ml-b74f8940-b97.go01-dem.ylcu-atmi.cloudera.site'),\n",
       " ('spark.driver.port', '38081'),\n",
       " ('spark.kubernetes.executor.podTemplateFile', '/tmp/spark-executor.json'),\n",
       " ('spark.kubernetes.container.image',\n",
       "  'docker.repository.cloudera.com/cloudera/cdsw/ml-runtime-jupyterlab-python3.7-standard:2022.11.1-b2'),\n",
       " ('spark.dynamicAllocation.shuffleTracking.enabled', 'true'),\n",
       " ('spark.kubernetes.namespace', 'mlx-user-16'),\n",
       " ('spark.ui.proxyRedirectUri',\n",
       "  'https://spark-a8r5dpdadtfatd8k.ml-b74f8940-b97.go01-dem.ylcu-atmi.cloudera.site'),\n",
       " ('spark.kubernetes.executor.config.dir', '/var/spark/conf'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  's3a://go01-demo/warehouse/tablespace/external/hive'),\n",
       " ('spark.hadoop.iceberg.engine.hive.enabled', 'true'),\n",
       " ('spark.driver.host', '100.100.175.202'),\n",
       " ('spark.app.startTime', '1695507716493'),\n",
       " ('spark.repl.local.jars',\n",
       "  'file:///runtime-addons/spark323-19-hf2-g3aapf/opt/spark/optional-lib/hive-warehouse-connector-assembly-1.0.0.1.19.7215.0-118.jar,file:///runtime-addons/spark323-19-hf2-g3aapf/opt/spark/optional-lib/iceberg-hive-runtime-1.1.0.1.19.7215.0-118.jar,file:///runtime-addons/spark323-19-hf2-g3aapf/opt/spark/optional-lib/iceberg-spark-runtime-3.2_2.12-1.1.0.1.19.7215.0-118.jar'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.initial.jar.urls',\n",
       "  'spark://100.100.175.202:38081/jars/iceberg-hive-runtime-1.1.0.1.19.7215.0-118.jar,spark://100.100.175.202:38081/jars/iceberg-spark-runtime-3.2_2.12-1.1.0.1.19.7215.0-118.jar,spark://100.100.175.202:38081/jars/hive-warehouse-connector-assembly-1.0.0.1.19.7215.0-118.jar'),\n",
       " ('spark.yarn.rmProxy.enabled', 'false'),\n",
       " ('spark.app.id', 'spark-application-1695507718414'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.kryo.registrator',\n",
       "  'com.qubole.spark.hiveacid.util.HiveAcidKyroRegistrator'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.dynamicAllocation.enabled', 'true'),\n",
       " ('spark.deploy.mode', 'client'),\n",
       " ('spark.kubernetes.executor.podNamePrefix', 'cdsw-a8r5dpdadtfatd8k'),\n",
       " ('spark.app.name', 'None'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.sql.catalog.spark_catalog.type', 'hive'),\n",
       " ('spark.authenticate', 'true')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6df2dd3d-5572-4d05-881c-bf24fb500080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+\n",
      "|      catalog|namespace|\n",
      "+-------------+---------+\n",
      "|spark_catalog|  default|\n",
      "+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show catalog and database\n",
    "spark.sql(\"SHOW CURRENT NAMESPACE\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb0dd49b-bdd2-4053-8842-f208781f81be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new database\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS spark_catalog.lakehouse\")\n",
    "spark.sql(\"USE spark_catalog.lakehouse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3fa100d-b031-4b20-9e17-3c8ee9959334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS lakehouse.coffees_table_part_evol PURGE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db6f6db1-e801-4e10-a88f-e70502c788a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE IF NOT EXISTS coffees_table_part_evol (coffee_id BIGINT, coffee_size STRING, coffee_sale_ts TIMESTAMP)\\\n",
    "          USING ICEBERG\\\n",
    "          PARTITIONED BY (months(coffee_sale_ts))\\\n",
    "          TBLPROPERTIES ('write.delete.mode'='copy-on-write',\\\n",
    "                          'write.update.mode'='copy-on-write',\\\n",
    "                          'write.merge.mode'='copy-on-write',\\\n",
    "                          'format-version' = '2')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5e860f0-ebef-430a-842d-0d79c6e1f4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"INSERT INTO lakehouse.coffees_table_part_evol\\\n",
    "    VALUES (2, 'tall', cast(date_format('2023-08-01 11:10:00', 'yyyy-MM-dd HH:mm:ss') as timestamp)),\\\n",
    "    (3, 'venti', cast(date_format('2023-04-01 12:01:00', 'yyyy-MM-dd HH:mm:ss') as timestamp)),\\\n",
    "    (4, 'venti', cast(date_format('2023-07-01 12:01:00', 'yyyy-MM-dd HH:mm:ss') as timestamp)),\\\n",
    "    (5, 'grande', cast(date_format('2023-07-01 12:01:00', 'yyyy-MM-dd HH:mm:ss') as timestamp)),\\\n",
    "    (6, 'grande', cast(date_format('2023-07-01 12:01:00', 'yyyy-MM-dd HH:mm:ss') as timestamp)),\\\n",
    "    (7, 'venti', cast(date_format('2023-05-01 12:01:00', 'yyyy-MM-dd HH:mm:ss') as timestamp)),\\\n",
    "    (8, 'grande', cast(date_format('2023-04-01 12:01:00', 'yyyy-MM-dd HH:mm:ss') as timestamp)),\\\n",
    "    (9, 'tall', cast(date_format('2023-05-01 12:01:00', 'yyyy-MM-dd HH:mm:ss') as timestamp)),\\\n",
    "    (10, 'tall', cast(date_format('2023-05-01 12:01:00', 'yyyy-MM-dd HH:mm:ss') as timestamp))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6538cb9-c8b8-426f-b666-2a90cfe79151",
   "metadata": {},
   "source": [
    "Spark partitioning is a way to split the data into multiple partitions so that you can execute transformations on multiple partitions in parallel which allows completing the job faster. You can also write partitioned data into a file system (multiple sub-directories) for faster reads by downstream systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e71b1e7-d842-40f1-9e50-a442629da39d",
   "metadata": {},
   "source": [
    "Iceberg provides a method to look into the current partitions via Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d42d861d-f0e2-436d-a06e-e9a446f6f8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TABLE PARTITIONS BEFORE ALTER PARTITION STATEMENT: \n",
      "+---------+------------+----------+-------+\n",
      "|partition|record_count|file_count|spec_id|\n",
      "+---------+------------+----------+-------+\n",
      "|    {642}|           3|         1|      0|\n",
      "|    {643}|           1|         1|      0|\n",
      "|    {639}|           2|         1|      0|\n",
      "|    {640}|           3|         1|      0|\n",
      "+---------+------------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"TABLE PARTITIONS BEFORE ALTER PARTITION STATEMENT: \")\n",
    "spark.sql(\"SELECT * FROM lakehouse.coffees_table_part_evol.partitions\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b8a51c-87fc-46c3-b612-50be1a564f96",
   "metadata": {},
   "source": [
    "To migrate from daily to hourly partitioning with transforms, it is not necessary to drop the daily partition field. Keeping the field ensures existing metadata table queries continue to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7dff711-60d1-4820-b00f-766a65dc9d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLACE PARTITION MONTHS(COFFEE SALE TS) WITH DAY (COFFEE SALE TS): \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"REPLACE PARTITION MONTHS(COFFEE SALE TS) WITH DAY (COFFEE SALE TS): \")\n",
    "#print(\"ALTER TABLE spark_catalog.lakehouse.coffees_table_part_evol ADD PARTITION FIELD months(event_ts)\")\n",
    "#spark.sql(\"ALTER TABLE spark_catalog.lakehouse.coffees_table_part_evol ADD PARTITION FIELD months(event_ts)\")\n",
    "#spark.sql(\"ALTER TABLE spark_catalog.lakehouse.part_evol_tbl REPLACE PARTITION FIELD hours(dob) WITH state\")\n",
    "#spark.sql(\"ALTER TABLE prod.db.sample ADD PARTITION FIELD month\")\n",
    "\n",
    "spark.sql(\"ALTER TABLE lakehouse.coffees_table_part_evol REPLACE PARTITION FIELD months(coffee_sale_ts) WITH days(coffee_sale_ts)\")\n",
    "\n",
    "#ALTER TABLE spark_catalog.lakehouse.part_evol_tbl ADD PARTITION FIELD days(event_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de029fed-8f5b-4929-861e-8d76d15600fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TABLE PARTITIONS AFTER ALTER PARTITION STATEMENT: \n",
      "+-----------+------------+----------+-------+\n",
      "|  partition|record_count|file_count|spec_id|\n",
      "+-----------+------------+----------+-------+\n",
      "|{639, null}|           2|         1|      0|\n",
      "|{643, null}|           1|         1|      0|\n",
      "|{642, null}|           3|         1|      0|\n",
      "|{640, null}|           3|         1|      0|\n",
      "+-----------+------------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"TABLE PARTITIONS AFTER ALTER PARTITION STATEMENT: \")\n",
    "spark.sql(\"SELECT * FROM spark_catalog.lakehouse.coffees_table_part_evol.PARTITIONS\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2a3a99-a7c6-4e40-a0d7-2fec4683893d",
   "metadata": {},
   "source": [
    "#### Notice partitioning is unchaged. This is because we haven't added any data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f529101e-162b-4d5e-a032-1da3a669ca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "appendDf = spark.sql(\"SELECT * FROM spark_catalog.lakehouse.coffees_table_part_evol TABLESAMPLE (50 PERCENT)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55ee6094-1d65-45f9-a52c-40c43e4cb599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "appendDf.sortWithinPartitions(\"coffee_size\").writeTo(\"spark_catalog.lakehouse.coffees_table_part_evol\").using(\"iceberg\").append() #.append()#replace()#overwritePartitions()#create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "459f7b9d-4f02-4f32-800b-50ad41eabdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TABLE PARTITIONS AFTER APPEND STATEMENT: \n",
      "+------------------+------------+----------+-------+\n",
      "|         partition|record_count|file_count|spec_id|\n",
      "+------------------+------------+----------+-------+\n",
      "|       {639, null}|           2|         1|      0|\n",
      "|       {643, null}|           1|         1|      0|\n",
      "|       {642, null}|           3|         1|      0|\n",
      "|       {640, null}|           3|         1|      0|\n",
      "|{null, 2023-05-01}|           1|         1|      1|\n",
      "|{null, 2023-04-01}|           1|         1|      1|\n",
      "+------------------+------------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"TABLE PARTITIONS AFTER APPEND STATEMENT: \")\n",
    "spark.sql(\"SELECT * FROM spark_catalog.lakehouse.coffees_table_part_evol.PARTITIONS\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa06db9-154f-4493-a7ac-93dfccfa3f9a",
   "metadata": {},
   "source": [
    "Adding a partition field is a metadata operation and does not change any of the existing table data. New data will be written with the new partitioning, but existing data will remain in the old partition layout. Old data files will have null values for the new partition fields in metadata tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4e6ce0-ba7e-485c-8781-cee498e4c230",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
